{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## alpha-RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Justin Tan\n",
    "\n",
    "RNN alpha build for rare decay identification in TensorFlow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "class config(object):\n",
    "    # Set network parameters\n",
    "    # Empirically, depth more important than layer size - output dimension\n",
    "    mode = 'continuum'\n",
    "    channel = 'rho0'\n",
    "    n_particles = 5\n",
    "    n_features = 100\n",
    "    seq_length = n_features/n_particles\n",
    "    steps_per_epoch = 1e3\n",
    "    rnn_cell = 'lru_cell' # 'gru'\n",
    "    hidden_units = 512  # Number of neurons per RNN Cell\n",
    "    keep_prob = 0.85\n",
    "    input_keep_prob = 0.85\n",
    "    recurrent_keep_prob = 0.85\n",
    "    num_epochs = 64\n",
    "    batch_size = 128\n",
    "    num_layers = 3 # Note: 3 layers is considered 'deep'\n",
    "    learning_rate = 1e-4\n",
    "    lr_epoch_decay = 0.96\n",
    "    n_classes = 2\n",
    "\n",
    "class directories(object):\n",
    "    data = 'data'\n",
    "    tensorboard = 'tensorboard'\n",
    "    checkpoints = 'checkpoints'\n",
    "    samples = 'samples'\n",
    "    \n",
    "architecture = '{} - {} | Base cell: {} | Hidden units: {} | Layers: {} | Batch: {} | Epochs: {}'.format(\n",
    "    config.channel, config.mode, config.rnn_cell, config.hidden_units, config.num_layers, config.batch_size, config.num_epochs)\n",
    "\n",
    "class reader():\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df = df\n",
    "        self.batch_size = config.batch_size\n",
    "        self.steps_per_epoch = len(df) // config.batch_size\n",
    "        self.epochs = 0\n",
    "        self.proceed = True\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.df_X = self.df.drop('Labels', axis = 1)\n",
    "        self.df_y = self.df['Labels']\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.pointer + 1 >= self.steps_per_epoch:\n",
    "            inputs = self.df_X.iloc[self.pointer*batch_size:]\n",
    "            targets = self.df_y.iloc[self.pointer*batch_size:]\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.proceed = False\n",
    "            \n",
    "        inputs = self.df_X.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        targets = self.df_y.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        self.pointer += 1\n",
    "                \n",
    "        return inputs, targets\n",
    "\n",
    "def save_summary(config, delta_t, train_acc, test_acc):\n",
    "    import json\n",
    "    summary = {\n",
    "        'Timestamp': time.strftime('%c'),\n",
    "        'Base cell': config.rnn_cell,\n",
    "        'Hidden units': config.hidden_units,\n",
    "        'Layers': config.num_layers,\n",
    "        'Batch_size': config.batch_size,\n",
    "        'Seq_length': config.seq_length,\n",
    "        'Dropout': config.keep_prob,\n",
    "        'Epochs': config.num_epochs,\n",
    "        'Time': delta_t,\n",
    "        'Final train acc': train_acc,\n",
    "        'Final test acc': test_acc\n",
    "    }\n",
    "    # Writing JSON data\n",
    "    if os.path.isfile('rnn_summary.json'):\n",
    "        with open('rnn_summary_{}.json.format(config.name)', 'r+') as f:\n",
    "            new = json.load(f)\n",
    "        new.append(summary)\n",
    "        with open('rnn_summary.json', 'w') as f:\n",
    "            json.dump(new, f, indent = 4)\n",
    "    else:\n",
    "        with open('rnn_summary.json', 'w') as f:\n",
    "             json.dump([summary], f, indent = 4)\n",
    "\n",
    "def load_data(file_name, test_size = 0.05):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df = pd.read_hdf(file_name, 'df')\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df.drop('Labels', axis = 1),\n",
    "                                                                    df['Labels'], test_size = test_size, random_state=42)\n",
    "    return df_X_train, df_X_test, df_y_train, df_y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_file = '/home/ubuntu/radiative/data/rnn/rnn_B02rho0gamma_continuum.h5'\n",
    "\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = load_data(test_file)\n",
    "config.n_features = df_X_train.shape[1]\n",
    "config.seq_length = config.n_features//config.n_particles\n",
    "config.steps_per_epoch = len(df_X_train) // config.batch_size\n",
    "assert config.seq_length == config.n_features/config.n_particles, 'Discrepancy in input feature dimension'\n",
    "\n",
    "df_train = pd.concat([df_X_train, df_y_train], axis = 1)\n",
    "df_test = pd.concat([df_X_test, df_y_test], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "readerTrain = reader(df_train)\n",
    "readerTest = reader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5.12394619e+00,  -7.94767961e-02,  -3.48720968e-01,\n",
       "          6.89857081e-02,   3.57663065e-01,   3.64255279e-01,\n",
       "         -1.79487824e+00,   1.89388365e-01,   1.48642960e-03,\n",
       "          1.24982413e-04,   7.74669519e-04,   2.25706212e-03,\n",
       "          1.34642087e-02,   5.65924263e+00,   1.54790401e+00,\n",
       "          2.96310177e+01,   1.13851771e-01,   5.11098242e+00],\n",
       "       [  2.61728239e+00,   2.04600549e+00,  -4.84532654e-01,\n",
       "          1.39081919e+00,   2.10259604e+00,   2.52096987e+00,\n",
       "         -2.32534915e-01,   5.51700056e-01,   1.53519592e-04,\n",
       "          7.49543105e-05,   5.93450794e-04,   7.81479699e-04,\n",
       "          5.92091560e-01,   7.80361032e+00,   1.96310759e+00,\n",
       "          5.70425415e+01,  -7.25154400e-01,   7.03476071e-01],\n",
       "       [  2.05967164e+00,   1.79474211e+00,  -2.70597786e-01,\n",
       "          9.63558376e-01,   1.81502676e+00,   2.05493712e+00,\n",
       "         -1.49645388e-01,   4.68899161e-01,   1.85109839e-05,\n",
       "          3.85362773e-06,   1.39815065e-05,   2.92153127e-05,\n",
       "          5.07277668e-01,   8.51536846e+00,   2.03612328e+00,\n",
       "          6.83656921e+01,  -4.76340562e-01,   1.39570177e-01],\n",
       "       [  5.55337667e-01,   2.50287563e-01,  -2.13602960e-01,\n",
       "          4.25029397e-01,   3.29044223e-01,   5.37512839e-01,\n",
       "         -7.06482053e-01,   7.90733457e-01,   1.35387323e-04,\n",
       "          7.06458086e-05,   5.85658359e-04,   7.56367692e-04,\n",
       "          1.00901210e+00,   1.02972803e+01,   2.41617870e+00,\n",
       "          1.00196068e+02,  -7.75152564e-01,   1.39570177e-01],\n",
       "       [  2.50666380e+00,  -2.12548232e+00,   1.35811701e-01,\n",
       "         -1.32183349e+00,   2.12981701e+00,   2.50666380e+00,\n",
       "          3.07778263e+00,  -5.27327776e-01,   1.33290945e-03,\n",
       "          5.00281240e-05,   1.81218755e-04,   1.47558260e-03,\n",
       "         -5.86436391e-01,   8.86163235e+00,   4.43173409e+00,\n",
       "          5.88882523e+01,   1.00000000e+00,   0.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.values.reshape([-1,config.n_particles, config.seq_length])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cell_dropout(base_cell, keep_prob):\n",
    "    # Apply dropout between RNN layers - only on the output\n",
    "    cell_dropout = tf.contrib.rnn.DropoutWrapper(base_cell, output_keep_prob=keep_prob)\n",
    "    return cell_dropout\n",
    "\n",
    "def layer_weights(shape, name = 'weights'):\n",
    "    # Return weight tensor of given shape using Xavier initialization\n",
    "    W = tf.get_variable(name, shape = shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return W\n",
    "\n",
    "def layer_biases(shape, name = 'biases'):\n",
    "    # Return bias tensor of given shape with small initialized constant value\n",
    "    b = tf.get_variable(name, shape = shape, initializer = tf.constant_initializer(0.01))\n",
    "    return b\n",
    "\n",
    "class alphaRNN():\n",
    "    def __init__(self, config, training = True):\n",
    "        self.config = config\n",
    "        self.scope = 'alpha'\n",
    "                \n",
    "        # Placeholders for feed_dict\n",
    "        self.inputs = tf.placeholder(tf.float32, shape = [None, self.config.n_features])\n",
    "        self.targets = tf.placeholder(tf.int32, shape = [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32) # Dropout on input connections\n",
    "        \n",
    "        # Reshape input to batch_size x n_particles x seq_length tensor\n",
    "        rnn_inputs = tf.reshape(self.inputs, [-1, config.n_particles, config.seq_length])\n",
    "        \n",
    "        # Place operations necessary to perform inference onto graph\n",
    "        if config.rnn_cell == 'lstm':\n",
    "            base_cell = tf.contrib.rnn.LSTMCell(num_units = config.hidden_units, forget_bias = 1.0, state_is_tuple = True)\n",
    "        elif config.rnn_cell == 'gru':\n",
    "            base_cell = tf.contrib.rnn.GRUCell(num_units = config.hidden_units)\n",
    "        elif config.rnn_cell == 'layer-norm':\n",
    "            base_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units = config.hidden_units,\n",
    "                                                          forget_bias = 1.0, dropout_keep_prob = self.config.recurrent_keep_prob)\n",
    "        else:\n",
    "            base_cell = tf.contrib.rnn.LayerNormBasicLSTMCell(num_units = config.hidden_units,\n",
    "                                                          forget_bias = 1.0, dropout_keep_prob = self.config.recurrent_keep_prob)\n",
    "        self.cell = base_cell\n",
    "        # Apply Dropout operator on non-recurrent connections\n",
    "        if training and self.config.input_keep_prob < 1:\n",
    "            rnn_inputs = tf.nn.dropout(rnn_inputs, self.keep_prob)\n",
    "            self.cell = tf.contrib.rnn.DropoutWrapper(base_cell, input_keep_prob=self.config.input_keep_prob)\n",
    "\n",
    "        # Wrap stacked cells into a single cell\n",
    "        self.multicell = tf.contrib.rnn.MultiRNNCell(\n",
    "            [self.cell for _ in range(config.num_layers)], state_is_tuple=True)\n",
    "\n",
    "        # Accept previous hidden state as input\n",
    "        self.init_state = self.multicell.zero_state(self.config.batch_size, tf.float32)\n",
    "\n",
    "        # Outputs shaped [batch_size, max_time, cell.output_size]\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(\n",
    "            cell = self.multicell, inputs = rnn_inputs, initial_state = self.init_state, scope = self.scope)\n",
    "        \n",
    "        # Extract output from last time step\n",
    "        output = rnn_outputs[:,-1,:]\n",
    "\n",
    "        with tf.variable_scope('softmax'):\n",
    "            softmax_W = layer_weights(shape = [config.hidden_units, config.n_classes], name = 'smx_W')\n",
    "            softmax_b = layer_biases(shape = [config.n_classes], name = 'smx_b')\n",
    "\n",
    "        self.logits_RNN = tf.matmul(output, softmax_W) + softmax_b  # Unormalized log probabilties for next char\n",
    "        self.predictions = tf.nn.softmax(self.logits_RNN)\n",
    "        \n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.logits_RNN, labels = self.targets))\n",
    "        tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "        \n",
    "        # Anneal learning rate\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        learning_rate = tf.train.exponential_decay(config.learning_rate, global_step,\n",
    "                                                       decay_steps = config.steps_per_epoch, decay_rate = config.lr_epoch_decay, staircase=True)\n",
    "\n",
    "        self.train_op = tf.train.AdamOptimizer(config.learning_rate).minimize(self.cross_entropy, name = 'optimizer',\n",
    "                                                                              global_step = global_step)  \n",
    "        \n",
    "        # Evaluate correctness\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(self.predictions, 1), tf.int32), self.targets)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', self.accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, restore = False):\n",
    "    \n",
    "    pRNN = alphaRNN(config, training = True)\n",
    "    start_time = time.time()\n",
    "    v_acc_best = 0.\n",
    "    saver = tf.train.Saver()\n",
    "    merge_op = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(\n",
    "        os.path.join(directories.tensorboard, 'train_{}'.format(time.strftime('%d-%m_%I:%M'))), graph = tf.get_default_graph())\n",
    "    test_writer = tf.summary.FileWriter(os.path.join(directories.tensorboard, 'test_{}'.format(time.strftime('%d-%m_%I:%M'))))\n",
    "    ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        if restore and ckpt.model_checkpoint_path:\n",
    "            print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "#             saver = tf.train.import_meta_graph('checkpoints/char-RNN__epoch49.ckpt-49.meta')\n",
    "#             saver.restore(sess, 'checkpoints/char-RNN__epoch49.ckpt-49')\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                    \n",
    "        for epoch in range(config.num_epochs):\n",
    "            \n",
    "            readerTrain.proceed = True\n",
    "            begin = time.time()\n",
    "            step = 0\n",
    "            print('(*) Entering Epoch {} ({:.3f} s)'.format(epoch, time.time() - start_time))\n",
    "\n",
    "            # Save every 10 epochs    \n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess,\n",
    "                                       os.path.join(directories.checkpoints,'pRNN_{}_{}_epoch{}.ckpt'.format(config.mode, config.channel, epoch)),\n",
    "                                       global_step = epoch)\n",
    "                print('Graph saved to file: {}'.format(save_path))\n",
    "\n",
    "            while(readerTrain.proceed):\n",
    "                # Iterate through entire corpus\n",
    "                x_train, y_train = readerTrain.next_batch(config.batch_size)\n",
    "                feed_dict_train = {pRNN.inputs: x_train.values, pRNN.targets: y_train.values, pRNN.keep_prob: config.keep_prob}\n",
    "                t_op = sess.run(pRNN.train_op, feed_dict = feed_dict_train)\n",
    "                step += 1\n",
    "\n",
    "                if step % (config.steps_per_epoch // 10) == 0:            \n",
    "                    # Evaluate model\n",
    "                    improved = ''\n",
    "                    x_test, y_test = readerTest.next_batch(config.batch_size)\n",
    "                    feed_dict_test = {pRNN.inputs: x_test.values, pRNN.targets: y_test.values, pRNN.keep_prob: 1.0}\n",
    "\n",
    "                    t_acc, t_loss, t_summary = sess.run([pRNN.accuracy, pRNN.cross_entropy, merge_op],\n",
    "                                                        feed_dict = feed_dict_train)\n",
    "                    v_acc, v_loss, v_summary, = sess.run([pRNN.accuracy, pRNN.cross_entropy, merge_op],\n",
    "                                                        feed_dict = feed_dict_test)\n",
    "\n",
    "                    train_writer.add_summary(t_summary, step)\n",
    "                    test_writer.add_summary(v_summary, step)\n",
    "                    \n",
    "                    if epoch > 5 and v_acc > v_acc_best:\n",
    "                        v_acc_best = v_acc\n",
    "                        improved = '*'\n",
    "                        save_path = saver.save(sess, os.path.join(directories.checkpoints, 'best.ckpt'), global_step = epoch)\n",
    "                    \n",
    "                    print('Epoch {}, Step {} | Training Accuracy: {:.3f} | Test Accuracy: {:.3f} | Training Loss: {:.3f} | Test Loss {:.3f} {}'\n",
    "                          .format(epoch, step, t_acc, v_acc, t_loss, v_loss, improved))\n",
    "\n",
    "        save_path = saver.save(sess, os.path.join(directories.checkpoints, 'pRNN_end'),\n",
    "                               global_step = epoch)\n",
    "        print('Metagraph saved to file: {}'.format(save_path))\n",
    "\n",
    "        final_train_accuracy = pRNN.accuracy.eval(feed_dict = {pRNN.inputs: df_X_train.values, pRNN.targets: df_y_train.values, pRNN.keep_prob: 1.0})\n",
    "        final_test_accuracy = pRNN.accuracy.eval(feed_dict = {pRNN.inputs: df_X_test.values, pRNN.targets: df_y_test.values, pRNN.keep_prob: 1.0})\n",
    "        delta_t = time.time() - start_time\n",
    "            \n",
    "    print(\"Training Complete. Time elapsed: {:.3f} s\".format(delta_t))\n",
    "    print(\"Train accuracy: %g\\nValidation accuracy: %g\" %(final_train_accuracy, final_test_accuracy))\n",
    "\n",
    "    print('Architecture: {}'.format(architecture))\n",
    "    save_summary(config, delta_t, final_train_accuracy, final_test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) Entering Epoch 0 (2.085 s)\n",
      "Graph saved to file: checkpoints/pRNN_continuum_rho0_epoch0.ckpt-0\n"
     ]
    }
   ],
   "source": [
    "train(config)#, restore = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
