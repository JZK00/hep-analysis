{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## relational-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Justin Tan\n",
    "\n",
    "To-do: Update to TF 1.2, fused batch norm\n",
    "\n",
    "Implements relational network as described in https://arxiv.org/pdf/1706.01427.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import json\n",
    "\n",
    "class config(object):\n",
    "    # Set network parameters\n",
    "    mode = 'continuum'\n",
    "    channel = 'rho0'\n",
    "    n_features = 100\n",
    "    keep_prob = 0.85\n",
    "    num_epochs = 128\n",
    "    batch_size = 128\n",
    "    steps_per_epoch = 0\n",
    "    learning_rate = 1e-4\n",
    "    lr_epoch_decay = 0.98\n",
    "    n_classes = 2\n",
    "    cardinality = 5\n",
    "    n_obj_features = 20\n",
    "\n",
    "class g_config(object):\n",
    "    n_layers = 4,\n",
    "    nodes = [1024, 1024, 1024, 512]\n",
    "    keep_prob = 0.95\n",
    "\n",
    "class f_config(object):\n",
    "    n_layers = 3,\n",
    "    nodes = [1024, 1024, 512, 256]\n",
    "    keep_prob = 0.95\n",
    "    \n",
    "class directories(object):\n",
    "    data = 'data'\n",
    "    tensorboard = 'tensorboard'\n",
    "    checkpoints = 'checkpoints'\n",
    "\n",
    "class reader():\n",
    "    # Iterates over data and returns batches\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df = df\n",
    "        self.batch_size = config.batch_size\n",
    "        self.steps_per_epoch = len(df) // config.batch_size\n",
    "        self.epochs = 0\n",
    "        self.proceed = True\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.df_X = self.df.drop('Labels', axis = 1)\n",
    "        self.df_y = self.df['Labels']\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.pointer + 1 >= self.steps_per_epoch:\n",
    "            inputs = self.df_X.iloc[self.pointer*batch_size:]\n",
    "            targets = self.df_y.iloc[self.pointer*batch_size:]\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.proceed = False\n",
    "            \n",
    "        inputs = self.df_X.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        targets = self.df_y.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        self.pointer += 1\n",
    "                \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name, test_size = 0.05):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df = pd.read_hdf(file_name, 'df')\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df.drop('Labels', axis = 1),\n",
    "                                                                    df['Labels'], test_size = test_size, random_state=42)\n",
    "    return df_X_train, df_X_test, df_y_train, df_y_test\n",
    "\n",
    "def save_summary(config, delta_t, train_acc, test_acc, test_auc):\n",
    "    summary = {\n",
    "        'Channel': config.channel,\n",
    "        'Mode': config.mode,\n",
    "        'Timestamp': time.strftime('%c'),\n",
    "        'Hidden units': config.hidden_units,\n",
    "        'Layers': config.n_layers,\n",
    "        'Batch_size': config.batch_size,\n",
    "        'Dropout': config.keep_prob,\n",
    "        'Epochs': config.num_epochs,\n",
    "        'Time': delta_t,\n",
    "        'Final train acc': train_acc,\n",
    "        'Final test acc': test_acc,\n",
    "        'Final test AUC': test_auc\n",
    "    }\n",
    "    # Writing JSON data\n",
    "    if os.path.isfile('vdnn_summary.json'):\n",
    "        with open('vdnn_summary.json.', 'r+') as f:\n",
    "            new = json.load(f)\n",
    "        new.append(summary)\n",
    "        with open('vdnn_summary.json', 'w') as f:\n",
    "            json.dump(new, f, indent = 4)\n",
    "    else:\n",
    "        with open('vdnn_summary.json', 'w') as f:\n",
    "             json.dump([summary], f, indent = 4)\n",
    "\n",
    "def layer_weights(shape):\n",
    "    # Return weight tensor of given shape using Xavier initialization\n",
    "    W = tf.get_variable(\"weights\", shape = shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return W\n",
    "\n",
    "def layer_biases(shape):\n",
    "    # Return bias tensor of given shape with small initialized constant value\n",
    "    b = tf.get_variable(\"biases\", shape = shape, initializer = tf.constant_initializer(0.001))\n",
    "    return b\n",
    "\n",
    "def hidden_layer_ops(x, shape, name, keep_prob, activation=tf.nn.relu):\n",
    "    # Add operations to graph to construct hidden layers\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        \n",
    "        # Apply non-linearity. Default is ReLU\n",
    "        actv = activation(tf.matmul(x, weights) + biases)\n",
    "        layer_output = tf.nn.dropout(actv, keep_prob)\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def readout_ops(x, shape, name):\n",
    "    # Don't apply non-linearity, dropout on output layer\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        layer_output = tf.matmul(x, weights) + biases\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def hidden_layer_ops(x, shape, name, keep_prob, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        \n",
    "        # Apply non-linearity. Default is ReLU\n",
    "        actv = activation(tf.matmul(x, weights) + biases)\n",
    "        layer_output = tf.nn.dropout(actv, keep_prob)\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def BN_layer_ops(x, shape, name, keep_prob, phase, activation=tf.nn.relu):\n",
    "    with tf.variable_scope(name) as scope:\n",
    "         # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        z_BN = tf.matmul(x, weights) + biases\n",
    "        \n",
    "        # Place BN transform before non-linearity - update to TF 1.2!\n",
    "        theta_BN = tf.contrib.layers.batch_norm(z_BN, center=True, scale=True, is_training=phase, \n",
    "                                                decay=0.95, zero_debias_moving_mean=True, scope='bn', fused = True)\n",
    "        BN_actv = activation(theta_BN)\n",
    "        BN_layer_output = tf.nn.dropout(BN_actv, keep_prob)\n",
    "\n",
    "    return BN_layer_output\n",
    "\n",
    "def build_network(x, n_layers, hidden_layer_nodes, training_phase):\n",
    "    assert n_layers == len(hidden_layer_nodes), 'Specified layer nodes and number of layers do not correspond.'\n",
    "    layers = [x]\n",
    "    with tf.variable_scope('BN_layers') as scope:\n",
    "        hidden_1 = BN_layer_ops(x, shape = [config.n_features, hidden_layer_nodes[0]], name = 'BNhidden0',\n",
    "                                keep_prob = config.keep_prob, phase = training_phase)\n",
    "        layers.append(hidden_1)\n",
    "        for n in range(1,n_layers-1):\n",
    "            hidden_n = BN_layer_ops(layers[-1], shape = [hidden_layer_nodes[n], hidden_layer_nodes[n+1]], name = 'BNhidden{}'.format(n),\n",
    "                                   keep_prob = config.keep_prob, phase = training_phase)\n",
    "            layers.append(hidden_n)\n",
    "\n",
    "        readout = readout_ops(layers[-1], shape = [hidden_layer_nodes[-1], config.n_classes], name = 'readout')\n",
    "        layers.append(readout)\n",
    "    \n",
    "    return layers\n",
    "\n",
    "def plot_ROC_curve(y_true, y_pred, meta = ''):\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "    # Compute ROC curve, integrate\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.axes([.1,.1,.8,.7])\n",
    "    plt.figtext(.5,.9, r'$\\mathrm{Receiver \\;Operating \\;Characteristic}$', fontsize=15, ha='center')\n",
    "    plt.figtext(.5,.85, meta, fontsize=10,ha='center')\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=2, label='ROC (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=1.0, linestyle='--')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(r'$\\mathrm{False \\;Positive \\;Rate}$')\n",
    "    plt.ylabel(r'$\\mathrm{True \\;Positive \\;Rate}$')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join('graphs', '{}_{}_ROC.pdf'.format(args.channel, args.mode), format='pdf', dpi=1000))\n",
    "    plt.show()\n",
    "    plt.gcf().clear()\n",
    "\n",
    "def g_network(x, training_phase, i, j):\n",
    "    g_hidden_1 = BN_layer_ops(x, shape = [x.get_shape()[1], g_config.nodes[0]], name = 'g_{}_hidden1'.format(i,j),\n",
    "                            keep_prob = g_config.keep_prob, phase = training_phase)\n",
    "    g_hidden_2 = BN_layer_ops(g_hidden_1, shape = [g_config.nodes[0], g_config.nodes[1]], name = 'g_{}_hidden2'.format(i,j),\n",
    "                            keep_prob = g_config.keep_prob, phase = training_phase)\n",
    "    g_hidden_3 = BN_layer_ops(g_hidden_2, shape = [g_config.nodes[1], g_config.nodes[2]], name = 'g_{}_hidden3'.format(i,j),\n",
    "                            keep_prob = g_config.keep_prob, phase = training_phase)\n",
    "    g_hidden_4 = BN_layer_ops(g_hidden_3, shape = [g_config.nodes[2], g_config.nodes[3]], name = 'g_{}_hidden4'.format(i,j),\n",
    "                            keep_prob = g_config.keep_prob, phase = training_phase)\n",
    "    return g_hidden_4\n",
    "\n",
    "def f_network(x, training_phase):\n",
    "    f_hidden_1 = BN_layer_ops(x, shape = [x.get_shape()[1], f_config.nodes[0]], name = 'f_hidden1',\n",
    "                             keep_prob = f_config.keep_prob, phase = training_phase)\n",
    "    f_hidden_2 = BN_layer_ops(f_hidden_1, shape = [f_config.nodes[0], f_config.nodes[1]], name = 'f_hidden2',\n",
    "                            keep_prob = f_config.keep_prob, phase = training_phase)\n",
    "    f_hidden_3 = BN_layer_ops(f_hidden_2, shape = [f_config.nodes[1], f_config.nodes[2]], name = 'f_hidden3',\n",
    "                            keep_prob = f_config.keep_prob, phase = training_phase)\n",
    "    f_hidden_4 = BN_layer_ops(f_hidden_3, shape = [f_config.nodes[2], f_config.nodes[3]], name = 'f_hidden4',\n",
    "                            keep_prob = f_config.keep_prob, phase = training_phase)\n",
    "    f_readout = readout_ops(f_hidden_4, shape = [f_config.nodes[3], config.n_classes], name = 'f_readout')\n",
    "\n",
    "    return f_readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = '/data/rnn/rnn_B02rho0gamma_kst.h5'\n",
    "\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = load_data(test_file)\n",
    "df_y_train = df_y_train.astype(np.int8)\n",
    "df_y_test = df_y_test.astype(np.int8)\n",
    "df_train = pd.concat([df_X_train, df_y_train], axis = 1)\n",
    "df_test = pd.concat([df_X_test, df_y_test], axis = 1)\n",
    "config.n_features = df_train.shape[1]-1\n",
    "config.steps_per_epoch = df_train.shape[0] // config.batch_size\n",
    "config.n_obj_features = config.n_features // config.cardinality\n",
    "assert config.n_features % config.cardinality == 0, 'Number of features does not divide evenly between objects'\n",
    "\n",
    "readerTrain = reader(df_train)\n",
    "readerTest = reader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class relationalNN():\n",
    "    # Builds the computational graph\n",
    "    # Object set = {B, rho0, pi+, pi-, gamma}\n",
    "    def __init__(self, config, training = True):\n",
    "        \n",
    "        self.config = config\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, shape = [None, self.config.n_features])\n",
    "        self.y_true = tf.placeholder(tf.int32, shape = None)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.training_phase = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.objects = tf.reshape(self.x, [-1, self.config.cardinality, self.config.n_obj_features])\n",
    "        batch_size = tf.shape(self.x)[0]\n",
    "        self.g = tf.zeros([batch_size, g_config.nodes[-1]])\n",
    "        \n",
    "        #with tf.variable_scope('test2'):\n",
    "        \"\"\"g\"\"\"\n",
    "        for i in range(self.config.cardinality):\n",
    "            oi = self.objects[:,i]\n",
    "            for j in range(self.config.cardinality):\n",
    "                if i != j:\n",
    "                    continue\n",
    "\n",
    "                oj = self.objects[:,j]\n",
    "                x_ = tf.concat([oi, oj], axis = 1)\n",
    "                g_out = g_network(x_, self.training_phase, i, j)\n",
    "                self.g = tf.add(self.g, g_out)\n",
    "\n",
    "        \"\"\"f\"\"\"\n",
    "        self.f = f_network(self.g, self.training_phase)\n",
    "        \n",
    "        \n",
    "        prediction = tf.nn.softmax(self.f)\n",
    "        classification = tf.argmax(self.f,1)\n",
    "\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.f, labels = self.y_true))\n",
    "        tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "        \n",
    "        # Anneal learning rate\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.beta = tf.train.exponential_decay(config.learning_rate, self.global_step,\n",
    "                                                       decay_steps = config.steps_per_epoch, decay_rate = config.lr_epoch_decay, staircase=True)\n",
    "        \n",
    "        with tf.control_dependencies(update_ops):\n",
    "            # Ensures that we execute the update_ops before performing the train_step\n",
    "            self.train_op = tf.train.AdamOptimizer(self.beta).minimize(self.cross_entropy, name = 'optimizer',\n",
    "                                                                                  global_step = self.global_step)  \n",
    "\n",
    "        # Evaluation metrics\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(self.f, 1), tf.int32), self.y_true)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "        _, self.auc_op = tf.metrics.auc(predictions = tf.argmax(self.f,1), labels = self.y_true, num_thresholds = 256)\n",
    "        tf.summary.scalar('auc', self.auc_op)\n",
    "        \n",
    "        self.merge_op = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'train_{}'.format(time.strftime('%d-%m_%I:%M'))), graph = tf.get_default_graph())\n",
    "        self.test_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'test_{}'.format(time.strftime('%d-%m_%I:%M'))))\n",
    "        \n",
    "    def predict(self, ckpt, metaGraph = None):\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            # Initialize variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            start_time = time.time()\n",
    "            \n",
    "            assert (ckpt.model_checkpoint_path or metaGraph), 'Missing checkpoint file!'\n",
    "            \n",
    "            if metaGraph:\n",
    "                saver = tf.train.import_meta_graph(metaGraph)\n",
    "                saver.restore(sess, os.path.splitext(metaGraph)[0])\n",
    "                print('{} restored.'.format(metaGraph))\n",
    "            else:    \n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "\n",
    "            # Make predictions using the trained model\n",
    "            feed_dict_test = {self.x: df_X_test.values, self.y_true: df_y_test.values, self.keep_prob: 1.0, self.training_phase: False}\n",
    "            network_output_test, class_test, final_v_acc, final_v_auc = sess.run(\n",
    "                [prediction, classification, vDNN.accuracy, vDNN.auc_op], feed_dict = feed_dict_test)\n",
    "\n",
    "            print(\"Validation accuracy: {:g}\\nValidation AUC: {:g}\".format(final_v_acc, final_v_auc))\n",
    "            \n",
    "            plot_ROC_curve(y_pred = class_test, y_true = df_y_test.values, meta = architecture)\n",
    "            delta_t = time.time() - start_time\n",
    "            print(\"Inference complete. Duration: %g s\" %(delta_t))\n",
    "            \n",
    "            return network_output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, restore = False):\n",
    "    # Executes training operations\n",
    "    \n",
    "    relNN = relationalNN(config, training = True)\n",
    "    start_time = time.time()\n",
    "    v_acc_best = 0.\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        if restore and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "            \n",
    "        for epoch in range(config.num_epochs):\n",
    "            \n",
    "            readerTrain.proceed = True\n",
    "            step = 0\n",
    "            # Save every 10 epochs    \n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess,\n",
    "                                       os.path.join(directories.checkpoints,\n",
    "                                                    'relNN_{}_{}_epoch{}.ckpt'.format(config.mode, config.channel, epoch)),\n",
    "                                       global_step = epoch)\n",
    "                print('Graph saved to file: {}'.format(save_path))\n",
    "                \n",
    "            print('(*) Entering Epoch {} ({:.3f} s)'.format(epoch, time.time() - start_time))\n",
    "            \n",
    "            while(readerTrain.proceed):\n",
    "                # Iterate through entire corpus\n",
    "                x_train, y_train = readerTrain.next_batch(config.batch_size)\n",
    "                feed_dict_train = {relNN.x: x_train.values, relNN.y_true: y_train.values, \n",
    "                                   relNN.keep_prob: config.keep_prob, relNN.training_phase: True}#, relNN.eta: eta}\n",
    "                t_op = sess.run(relNN.train_op, feed_dict = feed_dict_train)\n",
    "                step += 1\n",
    "\n",
    "                if step % (config.steps_per_epoch // 20) == 0:            \n",
    "                    # Evaluate model\n",
    "                    improved = ''\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    x_test, y_test = readerTest.next_batch(config.batch_size)\n",
    "                    feed_dict_test = {relNN.x: x_test.values, relNN.y_true: y_test.values, relNN.keep_prob: 1.0,\n",
    "                                      relNN.training_phase: False}#, relNN.eta: eta}\n",
    "\n",
    "                    t_acc, t_loss, t_summary = sess.run([relNN.accuracy, relNN.cross_entropy, relNN.merge_op],\n",
    "                                                        feed_dict = feed_dict_train)\n",
    "                    v_acc, v_loss, v_auc, v_summary, = sess.run([relNN.accuracy, relNN.cross_entropy, relNN.auc_op, relNN.merge_op],\n",
    "                                                        feed_dict = feed_dict_test)\n",
    "\n",
    "                    relNN.train_writer.add_summary(t_summary, step)\n",
    "                    relNN.test_writer.add_summary(v_summary, step)\n",
    "                    \n",
    "                    if epoch > 5 and v_acc > v_acc_best:\n",
    "                        v_acc_best = v_acc\n",
    "                        improved = '[*]'\n",
    "                        save_path = saver.save(sess, \n",
    "                                               os.path.join(directories.checkpoints,\n",
    "                                                            'relNN_{}_{}_best.ckpt'.format(config.mode, config.channel)),\n",
    "                                               global_step = epoch)\n",
    "                    \n",
    "                    print('Epoch {}, Step {} | Training Acc: {:.3f} | Test Acc: {:.3f} | Test Loss: {:.3f} | Test AUC {:.3f} ({:.2f} s) {}'\n",
    "                          .format(epoch, step, t_acc, v_acc, v_loss, v_auc, time.time() - start_time, improved))\n",
    "\n",
    "        save_path = saver.save(sess, os.path.join(directories.checkpoints, 'relNN_{}_{}_end.ckpt'.format(config.mode, config.channel)),\n",
    "                               global_step = epoch)\n",
    "        print('Model saved to file: {}'.format(save_path))\n",
    "\n",
    "        feed_dict_train = {relNN.x: df_X_train.values, relNN.y_true: df_y_train.values, relNN.keep_prob: 1.0, relNN.training_phase: False}\n",
    "        feed_dict_test = {relNN.x: df_X_test.values, relNN.y_true: df_y_test.values, relNN.keep_prob: 1.0, relNN.training_phase: False}\n",
    "        final_t_acc = relNN.accuracy.eval(feed_dict = feed_dict_train)\n",
    "        final_v_acc, final_v_AUC = sess.run([relNN.accuracy, relNN.auc_op], feed_dict = feed_dict_test)\n",
    "\n",
    "        delta_t = time.time() - start_time\n",
    "            \n",
    "    print(\"Training Complete. Time elapsed: {:.3f} s\".format(delta_t))\n",
    "    print(\"Train accuracy: {:g}\\nValidation accuracy: {:g}\\nValidation AUC: {:g}\".format(final_t_acc, final_v_acc, final_v_AUC))\n",
    "\n",
    "    print('Architecture: {}'.format(architecture))\n",
    "    save_summary(config, delta_t, final_t_acc, final_v_acc, final_v_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to file: checkpoints/relNN_continuum_rho0_epoch0.ckpt-0\n",
      "(*) Entering Epoch 0 (8.356 s)\n",
      "Epoch 0, Step 190 | Training Acc: 0.750 | Test Acc: 0.719 | Test Loss: 0.532 | Test AUC 0.707 (100.61 s) \n",
      "Epoch 0, Step 380 | Training Acc: 0.711 | Test Acc: 0.742 | Test Loss: 0.531 | Test AUC 0.713 (188.90 s) \n",
      "Epoch 0, Step 570 | Training Acc: 0.719 | Test Acc: 0.727 | Test Loss: 0.489 | Test AUC 0.710 (278.30 s) \n",
      "Epoch 0, Step 760 | Training Acc: 0.812 | Test Acc: 0.812 | Test Loss: 0.447 | Test AUC 0.782 (369.38 s) \n",
      "Epoch 0, Step 950 | Training Acc: 0.719 | Test Acc: 0.695 | Test Loss: 0.587 | Test AUC 0.699 (458.48 s) \n",
      "Epoch 0, Step 1140 | Training Acc: 0.789 | Test Acc: 0.750 | Test Loss: 0.525 | Test AUC 0.765 (548.97 s) \n",
      "Epoch 0, Step 1330 | Training Acc: 0.797 | Test Acc: 0.781 | Test Loss: 0.440 | Test AUC 0.799 (638.06 s) \n",
      "Epoch 0, Step 1520 | Training Acc: 0.750 | Test Acc: 0.797 | Test Loss: 0.444 | Test AUC 0.756 (726.02 s) \n",
      "Epoch 0, Step 1710 | Training Acc: 0.766 | Test Acc: 0.688 | Test Loss: 0.605 | Test AUC 0.732 (813.00 s) \n",
      "Epoch 0, Step 1900 | Training Acc: 0.805 | Test Acc: 0.711 | Test Loss: 0.536 | Test AUC 0.760 (901.72 s) \n",
      "Epoch 0, Step 2090 | Training Acc: 0.805 | Test Acc: 0.773 | Test Loss: 0.405 | Test AUC 0.764 (989.30 s) \n",
      "Epoch 0, Step 2280 | Training Acc: 0.852 | Test Acc: 0.781 | Test Loss: 0.451 | Test AUC 0.804 (1078.23 s) \n",
      "Epoch 0, Step 2470 | Training Acc: 0.898 | Test Acc: 0.852 | Test Loss: 0.393 | Test AUC 0.868 (1167.44 s) \n",
      "Epoch 0, Step 2660 | Training Acc: 0.812 | Test Acc: 0.812 | Test Loss: 0.458 | Test AUC 0.801 (1255.74 s) \n",
      "Epoch 0, Step 2850 | Training Acc: 0.812 | Test Acc: 0.812 | Test Loss: 0.432 | Test AUC 0.805 (1343.55 s) \n",
      "Epoch 0, Step 3040 | Training Acc: 0.781 | Test Acc: 0.766 | Test Loss: 0.520 | Test AUC 0.750 (1431.09 s) \n",
      "Epoch 0, Step 3230 | Training Acc: 0.805 | Test Acc: 0.773 | Test Loss: 0.433 | Test AUC 0.797 (1518.64 s) \n",
      "Epoch 0, Step 3420 | Training Acc: 0.742 | Test Acc: 0.797 | Test Loss: 0.460 | Test AUC 0.764 (1605.80 s) \n",
      "Epoch 0, Step 3610 | Training Acc: 0.867 | Test Acc: 0.672 | Test Loss: 0.555 | Test AUC 0.740 (1692.17 s) \n",
      "Epoch 0, Step 3800 | Training Acc: 0.773 | Test Acc: 0.805 | Test Loss: 0.401 | Test AUC 0.781 (1779.93 s) \n",
      "(*) Entering Epoch 1 (1781.835 s)\n",
      "Epoch 1, Step 190 | Training Acc: 0.820 | Test Acc: 0.875 | Test Loss: 0.319 | Test AUC 0.842 (1872.18 s) \n",
      "Epoch 1, Step 380 | Training Acc: 0.875 | Test Acc: 0.828 | Test Loss: 0.398 | Test AUC 0.839 (1959.85 s) \n",
      "Epoch 1, Step 570 | Training Acc: 0.797 | Test Acc: 0.875 | Test Loss: 0.326 | Test AUC 0.828 (2046.77 s) \n",
      "Epoch 1, Step 760 | Training Acc: 0.781 | Test Acc: 0.812 | Test Loss: 0.465 | Test AUC 0.793 (2133.45 s) \n",
      "Epoch 1, Step 950 | Training Acc: 0.758 | Test Acc: 0.859 | Test Loss: 0.316 | Test AUC 0.803 (2220.47 s) \n",
      "Epoch 1, Step 1140 | Training Acc: 0.828 | Test Acc: 0.789 | Test Loss: 0.488 | Test AUC 0.808 (2308.62 s) \n",
      "Epoch 1, Step 1330 | Training Acc: 0.773 | Test Acc: 0.859 | Test Loss: 0.316 | Test AUC 0.794 (2396.94 s) \n",
      "Epoch 1, Step 1520 | Training Acc: 0.875 | Test Acc: 0.781 | Test Loss: 0.468 | Test AUC 0.822 (2485.50 s) \n",
      "Epoch 1, Step 1710 | Training Acc: 0.836 | Test Acc: 0.867 | Test Loss: 0.351 | Test AUC 0.845 (2572.83 s) \n",
      "Epoch 1, Step 1900 | Training Acc: 0.922 | Test Acc: 0.789 | Test Loss: 0.468 | Test AUC 0.838 (2660.85 s) \n",
      "Epoch 1, Step 2090 | Training Acc: 0.844 | Test Acc: 0.750 | Test Loss: 0.465 | Test AUC 0.787 (2749.18 s) \n",
      "Epoch 1, Step 2280 | Training Acc: 0.859 | Test Acc: 0.797 | Test Loss: 0.382 | Test AUC 0.824 (2836.12 s) \n",
      "Epoch 1, Step 2470 | Training Acc: 0.844 | Test Acc: 0.820 | Test Loss: 0.409 | Test AUC 0.827 (2923.29 s) \n",
      "Epoch 1, Step 2660 | Training Acc: 0.914 | Test Acc: 0.812 | Test Loss: 0.425 | Test AUC 0.841 (3011.03 s) \n",
      "Epoch 1, Step 2850 | Training Acc: 0.844 | Test Acc: 0.805 | Test Loss: 0.440 | Test AUC 0.827 (3099.56 s) \n",
      "Epoch 1, Step 3040 | Training Acc: 0.844 | Test Acc: 0.805 | Test Loss: 0.426 | Test AUC 0.823 (3186.59 s) \n",
      "Epoch 1, Step 3230 | Training Acc: 0.805 | Test Acc: 0.797 | Test Loss: 0.438 | Test AUC 0.799 (3273.33 s) \n",
      "Epoch 1, Step 3420 | Training Acc: 0.805 | Test Acc: 0.812 | Test Loss: 0.431 | Test AUC 0.804 (3359.21 s) \n",
      "Epoch 1, Step 3610 | Training Acc: 0.812 | Test Acc: 0.789 | Test Loss: 0.409 | Test AUC 0.782 (3445.49 s) \n",
      "Epoch 1, Step 3800 | Training Acc: 0.844 | Test Acc: 0.797 | Test Loss: 0.406 | Test AUC 0.808 (3534.67 s) \n",
      "(*) Entering Epoch 2 (3534.669 s)\n",
      "Epoch 2, Step 190 | Training Acc: 0.867 | Test Acc: 0.852 | Test Loss: 0.368 | Test AUC 0.862 (3621.40 s) \n",
      "Epoch 2, Step 380 | Training Acc: 0.820 | Test Acc: 0.844 | Test Loss: 0.402 | Test AUC 0.815 (3707.84 s) \n",
      "Epoch 2, Step 570 | Training Acc: 0.836 | Test Acc: 0.664 | Test Loss: 0.566 | Test AUC 0.757 (3794.60 s) \n",
      "Epoch 2, Step 760 | Training Acc: 0.789 | Test Acc: 0.781 | Test Loss: 0.449 | Test AUC 0.764 (3881.47 s) \n",
      "Epoch 2, Step 950 | Training Acc: 0.781 | Test Acc: 0.758 | Test Loss: 0.490 | Test AUC 0.749 (3968.21 s) \n",
      "Epoch 2, Step 1140 | Training Acc: 0.781 | Test Acc: 0.758 | Test Loss: 0.499 | Test AUC 0.760 (4056.19 s) \n",
      "Epoch 2, Step 1330 | Training Acc: 0.820 | Test Acc: 0.836 | Test Loss: 0.336 | Test AUC 0.811 (4142.39 s) \n",
      "Epoch 2, Step 1520 | Training Acc: 0.797 | Test Acc: 0.852 | Test Loss: 0.349 | Test AUC 0.811 (4229.53 s) \n",
      "Epoch 2, Step 1710 | Training Acc: 0.844 | Test Acc: 0.867 | Test Loss: 0.339 | Test AUC 0.849 (4316.38 s) \n",
      "Epoch 2, Step 1900 | Training Acc: 0.805 | Test Acc: 0.812 | Test Loss: 0.416 | Test AUC 0.801 (4402.61 s) \n",
      "Epoch 2, Step 2090 | Training Acc: 0.844 | Test Acc: 0.859 | Test Loss: 0.310 | Test AUC 0.803 (4486.49 s) \n",
      "Epoch 2, Step 2280 | Training Acc: 0.828 | Test Acc: 0.812 | Test Loss: 0.449 | Test AUC 0.785 (4572.62 s) \n",
      "Epoch 2, Step 2470 | Training Acc: 0.844 | Test Acc: 0.789 | Test Loss: 0.486 | Test AUC 0.795 (4658.70 s) \n",
      "Epoch 2, Step 2660 | Training Acc: 0.828 | Test Acc: 0.828 | Test Loss: 0.388 | Test AUC 0.823 (4745.68 s) \n",
      "Epoch 2, Step 2850 | Training Acc: 0.906 | Test Acc: 0.766 | Test Loss: 0.448 | Test AUC 0.809 (4832.91 s) \n",
      "Epoch 2, Step 3040 | Training Acc: 0.836 | Test Acc: 0.828 | Test Loss: 0.395 | Test AUC 0.821 (4920.26 s) \n",
      "Epoch 2, Step 3230 | Training Acc: 0.836 | Test Acc: 0.820 | Test Loss: 0.427 | Test AUC 0.817 (5006.87 s) \n",
      "Epoch 2, Step 3420 | Training Acc: 0.922 | Test Acc: 0.852 | Test Loss: 0.376 | Test AUC 0.877 (5093.31 s) \n",
      "Epoch 2, Step 3610 | Training Acc: 0.805 | Test Acc: 0.820 | Test Loss: 0.394 | Test AUC 0.795 (5179.97 s) \n",
      "Epoch 2, Step 3800 | Training Acc: 0.828 | Test Acc: 0.781 | Test Loss: 0.431 | Test AUC 0.774 (5266.68 s) \n",
      "(*) Entering Epoch 3 (5266.684 s)\n",
      "Epoch 3, Step 190 | Training Acc: 0.859 | Test Acc: 0.805 | Test Loss: 0.408 | Test AUC 0.812 (5353.04 s) \n",
      "Epoch 3, Step 380 | Training Acc: 0.875 | Test Acc: 0.641 | Test Loss: 0.677 | Test AUC 0.730 (5439.02 s) \n",
      "Epoch 3, Step 570 | Training Acc: 0.828 | Test Acc: 0.805 | Test Loss: 0.429 | Test AUC 0.792 (5524.80 s) \n",
      "Epoch 3, Step 760 | Training Acc: 0.859 | Test Acc: 0.734 | Test Loss: 0.494 | Test AUC 0.773 (5610.63 s) \n",
      "Epoch 3, Step 950 | Training Acc: 0.836 | Test Acc: 0.773 | Test Loss: 0.485 | Test AUC 0.757 (5696.71 s) \n",
      "Epoch 3, Step 1140 | Training Acc: 0.812 | Test Acc: 0.758 | Test Loss: 0.537 | Test AUC 0.746 (5782.36 s) \n",
      "Epoch 3, Step 1330 | Training Acc: 0.820 | Test Acc: 0.828 | Test Loss: 0.392 | Test AUC 0.805 (5867.82 s) \n",
      "Epoch 3, Step 1520 | Training Acc: 0.859 | Test Acc: 0.797 | Test Loss: 0.421 | Test AUC 0.822 (5953.92 s) \n",
      "Epoch 3, Step 1710 | Training Acc: 0.859 | Test Acc: 0.828 | Test Loss: 0.385 | Test AUC 0.817 (6039.70 s) \n",
      "Epoch 3, Step 1900 | Training Acc: 0.852 | Test Acc: 0.734 | Test Loss: 0.523 | Test AUC 0.762 (6125.21 s) \n",
      "Epoch 3, Step 2090 | Training Acc: 0.836 | Test Acc: 0.859 | Test Loss: 0.350 | Test AUC 0.834 (6210.90 s) \n",
      "Epoch 3, Step 2280 | Training Acc: 0.812 | Test Acc: 0.836 | Test Loss: 0.342 | Test AUC 0.818 (6295.70 s) \n",
      "Epoch 3, Step 2470 | Training Acc: 0.820 | Test Acc: 0.750 | Test Loss: 0.510 | Test AUC 0.755 (6381.82 s) \n",
      "Epoch 3, Step 2660 | Training Acc: 0.883 | Test Acc: 0.773 | Test Loss: 0.428 | Test AUC 0.830 (6468.86 s) \n",
      "Epoch 3, Step 2850 | Training Acc: 0.820 | Test Acc: 0.758 | Test Loss: 0.508 | Test AUC 0.770 (6554.60 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Step 3040 | Training Acc: 0.797 | Test Acc: 0.852 | Test Loss: 0.319 | Test AUC 0.816 (6640.22 s) \n",
      "Epoch 3, Step 3230 | Training Acc: 0.891 | Test Acc: 0.797 | Test Loss: 0.410 | Test AUC 0.840 (6726.32 s) \n",
      "Epoch 3, Step 3420 | Training Acc: 0.844 | Test Acc: 0.812 | Test Loss: 0.409 | Test AUC 0.817 (6811.02 s) \n",
      "Epoch 3, Step 3610 | Training Acc: 0.844 | Test Acc: 0.828 | Test Loss: 0.413 | Test AUC 0.827 (6897.53 s) \n",
      "Epoch 3, Step 3800 | Training Acc: 0.828 | Test Acc: 0.820 | Test Loss: 0.415 | Test AUC 0.807 (6985.00 s) \n",
      "(*) Entering Epoch 4 (6984.999 s)\n",
      "Epoch 4, Step 190 | Training Acc: 0.836 | Test Acc: 0.852 | Test Loss: 0.287 | Test AUC 0.829 (7068.81 s) \n",
      "Epoch 4, Step 380 | Training Acc: 0.812 | Test Acc: 0.867 | Test Loss: 0.367 | Test AUC 0.818 (7153.14 s) \n",
      "Epoch 4, Step 570 | Training Acc: 0.883 | Test Acc: 0.859 | Test Loss: 0.393 | Test AUC 0.868 (7238.30 s) \n",
      "Epoch 4, Step 760 | Training Acc: 0.867 | Test Acc: 0.805 | Test Loss: 0.426 | Test AUC 0.827 (7323.38 s) \n",
      "Epoch 4, Step 950 | Training Acc: 0.867 | Test Acc: 0.664 | Test Loss: 0.688 | Test AUC 0.720 (7409.51 s) \n",
      "Epoch 4, Step 1140 | Training Acc: 0.828 | Test Acc: 0.797 | Test Loss: 0.422 | Test AUC 0.812 (7495.38 s) \n",
      "Epoch 4, Step 1330 | Training Acc: 0.852 | Test Acc: 0.836 | Test Loss: 0.386 | Test AUC 0.831 (7581.27 s) \n",
      "Epoch 4, Step 1520 | Training Acc: 0.867 | Test Acc: 0.805 | Test Loss: 0.443 | Test AUC 0.827 (7667.47 s) \n",
      "Epoch 4, Step 1710 | Training Acc: 0.820 | Test Acc: 0.867 | Test Loss: 0.314 | Test AUC 0.833 (7753.59 s) \n",
      "Epoch 4, Step 1900 | Training Acc: 0.867 | Test Acc: 0.688 | Test Loss: 0.528 | Test AUC 0.737 (7839.26 s) \n",
      "Epoch 4, Step 2090 | Training Acc: 0.883 | Test Acc: 0.773 | Test Loss: 0.469 | Test AUC 0.815 (7924.79 s) \n"
     ]
    }
   ],
   "source": [
    "train(config)#, restore = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "Classification on a new instance is given by the softmax of the output of the final readout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "vDNN = vanillaDNN(config, training = False)\n",
    "network_output = VDNN.predict(ckpt)\n",
    "\n",
    "np.save(os.path.join(directories.checkpoints, '{}_{}_y_pred.npy'.format(config.channel, config.mode)), network_output)\n",
    "np.save(os.path.join(directories.checkpoints, '{}_{}_y_test.npy'.format(config.channel, config.mode)), df_y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
