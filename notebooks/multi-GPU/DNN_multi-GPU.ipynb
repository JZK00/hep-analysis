{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DNN-mk-II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Justin Tan\n",
    "\n",
    "Deep neural network. Do anything from MNIST to signal classification. With bells and whistles. Heavy modified from vanilla implementation.\n",
    "\n",
    "June: Added cyclical annealing, parameter exponential moving average. Slight performance boost.\n",
    "\n",
    "Update 26/06: Moved to GPU cluster.\n",
    "\n",
    "July: Multi-GPU version\n",
    "\n",
    "To-do: Model ensemble, benchmark, distributed training (*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jul  4 05:35:01 2017       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           Off  | 0000:05:00.0     Off |                    0 |\r\n",
      "| N/A   45C    P0    60W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  Tesla K80           Off  | 0000:06:00.0     Off |                    0 |\r\n",
      "| N/A   41C    P0    71W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  Tesla K80           Off  | 0000:84:00.0     Off |                    0 |\r\n",
      "| N/A   47C    P0    59W / 149W |      0MiB / 11439MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  Tesla K80           Off  | 0000:85:00.0     Off |                    0 |\r\n",
      "| N/A   38C    P0    74W / 149W |      0MiB / 11439MiB |    100%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID  Type  Process name                               Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time, os\n",
    "\n",
    "class config(object):\n",
    "    # Set network parameters\n",
    "    mode = 'kst'\n",
    "    channel = 'rho0'\n",
    "    keep_prob = 0.8\n",
    "    num_epochs = 256\n",
    "    batch_size = 512\n",
    "    n_layers = 8\n",
    "    hidden_layer_nodes = [1024, 1024, 1024, 512, 512, 512, 256, 256]\n",
    "    learning_rate = 8e-6\n",
    "    lr_epoch_decay = 0.99\n",
    "    ema_decay = 0.999\n",
    "    cycles = 4 # Number of annealing cycles\n",
    "    n_classes = 2\n",
    "    n_gpus = 4\n",
    "    model = 'selu'\n",
    "\n",
    "class directories(object):\n",
    "    data = 'data'\n",
    "    tensorboard = 'tensorboard'\n",
    "    checkpoints = 'checkpoints'\n",
    "    \n",
    "architecture = '{} - {} | Layers: {} | Dropout: {} | Base LR: {} | Epochs: {}'.format(\n",
    "    config.channel, config.mode, config.n_layers, config.keep_prob, config.learning_rate, config.num_epochs)\n",
    "\n",
    "class reader():\n",
    "    # Iterates over data and returns batches\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.df = df\n",
    "        self.batch_size = config.batch_size\n",
    "        self.steps_per_epoch = len(df) // config.batch_size\n",
    "        self.epochs = 0\n",
    "        self.proceed = True\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.df_X = self.df.drop('labels', axis = 1)\n",
    "        self.df_y = self.df['labels']\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.pointer + 1 >= self.steps_per_epoch:\n",
    "            inputs = self.df_X.iloc[self.pointer*batch_size:]\n",
    "            targets = self.df_y.iloc[self.pointer*batch_size:]\n",
    "            self.epochs += 1\n",
    "            self.shuffle()\n",
    "            self.proceed = False\n",
    "            \n",
    "        inputs = self.df_X.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        targets = self.df_y.iloc[self.pointer*batch_size:(self.pointer+1)*batch_size]\n",
    "        self.pointer += 1\n",
    "                \n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for graph construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SELU helper functions\n",
    "import numbers\n",
    "from tensorflow.contrib import layers\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.python.framework import tensor_util\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import random_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.layers import utils\n",
    "\n",
    "\"\"\" When using SELUs you have to keep the following in mind:\n",
    "# (1) scale inputs to zero mean and unit variance\n",
    "# (2) use SELUs\n",
    "# (3) initialize weights with stddev sqrt(1/n)\n",
    "# (4) use SELU dropout\n",
    "\"\"\"\n",
    "    \n",
    "# (1) scale inputs to zero mean and unit variance\n",
    "\n",
    "# (2) use SELUs\n",
    "def selu(x):\n",
    "    with ops.name_scope('elu') as scope:\n",
    "        alpha = 1.6732632423543772848170429916717\n",
    "        scale = 1.0507009873554804934193349852946\n",
    "        return scale*tf.where(x>=0.0, x, alpha*tf.nn.elu(x))\n",
    "    \n",
    "# (3) initialize weights with stddev sqrt(1/n)\n",
    "SELU_initializer = layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')\n",
    "\n",
    "# (4) use this dropout\n",
    "def dropout_selu(x, rate, alpha= -1.7580993408473766, fixedPointMean=0.0, fixedPointVar=1.0,\n",
    "                 noise_shape=None, seed=None, name=None, training=False):\n",
    "    \"\"\"Dropout to a value with rescaling.\"\"\"\n",
    "\n",
    "    def dropout_selu_impl(x, rate, alpha, noise_shape, seed, name):\n",
    "        keep_prob = 1.0 - rate\n",
    "        x = ops.convert_to_tensor(x, name=\"x\")\n",
    "        if isinstance(keep_prob, numbers.Real) and not 0 < keep_prob <= 1:\n",
    "            raise ValueError(\"keep_prob must be a scalar tensor or a float in the \"\n",
    "                                             \"range (0, 1], got %g\" % keep_prob)\n",
    "        keep_prob = ops.convert_to_tensor(keep_prob, dtype=x.dtype, name=\"keep_prob\")\n",
    "        keep_prob.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        alpha = ops.convert_to_tensor(alpha, dtype=x.dtype, name=\"alpha\")\n",
    "        alpha.get_shape().assert_is_compatible_with(tensor_shape.scalar())\n",
    "\n",
    "        if tensor_util.constant_value(keep_prob) == 1:\n",
    "            return x\n",
    "\n",
    "        noise_shape = noise_shape if noise_shape is not None else array_ops.shape(x)\n",
    "        random_tensor = keep_prob\n",
    "        random_tensor += random_ops.random_uniform(noise_shape, seed=seed, dtype=x.dtype)\n",
    "        binary_tensor = math_ops.floor(random_tensor)\n",
    "        ret = x * binary_tensor + alpha * (1-binary_tensor)\n",
    "\n",
    "        a = math_ops.sqrt(fixedPointVar / (keep_prob *((1-keep_prob) * math_ops.pow(alpha-fixedPointMean,2) + fixedPointVar)))\n",
    "\n",
    "        b = fixedPointMean - a * (keep_prob * fixedPointMean + (1 - keep_prob) * alpha)\n",
    "        ret = a * ret + b\n",
    "        ret.set_shape(x.get_shape())\n",
    "        return ret\n",
    "\n",
    "    with ops.name_scope(name, \"dropout\", [x]) as name:\n",
    "        return utils.smart_cond(training,\n",
    "                                lambda: dropout_selu_impl(x, rate, alpha, noise_shape, seed, name),\n",
    "                                lambda: array_ops.identity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(file_name, test_size = 0.05):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    df = pd.read_hdf(file_name, 'df')\n",
    "    df_X_train, df_X_test, df_y_train, df_y_test = train_test_split(df.drop('labels', axis = 1),\n",
    "                                                                    df['labels'], test_size = test_size, random_state=42)\n",
    "    return df_X_train, df_X_test, df_y_train, df_y_test\n",
    "\n",
    "def save_summary(config, delta_t, train_acc, test_acc, test_auc):\n",
    "    import json\n",
    "    summary = {\n",
    "        'Channel': config.channel,\n",
    "        'Mode': config.mode,\n",
    "        'Timestamp': time.strftime('%c'),\n",
    "        'Model': config.model,\n",
    "        'Layers': config.n_layers,\n",
    "        'Batch_size': config.batch_size,\n",
    "        'Dropout': config.keep_prob,\n",
    "        'Epochs': config.num_epochs,\n",
    "        'Time': delta_t,\n",
    "        'Final train acc': train_acc,\n",
    "        'Final test acc': test_acc,\n",
    "        'Final test AUC': test_auc\n",
    "    }\n",
    "    # Writing JSON data\n",
    "    if os.path.isfile('vdnn_summary.json'):\n",
    "        with open('vdnn_summary.json.', 'r+') as f:\n",
    "            new = json.load(f)\n",
    "        new.append(summary)\n",
    "        with open('vdnn_summary.json', 'w') as f:\n",
    "            json.dump(new, f, indent = 4)\n",
    "    else:\n",
    "        with open('vdnn_summary.json', 'w') as f:\n",
    "             json.dump([summary], f, indent = 4)\n",
    "\n",
    "def layer_weights(shape, initializer = tf.contrib.layers.xavier_initializer()):\n",
    "    # Return weight tensor of given shape using Xavier initialization\n",
    "    W = tf.get_variable(\"weights\", shape = shape, initializer=initializer)\n",
    "    return W\n",
    "\n",
    "def layer_biases(shape, init_value = 0.0):\n",
    "    # Return bias tensor of given shape with small initialized constant value\n",
    "    b = tf.get_variable(\"biases\", shape = shape, initializer = tf.constant_initializer(init_value))\n",
    "    return b\n",
    "                \n",
    "\n",
    "def hidden_layer_ops(x, shape, name, keep_prob, activation=tf.nn.relu):\n",
    "    # Add operations to graph to construct hidden layers\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        \n",
    "        # Apply non-linearity. Default is ReLU\n",
    "        actv = activation(tf.matmul(x, weights) + biases)\n",
    "        layer_output = tf.nn.dropout(actv, keep_prob)\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def hidden_SELU_ops(x, shape, name, keep_prob, phase = True):\n",
    "    # Add operations to graph to construct hidden layers\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape, initializer = SELU_initializer)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        \n",
    "        # Apply non-linearity. Default is ReLU\n",
    "        actv = selu(tf.add(tf.matmul(x, weights), biases))\n",
    "        layer_output = dropout_selu(actv, rate = 1 - keep_prob, training = phase)\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def BN_layer_ops(x, shape, name, keep_prob, phase, activation=tf.nn.relu):\n",
    "    # High-level implementation of BN\n",
    "    with tf.variable_scope(name) as scope:\n",
    "         # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        z_BN = tf.matmul(x, weights) + biases\n",
    "        \n",
    "        # Place BN transform before non-linearity - update to TF 1.2!\n",
    "        theta_BN = tf.contrib.layers.batch_norm(z_BN, center=True, scale=True,is_training=phase, \n",
    "                                                decay=0.99, zero_debias_moving_mean=True, scope='bn', fused = True)\n",
    "        BN_actv = activation(theta_BN)\n",
    "        BN_layer_output = tf.nn.dropout(BN_actv, keep_prob)\n",
    "\n",
    "    return BN_layer_output\n",
    "\n",
    "def SELU_BN_layer_ops(x, shape, name, keep_prob, phase):\n",
    "    # High-level implementation of BN\n",
    "    with tf.variable_scope(name) as scope:\n",
    "         # scope.reuse_variables() # otherwise tf.get_variable() checks that already existing vars are not shared by accident\n",
    "        weights = layer_weights(shape = shape)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        z_BN = tf.matmul(x, weights) + biases\n",
    "        \n",
    "        # Place BN transform before non-linearity - update to TF 1.2!\n",
    "        theta_BN = tf.contrib.layers.batch_norm(z_BN, center=True, scale=True,is_training=phase, \n",
    "                                                decay=0.99, zero_debias_moving_mean=True, scope='bn', fused = True)\n",
    "        BN_actv = selu(theta_BN)\n",
    "        BN_layer_output = dropout_selu(BN_actv, rate = 1 - keep_prob, training = phase)\n",
    "\n",
    "    return BN_layer_output\n",
    "\n",
    "def readout_ops(x, shape, name, initializer = tf.contrib.layers.xavier_initializer()):\n",
    "    # Don't apply non-linearity, dropout on output layer\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        weights = layer_weights(shape = shape, initializer = initializer)\n",
    "        biases = layer_biases(shape = [shape[1]])\n",
    "        layer_output = tf.matmul(x, weights) + biases\n",
    "        \n",
    "    return layer_output\n",
    "\n",
    "def build_network(x, n_layers, hidden_layer_nodes, keep_prob, training_phase):\n",
    "    assert n_layers == len(hidden_layer_nodes), 'Specified layer nodes and number of layers do not correspond.'\n",
    "    layers = [x]\n",
    "    with tf.variable_scope('BN_layers') as scope:\n",
    "        hidden_1 = BN_layer_ops(x, shape = [config.n_features, hidden_layer_nodes[0]], name = 'BNhidden0',\n",
    "                                keep_prob = keep_prob, phase = training_phase)\n",
    "        layers.append(hidden_1)\n",
    "        for n in range(0,n_layers-1):\n",
    "            hidden_n = BN_layer_ops(layers[-1], shape = [hidden_layer_nodes[n], hidden_layer_nodes[n+1]], name = 'BNhidden{}'.format(n+1),\n",
    "                                   keep_prob = keep_prob, phase = training_phase)\n",
    "            layers.append(hidden_n)\n",
    "        readout = readout_ops(layers[-1], shape = [hidden_layer_nodes[-1], config.n_classes], name = 'readout')\n",
    "        \n",
    "    return readout\n",
    "\n",
    "def network_builder(x, n_layers, hidden_layer_nodes, keep_prob, training_phase):\n",
    "    assert n_layers == len(hidden_layer_nodes), 'Specified layer nodes and number of layers do not correspond.'\n",
    "    layers = [x]\n",
    "    if config.model == 'bn':\n",
    "        print('Building ReLU + Batch-norm architecture')\n",
    "        builder = BN_layer_ops\n",
    "    elif config.model == 'selu':\n",
    "        print('Building SELU architecture')\n",
    "        builder = hidden_SELU_ops\n",
    "    elif config.model == 'selu-bn':\n",
    "        print('Building SELU + Batch-norm architecture')\n",
    "        builder = SELU_BN_layer_ops\n",
    "    else:\n",
    "        print('Default architecture: SELU')\n",
    "        builder = hidden_SELU_ops\n",
    "        \n",
    "    with tf.variable_scope('hidden_layers') as scope:\n",
    "        hidden_1 = builder(x, shape = [config.n_features, hidden_layer_nodes[0]], name = 'hidden0',\n",
    "                                keep_prob = keep_prob, phase = training_phase)\n",
    "        layers.append(hidden_1)\n",
    "        for n in range(0,n_layers-1):\n",
    "            hidden_n = builder(layers[-1], shape = [hidden_layer_nodes[n], hidden_layer_nodes[n+1]], name = 'hidden{}'.format(n+1),\n",
    "                                   keep_prob = keep_prob, phase = training_phase)\n",
    "            layers.append(hidden_n)\n",
    "        readout = readout_ops(layers[-1], shape = [hidden_layer_nodes[-1], config.n_classes], name = 'readout', initializer = SELU_initializer)\n",
    "        \n",
    "        return readout\n",
    "\n",
    "def plot_ROC_curve(network_output, y_true, meta = ''):\n",
    "#     import matplotlib as mpl\n",
    "#     mpl.use('pgf')\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    y_score = network_output[:,1]\n",
    "    \n",
    "    # Compute ROC curve, integrate\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score)    \n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.axes([.1,.1,.8,.7])\n",
    "    plt.figtext(.5,.9, r'$\\mathrm{Receiver \\;Operating \\;Characteristic}$', fontsize=15, ha='center')\n",
    "    plt.figtext(.5,.85, meta, fontsize=10,ha='center')\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "                     lw=2, label='ROC (area = %0.3f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=1.0, linestyle='--')\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(r'$\\mathrm{False \\;Positive \\;Rate}$')\n",
    "    plt.ylabel(r'$\\mathrm{True \\;Positive \\;Rate}$')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join('graphs', '{}_{}_ROC.pdf'.format(config.channel, config.mode)), format='pdf', dpi=1000)\n",
    "    #plt.savefig(os.path.join('graphs', '{}_{}_ROC.pgf'.format(config.channel, config.mode)), format='pgf', dpi=1000)\n",
    "    print('AUC: {:g}'.format(roc_auc))\n",
    "    plt.show()\n",
    "    plt.gcf().clear()\n",
    "    return roc_auc\n",
    "    \n",
    "def cosine_anneal(initial_lr, t, T, M):\n",
    "    from math import ceil\n",
    "    beta = initial_lr/2 * (np.cos(np.pi* (t % ceil(T/M))/ceil(T/M)) + 1)\n",
    "    return beta\n",
    "\n",
    "def tower_computation(scope, inputs, labels, keep_prob, training_phase, n_gpu):\n",
    "    \"\"\" Calculate the total loss on a single computation tower.\n",
    "    Args:\n",
    "        scope: unique prefix string identifying the tower, e.g. 'tower_0'\n",
    "        inputs: 2D tensor of shape [batch_size, n_features]\n",
    "        labels: 1D tensor of shape [batch_size]\n",
    "    Returns:\n",
    "        cross_entropy: Tensor containing the total loss for a batch of data\n",
    "        readout: logits of readout layer\n",
    "    \"\"\"\n",
    "    # Build inference graph\n",
    "    readout = network_builder(inputs, config.n_layers, config.hidden_layer_nodes, keep_prob, training_phase)\n",
    "    # Get losses - try L2 loss?\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = readout, labels = labels))\n",
    "    tf.add_to_collection('losses_collection', cross_entropy)\n",
    "    \n",
    "    # Assemble all of the losses for the current tower only.\n",
    "    losses = tf.get_collection('losses_collection', scope)\n",
    "    \n",
    "    # Compute the moving average of all individual losses and the total loss.\n",
    "#     loss_averages = tf.train.ExponentialMovingAverage(0.9, name='avg')\n",
    "#     loss_averages_op = loss_averages.apply(losses)\n",
    "\n",
    "    for l in losses:\n",
    "        tf.summary.scalar('xentropy_{}-raw'.format(n_gpu), l)\n",
    "        #tf.summary.scalar('xentropy_{}'.format(n_gpu), loss_averages.average(l))\n",
    "        \n",
    "    return cross_entropy, readout\n",
    "\n",
    "def average_gradients(tower_grads):\n",
    "    \"\"\" Calculate the average gradient for each shared variable across all towers.\n",
    "    Args:\n",
    "    tower_grads: Nested list of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    average_grads = []\n",
    "    for grad_var_pair in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_var_pair:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_var_pair[0][1]\n",
    "        gv_pair = (grad, v)\n",
    "        average_grads.append(gv_pair)\n",
    "        \n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_file = '/data/projects/punim0011/jtan/data/dnn/norm_std_dnn_B02rho0gamma_kst.h5'\n",
    "assert config.batch_size % config.n_gpus == 0, 'Batch size must be divisible by number of GPUs'\n",
    "assert config.n_layers == len(config.hidden_layer_nodes), 'Inconsistent number of hidden layers'\n",
    "\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = load_data(test_file)\n",
    "df_y_train = df_y_train.astype(np.int8)\n",
    "df_y_test = df_y_test.astype(np.int8)\n",
    "df_train = pd.concat([df_X_train, df_y_train], axis = 1)\n",
    "df_test = pd.concat([df_X_test, df_y_test], axis = 1)\n",
    "config.n_features = df_train.shape[1]-1\n",
    "config.steps_per_epoch = df_train.shape[0] // config.batch_size\n",
    "config.T = config.steps_per_epoch*config.num_epochs\n",
    "\n",
    "readerTrain = reader(df_train)\n",
    "readerTest = reader(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vanillaDNN():\n",
    "    # Builds the computational graph\n",
    "    def __init__(self, config, training = True):\n",
    "        self.x = tf.placeholder(tf.float32, shape = [None, config.n_features])\n",
    "        self.y_true = tf.placeholder(tf.int32, shape = None)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        self.training_phase = tf.placeholder(tf.bool)\n",
    "        self.beta = tf.placeholder(tf.float32)\n",
    "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n",
    "        beta = tf.train.exponential_decay(config.learning_rate, self.global_step, \n",
    "                                          decay_steps = config.steps_per_epoch, decay_rate = config.lr_epoch_decay, staircase=True)\n",
    "\n",
    "        input_batches = tf.split(self.x, config.n_gpus, axis = 0)\n",
    "        label_batches = tf.split(self.y_true, config.n_gpus, axis = 0)\n",
    "        opt = tf.train.AdamOptimizer(config.learning_rate)\n",
    "\n",
    "        # Calculate gradients for each model tower\n",
    "        tower_grads, tower_readouts, tower_losses, tower_summaries = [], [], [], []\n",
    "        for gpu in range(config.n_gpus):\n",
    "            with tf.device('/gpu:{}'.format(gpu)):\n",
    "                with tf.variable_scope('vDNN', reuse=(gpu > 0)):\n",
    "                    with tf.name_scope('tower_{}'.format(gpu)) as scope:\n",
    "                        # Load one batch per GPU\n",
    "                        input_batch, label_batch = input_batches[gpu], label_batches[gpu]\n",
    "\n",
    "                        # Calculate loss for one tower of the model. Construct the entire model,\n",
    "                        # but share the variable across all towers\n",
    "                        loss, readout = tower_computation(scope, input_batch, label_batch, self.keep_prob, self.training_phase, gpu)\n",
    "                        # Reuse variables for the next tower, retain the summaries from the final tower.\n",
    "                        #tf.get_variable_scope().reuse_variables()\n",
    "                        summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n",
    "\n",
    "                        # Retain batch norm update operations only from the final tower.\n",
    "                        # batchnorm_updates = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope)\n",
    "\n",
    "                        # Calculate the gradients for given batch on this tower\n",
    "                        grads = opt.compute_gradients(loss)\n",
    "                        tower_grads.append(grads)\n",
    "                        tower_readouts.append(readout)\n",
    "                        tower_summaries.append(summaries)\n",
    "                        tower_losses.append(loss)\n",
    "\n",
    "        # Synchronize all towers\n",
    "        mean_grads = average_gradients(tower_grads)\n",
    "        self.readout = tf.concat(tower_readouts, axis = 0)\n",
    "\n",
    "        # Evaluation metrics\n",
    "        self.cross_entropy = tf.reduce_mean(tower_losses)\n",
    "        self.prediction = tf.nn.softmax(self.readout)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(self.readout, 1), tf.int32), self.y_true)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        _, self.auc_op = tf.metrics.auc(predictions = tf.argmax(self.readout,1), labels = self.y_true, num_thresholds = 512)\n",
    "\n",
    "        # Track moving average of trainable variables\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay = config.ema_decay, num_updates = self.global_step)\n",
    "        maintain_averages_op = self.ema.apply(tf.trainable_variables())\n",
    "\n",
    "        # Apply the gradients to adjust the shared variables.\n",
    "        apply_gradient_op = opt.apply_gradients(mean_grads, global_step=self.global_step)\n",
    "\n",
    "        # Group all updates to into a single train op.\n",
    "        #batchnorm_updates_op = tf.group(*batchnorm_updates)\n",
    "        self.train_op = tf.group(apply_gradient_op, maintain_averages_op)#, batchnorm_updates_op)\n",
    "\n",
    "        saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "        # Build the summary operation from the last tower summaries\n",
    "        tower_summaries.append(tf.summary.scalar('cross_entropy', self.cross_entropy))\n",
    "        tower_summaries.append(tf.summary.scalar('accuracy', self.accuracy))\n",
    "        tower_summaries.append(tf.summary.scalar('auc', self.auc_op))\n",
    "        tower_summaries.append(tf.summary.scalar('global_step', self.global_step))\n",
    "        tower_summaries.append(tf.summary.scalar('learning_rate', beta))\n",
    "        self.merge_op = tf.summary.merge(tower_summaries)\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'train_{}'.format(time.strftime('%d-%m_%I:%M'))), graph = tf.get_default_graph())\n",
    "        self.test_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'test_{}'.format(time.strftime('%d-%m_%I:%M'))))\n",
    "    \n",
    "    # Make predictions\n",
    "    def predict(self, ckpt, metaGraph = None):\n",
    "        pin_cpu = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True, device_count = {'GPU':0})\n",
    "    \n",
    "        # Restore the moving average version of the learned variables for eval.\n",
    "        #variable_averages = tf.train.ExponentialMovingAverage(config.ema_decay)\n",
    "        variables_to_restore = self.ema.variables_to_restore()\n",
    "        #variables_to_restore = variable_averages.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        \n",
    "        with tf.Session(config=pin_cpu) as sess:\n",
    "            # Initialize variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            start_time = time.time()\n",
    "            \n",
    "            assert (ckpt.model_checkpoint_path or metaGraph), 'Missing checkpoint file!'\n",
    "            \n",
    "            if metaGraph:\n",
    "                saver = tf.train.import_meta_graph(metaGraph)\n",
    "                saver.restore(sess, os.path.splitext(metaGraph)[0])\n",
    "                print('{} restored.'.format(metaGraph))\n",
    "            else:    \n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "                print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "\n",
    "            # Make predictions using the trained model\n",
    "            feed_dict_test = {self.x: df_X_test.values, self.y_true: df_y_test.values, self.keep_prob: 1.0, self.training_phase: False}\n",
    "            network_output_test, final_v_acc, final_v_auc = sess.run(\n",
    "                [self.prediction, self.accuracy, self.auc_op], feed_dict = feed_dict_test)\n",
    "\n",
    "            print(\"Validation accuracy: {:g}\\nValidation AUC: {:g}\".format(final_v_acc, final_v_auc))\n",
    "            \n",
    "            plot_ROC_curve(network_output = network_output_test, y_true = df_y_test.values,\n",
    "                           meta = architecture + ' | Test accuracy: {}'.format(final_v_acc))\n",
    "            delta_t = time.time() - start_time\n",
    "            print(\"Inference complete. Duration: %g s\" %(delta_t))\n",
    "            \n",
    "            return network_output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, restore = False):\n",
    "    # Executes training operations\n",
    "    vDNN = vanillaDNN(config, training = True)\n",
    "    start_time = time.time()\n",
    "    v_acc_best = 0.\n",
    "    global_step = 0\n",
    "    global_epoch = 0\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        # Initialize variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        if restore and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            global_epoch = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]\n",
    "            assert (type(global_epoch) == int), 'Epoch number untracked'\n",
    "            print('{} restored at epoch {}.'.format(ckpt.model_checkpoint_path, global_epoch))              \n",
    "            \n",
    "        for epoch in range(global_epoch,config.num_epochs):\n",
    "            \n",
    "            readerTrain.proceed = True\n",
    "            step = 0\n",
    "            # Save every 10 epochs    \n",
    "            if epoch % 10 == 0:\n",
    "                save_path = saver.save(sess,\n",
    "                                       os.path.join(directories.checkpoints,\n",
    "                                                    'vDNN_{}_{}_epoch{}.ckpt'.format(config.mode, config.channel, epoch)),\n",
    "                                       global_step = epoch)\n",
    "                print('Graph saved to file: {}'.format(save_path))\n",
    "                \n",
    "            print('(*) Entering Epoch {} ({:.3f} s)'.format(epoch, time.time() - start_time))\n",
    "            \n",
    "            while(readerTrain.proceed):\n",
    "                # Iterate through entire corpus\n",
    "                x_train, y_train = readerTrain.next_batch(config.batch_size)\n",
    "                beta = cosine_anneal(config.learning_rate, global_step, config.T, config.cycles)\n",
    "                feed_dict_train = {vDNN.x: x_train.values, vDNN.y_true: y_train.values, \n",
    "                                   vDNN.keep_prob: config.keep_prob, vDNN.training_phase: True, vDNN.beta: beta}\n",
    "                t_op = sess.run(vDNN.train_op, feed_dict = feed_dict_train)\n",
    "                step += 1\n",
    "                global_step += 1\n",
    "\n",
    "                if step % (config.steps_per_epoch // 5) == 0:            \n",
    "                    # Evaluate model\n",
    "                    improved = ''\n",
    "                    sess.run(tf.local_variables_initializer())\n",
    "                    x_test, y_test = readerTest.next_batch(config.batch_size)\n",
    "                    feed_dict_test = {vDNN.x: x_test.values, vDNN.y_true: y_test.values, vDNN.keep_prob: 1.0,\n",
    "                                      vDNN.training_phase: False, vDNN.beta: 0}\n",
    "\n",
    "                    t_acc, t_summary = sess.run([vDNN.accuracy, vDNN.merge_op],\n",
    "                                                        feed_dict = feed_dict_train)\n",
    "                    v_acc, v_loss, v_auc, v_summary, = sess.run([vDNN.accuracy, vDNN.cross_entropy, vDNN.auc_op, vDNN.merge_op],\n",
    "                                                        feed_dict = feed_dict_test)\n",
    "\n",
    "                    vDNN.train_writer.add_summary(t_summary, step)\n",
    "                    vDNN.test_writer.add_summary(v_summary, step)\n",
    "                    \n",
    "                    if epoch > 8 and v_acc > v_acc_best:\n",
    "                        v_acc_best = v_acc\n",
    "                        improved = '[*]'\n",
    "                        save_path = saver.save(sess, \n",
    "                                               os.path.join(directories.checkpoints,\n",
    "                                                            'vDNN_{}_{}_best.ckpt'.format(config.mode, config.channel)),\n",
    "                                               global_step = epoch)\n",
    "                    \n",
    "                    print('Epoch {}, Step {} | Training Acc: {:.3f} | Test Acc: {:.3f} | Test Loss: {:.3f} | Test AUC {:.3f} ({:.2f} s) {}'\n",
    "                          .format(epoch, step, t_acc, v_acc, v_loss, v_auc, time.time() - start_time, improved))\n",
    "\n",
    "        save_path = saver.save(sess, os.path.join(directories.checkpoints, 'vDNN_{}_{}_end.ckpt'.format(config.mode, config.channel)),\n",
    "                               global_step = epoch)\n",
    "        print('Model saved to file: {}'.format(save_path))\n",
    "\n",
    "# #         end = int(config.n_gpus*round(df_X_train.shape[0]/config.n_gpus))\n",
    "# #         feed_dict_train = {vDNN.x: df_X_train.iloc[:end].values, vDNN.y_true: df_y_train.iloc[:end].values, vDNN.keep_prob: 1.0, vDNN.training_phase: False}\n",
    "# #         feed_dict_test = {vDNN.x: df_X_test.iloc[:end].values, vDNN.y_true: df_y_test.iloc[:end].values, vDNN.keep_prob: 1.0, vDNN.training_phase: False}\n",
    "# #         final_t_acc = vDNN.accuracy.eval(feed_dict = feed_dict_train)\n",
    "# #         final_v_acc, final_v_AUC = sess.run([vDNN.accuracy, vDNN.auc_op], feed_dict = feed_dict_test\n",
    "#     print(\"Train accuracy: {:g}\\nValidation accuracy: {:g}\\nValidation AUC: {:g}\".format(final_t_acc, final_v_acc, final_v_AUC))\n",
    "#     save_summary(config, delta_t, final_t_acc, final_v_acc, final_v_AUC)\n",
    "\n",
    "    delta_t = time.time() - start_time        \n",
    "    print(\"Training Complete. Time elapsed: {:.3f} s\".format(delta_t))\n",
    "    print('Architecture: {}'.format(architecture))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch0.ckpt-0\n",
      "(*) Entering Epoch 0 (11.952 s)\n",
      "Epoch 0, Step 190 | Training Acc: 0.533 | Test Acc: 0.666 | Test Loss: 1.269 | Test AUC 0.612 (16.95 s) \n",
      "Epoch 0, Step 380 | Training Acc: 0.633 | Test Acc: 0.682 | Test Loss: 2.177 | Test AUC 0.667 (20.67 s) \n",
      "Epoch 0, Step 570 | Training Acc: 0.678 | Test Acc: 0.771 | Test Loss: 1.816 | Test AUC 0.730 (24.39 s) \n",
      "Epoch 0, Step 760 | Training Acc: 0.697 | Test Acc: 0.773 | Test Loss: 2.182 | Test AUC 0.738 (28.09 s) \n",
      "Epoch 0, Step 950 | Training Acc: 0.727 | Test Acc: 0.750 | Test Loss: 2.191 | Test AUC 0.741 (32.75 s) \n",
      "(*) Entering Epoch 1 (32.754 s)\n",
      "Epoch 1, Step 190 | Training Acc: 0.742 | Test Acc: 0.744 | Test Loss: 2.095 | Test AUC 0.747 (36.45 s) \n",
      "Epoch 1, Step 380 | Training Acc: 0.699 | Test Acc: 0.764 | Test Loss: 1.925 | Test AUC 0.728 (40.20 s) \n",
      "Epoch 1, Step 570 | Training Acc: 0.727 | Test Acc: 0.779 | Test Loss: 1.710 | Test AUC 0.749 (43.91 s) \n",
      "Epoch 1, Step 760 | Training Acc: 0.709 | Test Acc: 0.773 | Test Loss: 1.689 | Test AUC 0.739 (47.62 s) \n",
      "(*) Entering Epoch 2 (52.081 s)\n",
      "Epoch 2, Step 190 | Training Acc: 0.738 | Test Acc: 0.789 | Test Loss: 1.411 | Test AUC 0.762 (55.78 s) \n",
      "Epoch 2, Step 380 | Training Acc: 0.725 | Test Acc: 0.777 | Test Loss: 1.516 | Test AUC 0.745 (59.50 s) \n",
      "Epoch 2, Step 570 | Training Acc: 0.744 | Test Acc: 0.766 | Test Loss: 1.612 | Test AUC 0.752 (63.20 s) \n",
      "Epoch 2, Step 760 | Training Acc: 0.771 | Test Acc: 0.787 | Test Loss: 1.520 | Test AUC 0.777 (66.92 s) \n",
      "(*) Entering Epoch 3 (71.411 s)\n",
      "Epoch 3, Step 190 | Training Acc: 0.781 | Test Acc: 0.779 | Test Loss: 1.456 | Test AUC 0.781 (75.10 s) \n",
      "Epoch 3, Step 380 | Training Acc: 0.768 | Test Acc: 0.803 | Test Loss: 1.226 | Test AUC 0.785 (78.82 s) \n",
      "Epoch 3, Step 570 | Training Acc: 0.785 | Test Acc: 0.787 | Test Loss: 1.434 | Test AUC 0.782 (82.55 s) \n",
      "Epoch 3, Step 760 | Training Acc: 0.717 | Test Acc: 0.789 | Test Loss: 1.598 | Test AUC 0.748 (86.26 s) \n",
      "(*) Entering Epoch 4 (90.717 s)\n",
      "Epoch 4, Step 190 | Training Acc: 0.754 | Test Acc: 0.795 | Test Loss: 1.396 | Test AUC 0.775 (94.42 s) \n",
      "Epoch 4, Step 380 | Training Acc: 0.762 | Test Acc: 0.818 | Test Loss: 1.127 | Test AUC 0.786 (98.09 s) \n",
      "Epoch 4, Step 570 | Training Acc: 0.783 | Test Acc: 0.787 | Test Loss: 1.441 | Test AUC 0.781 (101.79 s) \n",
      "Epoch 4, Step 760 | Training Acc: 0.779 | Test Acc: 0.844 | Test Loss: 1.025 | Test AUC 0.813 (105.51 s) \n",
      "(*) Entering Epoch 5 (109.932 s)\n",
      "Epoch 5, Step 190 | Training Acc: 0.770 | Test Acc: 0.793 | Test Loss: 1.331 | Test AUC 0.780 (113.64 s) \n",
      "Epoch 5, Step 380 | Training Acc: 0.785 | Test Acc: 0.814 | Test Loss: 1.228 | Test AUC 0.794 (117.35 s) \n",
      "Epoch 5, Step 570 | Training Acc: 0.783 | Test Acc: 0.834 | Test Loss: 1.005 | Test AUC 0.805 (121.04 s) \n",
      "Epoch 5, Step 760 | Training Acc: 0.746 | Test Acc: 0.795 | Test Loss: 1.439 | Test AUC 0.763 (124.77 s) \n",
      "(*) Entering Epoch 6 (129.207 s)\n",
      "Epoch 6, Step 190 | Training Acc: 0.787 | Test Acc: 0.822 | Test Loss: 1.077 | Test AUC 0.799 (132.88 s) \n",
      "Epoch 6, Step 380 | Training Acc: 0.785 | Test Acc: 0.830 | Test Loss: 1.144 | Test AUC 0.799 (136.58 s) \n",
      "Epoch 6, Step 570 | Training Acc: 0.771 | Test Acc: 0.822 | Test Loss: 1.219 | Test AUC 0.787 (140.26 s) \n",
      "Epoch 6, Step 760 | Training Acc: 0.805 | Test Acc: 0.812 | Test Loss: 1.243 | Test AUC 0.799 (143.96 s) \n",
      "(*) Entering Epoch 7 (148.408 s)\n",
      "Epoch 7, Step 190 | Training Acc: 0.791 | Test Acc: 0.830 | Test Loss: 1.087 | Test AUC 0.800 (152.10 s) \n",
      "Epoch 7, Step 380 | Training Acc: 0.791 | Test Acc: 0.770 | Test Loss: 1.546 | Test AUC 0.765 (155.79 s) \n",
      "Epoch 7, Step 570 | Training Acc: 0.807 | Test Acc: 0.838 | Test Loss: 1.077 | Test AUC 0.808 (159.46 s) \n",
      "Epoch 7, Step 760 | Training Acc: 0.797 | Test Acc: 0.809 | Test Loss: 1.150 | Test AUC 0.793 (163.13 s) \n",
      "(*) Entering Epoch 8 (167.574 s)\n",
      "Epoch 8, Step 190 | Training Acc: 0.783 | Test Acc: 0.824 | Test Loss: 1.189 | Test AUC 0.791 (171.28 s) \n",
      "Epoch 8, Step 380 | Training Acc: 0.797 | Test Acc: 0.807 | Test Loss: 1.385 | Test AUC 0.788 (174.96 s) \n",
      "Epoch 8, Step 570 | Training Acc: 0.824 | Test Acc: 0.805 | Test Loss: 1.364 | Test AUC 0.802 (178.65 s) \n",
      "Epoch 8, Step 760 | Training Acc: 0.805 | Test Acc: 0.809 | Test Loss: 1.305 | Test AUC 0.793 (182.34 s) \n",
      "(*) Entering Epoch 9 (186.724 s)\n",
      "Epoch 9, Step 190 | Training Acc: 0.793 | Test Acc: 0.809 | Test Loss: 1.290 | Test AUC 0.785 (190.95 s) [*]\n",
      "Epoch 9, Step 380 | Training Acc: 0.807 | Test Acc: 0.861 | Test Loss: 1.069 | Test AUC 0.821 (195.33 s) [*]\n",
      "Epoch 9, Step 570 | Training Acc: 0.799 | Test Acc: 0.830 | Test Loss: 1.270 | Test AUC 0.797 (199.03 s) \n",
      "Epoch 9, Step 760 | Training Acc: 0.797 | Test Acc: 0.832 | Test Loss: 1.204 | Test AUC 0.802 (202.72 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch10.ckpt-10\n",
      "(*) Entering Epoch 10 (207.809 s)\n",
      "Epoch 10, Step 190 | Training Acc: 0.807 | Test Acc: 0.811 | Test Loss: 1.365 | Test AUC 0.797 (211.50 s) \n",
      "Epoch 10, Step 380 | Training Acc: 0.828 | Test Acc: 0.838 | Test Loss: 1.066 | Test AUC 0.819 (215.20 s) \n",
      "Epoch 10, Step 570 | Training Acc: 0.801 | Test Acc: 0.820 | Test Loss: 1.371 | Test AUC 0.801 (218.88 s) \n",
      "Epoch 10, Step 760 | Training Acc: 0.836 | Test Acc: 0.824 | Test Loss: 1.268 | Test AUC 0.812 (222.57 s) \n",
      "(*) Entering Epoch 11 (227.011 s)\n",
      "Epoch 11, Step 190 | Training Acc: 0.816 | Test Acc: 0.846 | Test Loss: 1.066 | Test AUC 0.820 (230.70 s) \n",
      "Epoch 11, Step 380 | Training Acc: 0.799 | Test Acc: 0.834 | Test Loss: 1.127 | Test AUC 0.800 (234.36 s) \n",
      "Epoch 11, Step 570 | Training Acc: 0.834 | Test Acc: 0.852 | Test Loss: 0.997 | Test AUC 0.817 (238.03 s) \n",
      "Epoch 11, Step 760 | Training Acc: 0.803 | Test Acc: 0.820 | Test Loss: 1.302 | Test AUC 0.799 (241.72 s) \n",
      "(*) Entering Epoch 12 (246.172 s)\n",
      "Epoch 12, Step 190 | Training Acc: 0.820 | Test Acc: 0.834 | Test Loss: 1.180 | Test AUC 0.813 (249.92 s) \n",
      "Epoch 12, Step 380 | Training Acc: 0.822 | Test Acc: 0.822 | Test Loss: 1.244 | Test AUC 0.803 (253.62 s) \n",
      "Epoch 12, Step 570 | Training Acc: 0.832 | Test Acc: 0.832 | Test Loss: 1.249 | Test AUC 0.819 (257.31 s) \n",
      "Epoch 12, Step 760 | Training Acc: 0.779 | Test Acc: 0.816 | Test Loss: 1.371 | Test AUC 0.786 (261.00 s) \n",
      "(*) Entering Epoch 13 (265.414 s)\n",
      "Epoch 13, Step 190 | Training Acc: 0.828 | Test Acc: 0.838 | Test Loss: 1.138 | Test AUC 0.818 (269.11 s) \n",
      "Epoch 13, Step 380 | Training Acc: 0.791 | Test Acc: 0.832 | Test Loss: 1.214 | Test AUC 0.799 (272.79 s) \n",
      "Epoch 13, Step 570 | Training Acc: 0.842 | Test Acc: 0.830 | Test Loss: 1.328 | Test AUC 0.819 (276.49 s) \n",
      "Epoch 13, Step 760 | Training Acc: 0.842 | Test Acc: 0.844 | Test Loss: 1.103 | Test AUC 0.830 (280.20 s) \n",
      "(*) Entering Epoch 14 (284.637 s)\n",
      "Epoch 14, Step 190 | Training Acc: 0.824 | Test Acc: 0.834 | Test Loss: 1.288 | Test AUC 0.812 (288.33 s) \n",
      "Epoch 14, Step 380 | Training Acc: 0.836 | Test Acc: 0.822 | Test Loss: 1.108 | Test AUC 0.815 (292.00 s) \n",
      "Epoch 14, Step 570 | Training Acc: 0.803 | Test Acc: 0.859 | Test Loss: 0.916 | Test AUC 0.821 (295.68 s) \n",
      "Epoch 14, Step 760 | Training Acc: 0.795 | Test Acc: 0.840 | Test Loss: 1.147 | Test AUC 0.802 (299.35 s) \n",
      "(*) Entering Epoch 15 (303.786 s)\n",
      "Epoch 15, Step 190 | Training Acc: 0.795 | Test Acc: 0.842 | Test Loss: 1.181 | Test AUC 0.807 (307.48 s) \n",
      "Epoch 15, Step 380 | Training Acc: 0.809 | Test Acc: 0.826 | Test Loss: 1.153 | Test AUC 0.802 (311.19 s) \n",
      "Epoch 15, Step 570 | Training Acc: 0.811 | Test Acc: 0.807 | Test Loss: 1.432 | Test AUC 0.788 (314.87 s) \n",
      "Epoch 15, Step 760 | Training Acc: 0.816 | Test Acc: 0.838 | Test Loss: 1.219 | Test AUC 0.811 (318.58 s) \n",
      "(*) Entering Epoch 16 (323.021 s)\n",
      "Epoch 16, Step 190 | Training Acc: 0.838 | Test Acc: 0.873 | Test Loss: 1.022 | Test AUC 0.843 (327.28 s) [*]\n",
      "Epoch 16, Step 380 | Training Acc: 0.826 | Test Acc: 0.803 | Test Loss: 1.495 | Test AUC 0.802 (330.98 s) \n",
      "Epoch 16, Step 570 | Training Acc: 0.832 | Test Acc: 0.818 | Test Loss: 1.316 | Test AUC 0.813 (334.66 s) \n",
      "Epoch 16, Step 760 | Training Acc: 0.830 | Test Acc: 0.846 | Test Loss: 1.180 | Test AUC 0.826 (338.34 s) \n",
      "(*) Entering Epoch 17 (342.784 s)\n",
      "Epoch 17, Step 190 | Training Acc: 0.795 | Test Acc: 0.836 | Test Loss: 1.186 | Test AUC 0.803 (346.47 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Step 380 | Training Acc: 0.805 | Test Acc: 0.836 | Test Loss: 1.337 | Test AUC 0.809 (350.16 s) \n",
      "Epoch 17, Step 570 | Training Acc: 0.816 | Test Acc: 0.822 | Test Loss: 1.265 | Test AUC 0.804 (353.85 s) \n",
      "Epoch 17, Step 760 | Training Acc: 0.842 | Test Acc: 0.855 | Test Loss: 0.867 | Test AUC 0.835 (357.53 s) \n",
      "(*) Entering Epoch 18 (361.964 s)\n",
      "Epoch 18, Step 190 | Training Acc: 0.816 | Test Acc: 0.816 | Test Loss: 1.334 | Test AUC 0.807 (365.65 s) \n",
      "Epoch 18, Step 380 | Training Acc: 0.828 | Test Acc: 0.809 | Test Loss: 1.493 | Test AUC 0.800 (369.33 s) \n",
      "Epoch 18, Step 570 | Training Acc: 0.857 | Test Acc: 0.863 | Test Loss: 1.049 | Test AUC 0.849 (373.03 s) \n",
      "Epoch 18, Step 760 | Training Acc: 0.795 | Test Acc: 0.822 | Test Loss: 1.543 | Test AUC 0.789 (376.70 s) \n",
      "(*) Entering Epoch 19 (381.157 s)\n",
      "Epoch 19, Step 190 | Training Acc: 0.801 | Test Acc: 0.840 | Test Loss: 1.118 | Test AUC 0.807 (384.88 s) \n",
      "Epoch 19, Step 380 | Training Acc: 0.832 | Test Acc: 0.838 | Test Loss: 1.293 | Test AUC 0.822 (388.59 s) \n",
      "Epoch 19, Step 570 | Training Acc: 0.818 | Test Acc: 0.840 | Test Loss: 1.432 | Test AUC 0.815 (392.28 s) \n",
      "Epoch 19, Step 760 | Training Acc: 0.838 | Test Acc: 0.840 | Test Loss: 1.479 | Test AUC 0.825 (395.95 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch20.ckpt-20\n",
      "(*) Entering Epoch 20 (400.965 s)\n",
      "Epoch 20, Step 190 | Training Acc: 0.836 | Test Acc: 0.836 | Test Loss: 1.289 | Test AUC 0.820 (404.66 s) \n",
      "Epoch 20, Step 380 | Training Acc: 0.818 | Test Acc: 0.855 | Test Loss: 1.068 | Test AUC 0.825 (408.35 s) \n",
      "Epoch 20, Step 570 | Training Acc: 0.850 | Test Acc: 0.826 | Test Loss: 1.365 | Test AUC 0.826 (412.07 s) \n",
      "Epoch 20, Step 760 | Training Acc: 0.820 | Test Acc: 0.803 | Test Loss: 1.445 | Test AUC 0.790 (415.76 s) \n",
      "(*) Entering Epoch 21 (420.206 s)\n",
      "Epoch 21, Step 190 | Training Acc: 0.832 | Test Acc: 0.836 | Test Loss: 1.323 | Test AUC 0.824 (423.85 s) \n",
      "Epoch 21, Step 380 | Training Acc: 0.830 | Test Acc: 0.850 | Test Loss: 1.190 | Test AUC 0.831 (427.54 s) \n",
      "Epoch 21, Step 570 | Training Acc: 0.848 | Test Acc: 0.844 | Test Loss: 1.045 | Test AUC 0.829 (431.23 s) \n",
      "Epoch 21, Step 760 | Training Acc: 0.834 | Test Acc: 0.863 | Test Loss: 0.957 | Test AUC 0.835 (434.90 s) \n",
      "(*) Entering Epoch 22 (439.310 s)\n",
      "Epoch 22, Step 190 | Training Acc: 0.832 | Test Acc: 0.850 | Test Loss: 1.143 | Test AUC 0.826 (443.00 s) \n",
      "Epoch 22, Step 380 | Training Acc: 0.877 | Test Acc: 0.842 | Test Loss: 1.197 | Test AUC 0.842 (446.66 s) \n",
      "Epoch 22, Step 570 | Training Acc: 0.850 | Test Acc: 0.801 | Test Loss: 1.523 | Test AUC 0.811 (450.37 s) \n",
      "Epoch 22, Step 760 | Training Acc: 0.816 | Test Acc: 0.855 | Test Loss: 1.192 | Test AUC 0.825 (454.05 s) \n",
      "(*) Entering Epoch 23 (458.461 s)\n",
      "Epoch 23, Step 190 | Training Acc: 0.834 | Test Acc: 0.846 | Test Loss: 1.257 | Test AUC 0.824 (462.14 s) \n",
      "Epoch 23, Step 380 | Training Acc: 0.844 | Test Acc: 0.850 | Test Loss: 1.206 | Test AUC 0.831 (465.84 s) \n",
      "Epoch 23, Step 570 | Training Acc: 0.803 | Test Acc: 0.832 | Test Loss: 1.093 | Test AUC 0.800 (469.56 s) \n",
      "Epoch 23, Step 760 | Training Acc: 0.859 | Test Acc: 0.818 | Test Loss: 1.363 | Test AUC 0.825 (473.26 s) \n",
      "(*) Entering Epoch 24 (477.708 s)\n",
      "Epoch 24, Step 190 | Training Acc: 0.848 | Test Acc: 0.824 | Test Loss: 1.290 | Test AUC 0.818 (481.39 s) \n",
      "Epoch 24, Step 380 | Training Acc: 0.840 | Test Acc: 0.844 | Test Loss: 1.148 | Test AUC 0.822 (485.13 s) \n",
      "Epoch 24, Step 570 | Training Acc: 0.846 | Test Acc: 0.859 | Test Loss: 0.964 | Test AUC 0.841 (488.81 s) \n",
      "Epoch 24, Step 760 | Training Acc: 0.854 | Test Acc: 0.846 | Test Loss: 1.200 | Test AUC 0.836 (492.51 s) \n",
      "(*) Entering Epoch 25 (496.933 s)\n",
      "Epoch 25, Step 190 | Training Acc: 0.854 | Test Acc: 0.871 | Test Loss: 0.832 | Test AUC 0.850 (500.65 s) \n",
      "Epoch 25, Step 380 | Training Acc: 0.842 | Test Acc: 0.861 | Test Loss: 1.119 | Test AUC 0.839 (504.34 s) \n",
      "Epoch 25, Step 570 | Training Acc: 0.836 | Test Acc: 0.840 | Test Loss: 1.277 | Test AUC 0.822 (508.06 s) \n",
      "Epoch 25, Step 760 | Training Acc: 0.834 | Test Acc: 0.830 | Test Loss: 1.215 | Test AUC 0.817 (511.75 s) \n",
      "(*) Entering Epoch 26 (516.201 s)\n",
      "Epoch 26, Step 190 | Training Acc: 0.857 | Test Acc: 0.834 | Test Loss: 1.213 | Test AUC 0.830 (519.88 s) \n",
      "Epoch 26, Step 380 | Training Acc: 0.824 | Test Acc: 0.826 | Test Loss: 1.316 | Test AUC 0.818 (523.56 s) \n",
      "Epoch 26, Step 570 | Training Acc: 0.816 | Test Acc: 0.842 | Test Loss: 1.087 | Test AUC 0.817 (527.26 s) \n",
      "Epoch 26, Step 760 | Training Acc: 0.828 | Test Acc: 0.824 | Test Loss: 1.023 | Test AUC 0.813 (530.98 s) \n",
      "(*) Entering Epoch 27 (535.418 s)\n",
      "Epoch 27, Step 190 | Training Acc: 0.883 | Test Acc: 0.830 | Test Loss: 1.212 | Test AUC 0.846 (539.12 s) \n",
      "Epoch 27, Step 380 | Training Acc: 0.830 | Test Acc: 0.809 | Test Loss: 1.278 | Test AUC 0.811 (542.81 s) \n",
      "Epoch 27, Step 570 | Training Acc: 0.859 | Test Acc: 0.812 | Test Loss: 1.512 | Test AUC 0.827 (546.56 s) \n",
      "Epoch 27, Step 760 | Training Acc: 0.811 | Test Acc: 0.854 | Test Loss: 0.963 | Test AUC 0.821 (550.28 s) \n",
      "(*) Entering Epoch 28 (554.728 s)\n",
      "Epoch 28, Step 190 | Training Acc: 0.830 | Test Acc: 0.842 | Test Loss: 1.149 | Test AUC 0.823 (558.44 s) \n",
      "Epoch 28, Step 380 | Training Acc: 0.846 | Test Acc: 0.873 | Test Loss: 0.976 | Test AUC 0.855 (562.13 s) \n",
      "Epoch 28, Step 570 | Training Acc: 0.844 | Test Acc: 0.838 | Test Loss: 0.978 | Test AUC 0.836 (565.83 s) \n",
      "Epoch 28, Step 760 | Training Acc: 0.850 | Test Acc: 0.842 | Test Loss: 0.959 | Test AUC 0.833 (569.54 s) \n",
      "(*) Entering Epoch 29 (574.001 s)\n",
      "Epoch 29, Step 190 | Training Acc: 0.822 | Test Acc: 0.854 | Test Loss: 0.904 | Test AUC 0.825 (577.69 s) \n",
      "Epoch 29, Step 380 | Training Acc: 0.846 | Test Acc: 0.840 | Test Loss: 0.924 | Test AUC 0.834 (581.37 s) \n",
      "Epoch 29, Step 570 | Training Acc: 0.832 | Test Acc: 0.852 | Test Loss: 0.946 | Test AUC 0.835 (585.04 s) \n",
      "Epoch 29, Step 760 | Training Acc: 0.844 | Test Acc: 0.826 | Test Loss: 1.120 | Test AUC 0.828 (588.73 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch30.ckpt-30\n",
      "(*) Entering Epoch 30 (593.765 s)\n",
      "Epoch 30, Step 190 | Training Acc: 0.848 | Test Acc: 0.848 | Test Loss: 0.934 | Test AUC 0.840 (597.49 s) \n",
      "Epoch 30, Step 380 | Training Acc: 0.828 | Test Acc: 0.857 | Test Loss: 1.117 | Test AUC 0.837 (601.17 s) \n",
      "Epoch 30, Step 570 | Training Acc: 0.840 | Test Acc: 0.836 | Test Loss: 0.981 | Test AUC 0.824 (604.86 s) \n",
      "Epoch 30, Step 760 | Training Acc: 0.814 | Test Acc: 0.814 | Test Loss: 1.173 | Test AUC 0.803 (608.57 s) \n",
      "(*) Entering Epoch 31 (613.018 s)\n",
      "Epoch 31, Step 190 | Training Acc: 0.848 | Test Acc: 0.820 | Test Loss: 1.104 | Test AUC 0.824 (616.75 s) \n",
      "Epoch 31, Step 380 | Training Acc: 0.863 | Test Acc: 0.809 | Test Loss: 1.380 | Test AUC 0.831 (620.46 s) \n",
      "Epoch 31, Step 570 | Training Acc: 0.844 | Test Acc: 0.834 | Test Loss: 1.136 | Test AUC 0.831 (624.13 s) \n",
      "Epoch 31, Step 760 | Training Acc: 0.830 | Test Acc: 0.838 | Test Loss: 1.130 | Test AUC 0.822 (627.81 s) \n",
      "(*) Entering Epoch 32 (632.239 s)\n",
      "Epoch 32, Step 190 | Training Acc: 0.840 | Test Acc: 0.830 | Test Loss: 0.821 | Test AUC 0.826 (635.93 s) \n",
      "Epoch 32, Step 380 | Training Acc: 0.852 | Test Acc: 0.873 | Test Loss: 0.778 | Test AUC 0.854 (639.61 s) \n",
      "Epoch 32, Step 570 | Training Acc: 0.852 | Test Acc: 0.820 | Test Loss: 1.114 | Test AUC 0.829 (643.31 s) \n",
      "Epoch 32, Step 760 | Training Acc: 0.848 | Test Acc: 0.859 | Test Loss: 0.880 | Test AUC 0.850 (647.03 s) \n",
      "(*) Entering Epoch 33 (651.439 s)\n",
      "Epoch 33, Step 190 | Training Acc: 0.836 | Test Acc: 0.809 | Test Loss: 1.166 | Test AUC 0.817 (655.12 s) \n",
      "Epoch 33, Step 380 | Training Acc: 0.844 | Test Acc: 0.816 | Test Loss: 1.314 | Test AUC 0.823 (658.80 s) \n",
      "Epoch 33, Step 570 | Training Acc: 0.820 | Test Acc: 0.844 | Test Loss: 1.017 | Test AUC 0.824 (662.52 s) \n",
      "Epoch 33, Step 760 | Training Acc: 0.834 | Test Acc: 0.828 | Test Loss: 1.092 | Test AUC 0.822 (666.20 s) \n",
      "(*) Entering Epoch 34 (670.679 s)\n",
      "Epoch 34, Step 190 | Training Acc: 0.848 | Test Acc: 0.865 | Test Loss: 0.910 | Test AUC 0.846 (674.37 s) \n",
      "Epoch 34, Step 380 | Training Acc: 0.840 | Test Acc: 0.822 | Test Loss: 1.211 | Test AUC 0.821 (678.04 s) \n",
      "Epoch 34, Step 570 | Training Acc: 0.830 | Test Acc: 0.809 | Test Loss: 1.189 | Test AUC 0.806 (681.72 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Step 760 | Training Acc: 0.844 | Test Acc: 0.824 | Test Loss: 1.110 | Test AUC 0.831 (685.41 s) \n",
      "(*) Entering Epoch 35 (689.836 s)\n",
      "Epoch 35, Step 190 | Training Acc: 0.828 | Test Acc: 0.820 | Test Loss: 1.154 | Test AUC 0.818 (693.57 s) \n",
      "Epoch 35, Step 380 | Training Acc: 0.852 | Test Acc: 0.838 | Test Loss: 0.814 | Test AUC 0.842 (697.25 s) \n",
      "Epoch 35, Step 570 | Training Acc: 0.850 | Test Acc: 0.834 | Test Loss: 1.067 | Test AUC 0.842 (700.95 s) \n",
      "Epoch 35, Step 760 | Training Acc: 0.838 | Test Acc: 0.811 | Test Loss: 1.030 | Test AUC 0.819 (704.60 s) \n",
      "(*) Entering Epoch 36 (709.032 s)\n",
      "Epoch 36, Step 190 | Training Acc: 0.842 | Test Acc: 0.854 | Test Loss: 0.982 | Test AUC 0.842 (712.75 s) \n",
      "Epoch 36, Step 380 | Training Acc: 0.869 | Test Acc: 0.871 | Test Loss: 0.841 | Test AUC 0.863 (716.45 s) \n",
      "Epoch 36, Step 570 | Training Acc: 0.865 | Test Acc: 0.824 | Test Loss: 1.000 | Test AUC 0.840 (720.21 s) \n",
      "Epoch 36, Step 760 | Training Acc: 0.861 | Test Acc: 0.855 | Test Loss: 1.163 | Test AUC 0.857 (723.91 s) \n",
      "(*) Entering Epoch 37 (728.363 s)\n",
      "Epoch 37, Step 190 | Training Acc: 0.855 | Test Acc: 0.838 | Test Loss: 0.965 | Test AUC 0.840 (732.06 s) \n",
      "Epoch 37, Step 380 | Training Acc: 0.846 | Test Acc: 0.824 | Test Loss: 1.076 | Test AUC 0.826 (735.76 s) \n",
      "Epoch 37, Step 570 | Training Acc: 0.867 | Test Acc: 0.842 | Test Loss: 1.000 | Test AUC 0.849 (739.45 s) \n",
      "Epoch 37, Step 760 | Training Acc: 0.861 | Test Acc: 0.840 | Test Loss: 0.897 | Test AUC 0.848 (743.14 s) \n",
      "(*) Entering Epoch 38 (747.579 s)\n",
      "Epoch 38, Step 190 | Training Acc: 0.820 | Test Acc: 0.826 | Test Loss: 1.138 | Test AUC 0.816 (751.29 s) \n",
      "Epoch 38, Step 380 | Training Acc: 0.852 | Test Acc: 0.826 | Test Loss: 1.197 | Test AUC 0.837 (754.96 s) \n",
      "Epoch 38, Step 570 | Training Acc: 0.857 | Test Acc: 0.824 | Test Loss: 1.217 | Test AUC 0.834 (758.66 s) \n",
      "Epoch 38, Step 760 | Training Acc: 0.873 | Test Acc: 0.867 | Test Loss: 0.771 | Test AUC 0.864 (762.36 s) \n",
      "(*) Entering Epoch 39 (766.789 s)\n",
      "Epoch 39, Step 190 | Training Acc: 0.859 | Test Acc: 0.844 | Test Loss: 0.954 | Test AUC 0.845 (770.49 s) \n",
      "Epoch 39, Step 380 | Training Acc: 0.838 | Test Acc: 0.842 | Test Loss: 0.937 | Test AUC 0.837 (774.20 s) \n",
      "Epoch 39, Step 570 | Training Acc: 0.859 | Test Acc: 0.811 | Test Loss: 1.233 | Test AUC 0.831 (777.88 s) \n",
      "Epoch 39, Step 760 | Training Acc: 0.873 | Test Acc: 0.854 | Test Loss: 0.976 | Test AUC 0.859 (781.53 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch40.ckpt-40\n",
      "(*) Entering Epoch 40 (786.588 s)\n",
      "Epoch 40, Step 190 | Training Acc: 0.863 | Test Acc: 0.859 | Test Loss: 0.958 | Test AUC 0.857 (790.28 s) \n",
      "Epoch 40, Step 380 | Training Acc: 0.844 | Test Acc: 0.844 | Test Loss: 0.988 | Test AUC 0.840 (793.97 s) \n",
      "Epoch 40, Step 570 | Training Acc: 0.848 | Test Acc: 0.842 | Test Loss: 1.179 | Test AUC 0.834 (797.66 s) \n",
      "Epoch 40, Step 760 | Training Acc: 0.844 | Test Acc: 0.830 | Test Loss: 1.168 | Test AUC 0.831 (801.34 s) \n",
      "(*) Entering Epoch 41 (805.770 s)\n",
      "Epoch 41, Step 190 | Training Acc: 0.850 | Test Acc: 0.824 | Test Loss: 1.083 | Test AUC 0.835 (809.45 s) \n",
      "Epoch 41, Step 380 | Training Acc: 0.840 | Test Acc: 0.861 | Test Loss: 0.834 | Test AUC 0.847 (813.12 s) \n",
      "Epoch 41, Step 570 | Training Acc: 0.869 | Test Acc: 0.844 | Test Loss: 1.026 | Test AUC 0.853 (816.80 s) \n",
      "Epoch 41, Step 760 | Training Acc: 0.840 | Test Acc: 0.855 | Test Loss: 0.781 | Test AUC 0.845 (820.51 s) \n",
      "(*) Entering Epoch 42 (824.919 s)\n",
      "Epoch 42, Step 190 | Training Acc: 0.838 | Test Acc: 0.832 | Test Loss: 1.077 | Test AUC 0.834 (828.58 s) \n",
      "Epoch 42, Step 380 | Training Acc: 0.881 | Test Acc: 0.846 | Test Loss: 0.951 | Test AUC 0.861 (832.28 s) \n",
      "Epoch 42, Step 570 | Training Acc: 0.854 | Test Acc: 0.885 | Test Loss: 0.766 | Test AUC 0.859 (836.52 s) [*]\n",
      "Epoch 42, Step 760 | Training Acc: 0.861 | Test Acc: 0.857 | Test Loss: 0.868 | Test AUC 0.855 (840.23 s) \n",
      "(*) Entering Epoch 43 (844.687 s)\n",
      "Epoch 43, Step 190 | Training Acc: 0.869 | Test Acc: 0.830 | Test Loss: 1.043 | Test AUC 0.848 (848.36 s) \n",
      "Epoch 43, Step 380 | Training Acc: 0.846 | Test Acc: 0.822 | Test Loss: 1.202 | Test AUC 0.825 (852.04 s) \n",
      "Epoch 43, Step 570 | Training Acc: 0.857 | Test Acc: 0.889 | Test Loss: 0.604 | Test AUC 0.873 (856.33 s) [*]\n",
      "Epoch 43, Step 760 | Training Acc: 0.852 | Test Acc: 0.828 | Test Loss: 1.087 | Test AUC 0.835 (860.01 s) \n",
      "(*) Entering Epoch 44 (864.442 s)\n",
      "Epoch 44, Step 190 | Training Acc: 0.857 | Test Acc: 0.814 | Test Loss: 1.068 | Test AUC 0.829 (868.11 s) \n",
      "Epoch 44, Step 380 | Training Acc: 0.852 | Test Acc: 0.826 | Test Loss: 1.285 | Test AUC 0.835 (871.78 s) \n",
      "Epoch 44, Step 570 | Training Acc: 0.863 | Test Acc: 0.818 | Test Loss: 1.176 | Test AUC 0.837 (875.47 s) \n",
      "Epoch 44, Step 760 | Training Acc: 0.859 | Test Acc: 0.832 | Test Loss: 1.117 | Test AUC 0.839 (879.16 s) \n",
      "(*) Entering Epoch 45 (883.595 s)\n",
      "Epoch 45, Step 190 | Training Acc: 0.830 | Test Acc: 0.818 | Test Loss: 1.286 | Test AUC 0.821 (887.29 s) \n",
      "Epoch 45, Step 380 | Training Acc: 0.875 | Test Acc: 0.824 | Test Loss: 1.061 | Test AUC 0.843 (890.96 s) \n",
      "Epoch 45, Step 570 | Training Acc: 0.852 | Test Acc: 0.834 | Test Loss: 1.090 | Test AUC 0.837 (894.66 s) \n",
      "Epoch 45, Step 760 | Training Acc: 0.883 | Test Acc: 0.854 | Test Loss: 0.840 | Test AUC 0.861 (898.36 s) \n",
      "(*) Entering Epoch 46 (902.788 s)\n",
      "Epoch 46, Step 190 | Training Acc: 0.857 | Test Acc: 0.820 | Test Loss: 1.073 | Test AUC 0.835 (906.45 s) \n",
      "Epoch 46, Step 380 | Training Acc: 0.857 | Test Acc: 0.834 | Test Loss: 0.878 | Test AUC 0.847 (910.15 s) \n",
      "Epoch 46, Step 570 | Training Acc: 0.850 | Test Acc: 0.832 | Test Loss: 0.915 | Test AUC 0.842 (913.82 s) \n",
      "Epoch 46, Step 760 | Training Acc: 0.863 | Test Acc: 0.857 | Test Loss: 0.875 | Test AUC 0.855 (917.53 s) \n",
      "(*) Entering Epoch 47 (921.952 s)\n",
      "Epoch 47, Step 190 | Training Acc: 0.861 | Test Acc: 0.820 | Test Loss: 1.182 | Test AUC 0.838 (925.64 s) \n",
      "Epoch 47, Step 380 | Training Acc: 0.850 | Test Acc: 0.828 | Test Loss: 0.932 | Test AUC 0.833 (929.33 s) \n",
      "Epoch 47, Step 570 | Training Acc: 0.838 | Test Acc: 0.828 | Test Loss: 0.983 | Test AUC 0.830 (933.03 s) \n",
      "Epoch 47, Step 760 | Training Acc: 0.846 | Test Acc: 0.832 | Test Loss: 0.891 | Test AUC 0.834 (936.73 s) \n",
      "(*) Entering Epoch 48 (941.166 s)\n",
      "Epoch 48, Step 190 | Training Acc: 0.871 | Test Acc: 0.844 | Test Loss: 0.902 | Test AUC 0.854 (944.86 s) \n",
      "Epoch 48, Step 380 | Training Acc: 0.859 | Test Acc: 0.865 | Test Loss: 0.743 | Test AUC 0.862 (948.54 s) \n",
      "Epoch 48, Step 570 | Training Acc: 0.869 | Test Acc: 0.830 | Test Loss: 1.152 | Test AUC 0.845 (952.22 s) \n",
      "Epoch 48, Step 760 | Training Acc: 0.832 | Test Acc: 0.838 | Test Loss: 1.052 | Test AUC 0.833 (955.96 s) \n",
      "(*) Entering Epoch 49 (960.391 s)\n",
      "Epoch 49, Step 190 | Training Acc: 0.855 | Test Acc: 0.867 | Test Loss: 0.851 | Test AUC 0.854 (964.07 s) \n",
      "Epoch 49, Step 380 | Training Acc: 0.830 | Test Acc: 0.863 | Test Loss: 0.923 | Test AUC 0.842 (967.76 s) \n",
      "Epoch 49, Step 570 | Training Acc: 0.850 | Test Acc: 0.838 | Test Loss: 0.990 | Test AUC 0.842 (971.46 s) \n",
      "Epoch 49, Step 760 | Training Acc: 0.846 | Test Acc: 0.826 | Test Loss: 1.090 | Test AUC 0.834 (975.15 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch50.ckpt-50\n",
      "(*) Entering Epoch 50 (980.167 s)\n",
      "Epoch 50, Step 190 | Training Acc: 0.855 | Test Acc: 0.865 | Test Loss: 0.734 | Test AUC 0.858 (983.82 s) \n",
      "Epoch 50, Step 380 | Training Acc: 0.834 | Test Acc: 0.869 | Test Loss: 1.060 | Test AUC 0.848 (987.51 s) \n",
      "Epoch 50, Step 570 | Training Acc: 0.867 | Test Acc: 0.842 | Test Loss: 1.018 | Test AUC 0.852 (991.21 s) \n",
      "Epoch 50, Step 760 | Training Acc: 0.885 | Test Acc: 0.844 | Test Loss: 1.068 | Test AUC 0.862 (994.88 s) \n",
      "(*) Entering Epoch 51 (999.346 s)\n",
      "Epoch 51, Step 190 | Training Acc: 0.869 | Test Acc: 0.791 | Test Loss: 1.382 | Test AUC 0.829 (1003.04 s) \n",
      "Epoch 51, Step 380 | Training Acc: 0.857 | Test Acc: 0.814 | Test Loss: 1.221 | Test AUC 0.832 (1006.74 s) \n",
      "Epoch 51, Step 570 | Training Acc: 0.879 | Test Acc: 0.844 | Test Loss: 0.828 | Test AUC 0.860 (1010.46 s) \n",
      "Epoch 51, Step 760 | Training Acc: 0.871 | Test Acc: 0.805 | Test Loss: 1.202 | Test AUC 0.834 (1014.12 s) \n",
      "(*) Entering Epoch 52 (1018.586 s)\n",
      "Epoch 52, Step 190 | Training Acc: 0.861 | Test Acc: 0.867 | Test Loss: 0.752 | Test AUC 0.863 (1022.30 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Step 380 | Training Acc: 0.857 | Test Acc: 0.832 | Test Loss: 1.104 | Test AUC 0.843 (1026.02 s) \n",
      "Epoch 52, Step 570 | Training Acc: 0.846 | Test Acc: 0.822 | Test Loss: 1.056 | Test AUC 0.831 (1029.74 s) \n",
      "Epoch 52, Step 760 | Training Acc: 0.836 | Test Acc: 0.830 | Test Loss: 1.023 | Test AUC 0.831 (1033.42 s) \n",
      "(*) Entering Epoch 53 (1037.880 s)\n",
      "Epoch 53, Step 190 | Training Acc: 0.857 | Test Acc: 0.791 | Test Loss: 1.128 | Test AUC 0.822 (1041.56 s) \n",
      "Epoch 53, Step 380 | Training Acc: 0.854 | Test Acc: 0.836 | Test Loss: 1.058 | Test AUC 0.840 (1045.26 s) \n",
      "Epoch 53, Step 570 | Training Acc: 0.871 | Test Acc: 0.844 | Test Loss: 0.913 | Test AUC 0.855 (1048.95 s) \n",
      "Epoch 53, Step 760 | Training Acc: 0.854 | Test Acc: 0.822 | Test Loss: 1.223 | Test AUC 0.834 (1052.60 s) \n",
      "(*) Entering Epoch 54 (1057.039 s)\n",
      "Epoch 54, Step 190 | Training Acc: 0.869 | Test Acc: 0.852 | Test Loss: 0.776 | Test AUC 0.861 (1060.76 s) \n",
      "Epoch 54, Step 380 | Training Acc: 0.855 | Test Acc: 0.844 | Test Loss: 0.962 | Test AUC 0.848 (1064.40 s) \n",
      "Epoch 54, Step 570 | Training Acc: 0.885 | Test Acc: 0.824 | Test Loss: 1.047 | Test AUC 0.852 (1068.09 s) \n",
      "Epoch 54, Step 760 | Training Acc: 0.855 | Test Acc: 0.822 | Test Loss: 1.113 | Test AUC 0.835 (1071.78 s) \n",
      "(*) Entering Epoch 55 (1076.240 s)\n",
      "Epoch 55, Step 190 | Training Acc: 0.832 | Test Acc: 0.826 | Test Loss: 0.942 | Test AUC 0.827 (1079.91 s) \n",
      "Epoch 55, Step 380 | Training Acc: 0.879 | Test Acc: 0.838 | Test Loss: 1.012 | Test AUC 0.856 (1083.63 s) \n",
      "Epoch 55, Step 570 | Training Acc: 0.871 | Test Acc: 0.822 | Test Loss: 1.091 | Test AUC 0.847 (1087.33 s) \n",
      "Epoch 55, Step 760 | Training Acc: 0.848 | Test Acc: 0.848 | Test Loss: 0.821 | Test AUC 0.843 (1091.03 s) \n",
      "(*) Entering Epoch 56 (1095.486 s)\n",
      "Epoch 56, Step 190 | Training Acc: 0.867 | Test Acc: 0.834 | Test Loss: 1.043 | Test AUC 0.850 (1099.16 s) \n",
      "Epoch 56, Step 380 | Training Acc: 0.852 | Test Acc: 0.834 | Test Loss: 0.926 | Test AUC 0.840 (1102.85 s) \n",
      "Epoch 56, Step 570 | Training Acc: 0.869 | Test Acc: 0.844 | Test Loss: 0.967 | Test AUC 0.854 (1106.54 s) \n",
      "Epoch 56, Step 760 | Training Acc: 0.879 | Test Acc: 0.855 | Test Loss: 0.883 | Test AUC 0.863 (1110.27 s) \n",
      "(*) Entering Epoch 57 (1114.731 s)\n",
      "Epoch 57, Step 190 | Training Acc: 0.840 | Test Acc: 0.826 | Test Loss: 1.022 | Test AUC 0.831 (1118.42 s) \n",
      "Epoch 57, Step 380 | Training Acc: 0.863 | Test Acc: 0.854 | Test Loss: 0.849 | Test AUC 0.856 (1122.12 s) \n",
      "Epoch 57, Step 570 | Training Acc: 0.893 | Test Acc: 0.842 | Test Loss: 0.785 | Test AUC 0.864 (1125.80 s) \n",
      "Epoch 57, Step 760 | Training Acc: 0.857 | Test Acc: 0.834 | Test Loss: 1.084 | Test AUC 0.843 (1129.51 s) \n",
      "(*) Entering Epoch 58 (1133.942 s)\n",
      "Epoch 58, Step 190 | Training Acc: 0.842 | Test Acc: 0.836 | Test Loss: 1.145 | Test AUC 0.837 (1137.61 s) \n",
      "Epoch 58, Step 380 | Training Acc: 0.871 | Test Acc: 0.852 | Test Loss: 0.966 | Test AUC 0.862 (1141.33 s) \n",
      "Epoch 58, Step 570 | Training Acc: 0.861 | Test Acc: 0.832 | Test Loss: 1.063 | Test AUC 0.846 (1145.04 s) \n",
      "Epoch 58, Step 760 | Training Acc: 0.865 | Test Acc: 0.838 | Test Loss: 1.021 | Test AUC 0.847 (1148.75 s) \n",
      "(*) Entering Epoch 59 (1153.185 s)\n",
      "Epoch 59, Step 190 | Training Acc: 0.875 | Test Acc: 0.842 | Test Loss: 1.209 | Test AUC 0.858 (1156.84 s) \n",
      "Epoch 59, Step 380 | Training Acc: 0.828 | Test Acc: 0.850 | Test Loss: 0.976 | Test AUC 0.832 (1160.58 s) \n",
      "Epoch 59, Step 570 | Training Acc: 0.863 | Test Acc: 0.809 | Test Loss: 1.352 | Test AUC 0.831 (1164.24 s) \n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch0.ckpt-0\n",
      "(*) Entering Epoch 0 (13.018 s)\n",
      "Epoch 0, Step 190 | Training Acc: 0.830 | Test Acc: 0.676 | Test Loss: 0.965 | Test AUC 0.771 (22.18 s) \n",
      "Epoch 0, Step 380 | Training Acc: 0.812 | Test Acc: 0.816 | Test Loss: 0.483 | Test AUC 0.815 (29.07 s) \n",
      "Epoch 0, Step 570 | Training Acc: 0.818 | Test Acc: 0.814 | Test Loss: 0.453 | Test AUC 0.818 (35.93 s) \n",
      "Epoch 0, Step 760 | Training Acc: 0.838 | Test Acc: 0.828 | Test Loss: 0.376 | Test AUC 0.828 (42.77 s) \n",
      "Epoch 0, Step 950 | Training Acc: 0.873 | Test Acc: 0.865 | Test Loss: 0.325 | Test AUC 0.868 (50.58 s) \n",
      "(*) Entering Epoch 1 (50.583 s)\n",
      "Epoch 1, Step 190 | Training Acc: 0.850 | Test Acc: 0.865 | Test Loss: 0.341 | Test AUC 0.849 (57.42 s) \n",
      "Epoch 1, Step 380 | Training Acc: 0.863 | Test Acc: 0.830 | Test Loss: 0.374 | Test AUC 0.834 (64.25 s) \n",
      "Epoch 1, Step 570 | Training Acc: 0.861 | Test Acc: 0.883 | Test Loss: 0.325 | Test AUC 0.863 (71.08 s) \n",
      "Epoch 1, Step 760 | Training Acc: 0.879 | Test Acc: 0.861 | Test Loss: 0.301 | Test AUC 0.864 (77.92 s) \n",
      "(*) Entering Epoch 2 (85.322 s)\n",
      "Epoch 2, Step 190 | Training Acc: 0.893 | Test Acc: 0.852 | Test Loss: 0.314 | Test AUC 0.863 (92.17 s) \n",
      "Epoch 2, Step 380 | Training Acc: 0.871 | Test Acc: 0.875 | Test Loss: 0.285 | Test AUC 0.864 (99.02 s) \n",
      "Epoch 2, Step 570 | Training Acc: 0.850 | Test Acc: 0.877 | Test Loss: 0.294 | Test AUC 0.851 (105.84 s) \n",
      "Epoch 2, Step 760 | Training Acc: 0.875 | Test Acc: 0.877 | Test Loss: 0.317 | Test AUC 0.863 (112.64 s) \n",
      "(*) Entering Epoch 3 (120.008 s)\n",
      "Epoch 3, Step 190 | Training Acc: 0.859 | Test Acc: 0.883 | Test Loss: 0.274 | Test AUC 0.858 (126.83 s) \n",
      "Epoch 3, Step 380 | Training Acc: 0.861 | Test Acc: 0.879 | Test Loss: 0.282 | Test AUC 0.856 (133.61 s) \n",
      "Epoch 3, Step 570 | Training Acc: 0.896 | Test Acc: 0.885 | Test Loss: 0.264 | Test AUC 0.888 (140.37 s) \n",
      "Epoch 3, Step 760 | Training Acc: 0.887 | Test Acc: 0.896 | Test Loss: 0.258 | Test AUC 0.883 (147.19 s) \n",
      "(*) Entering Epoch 4 (154.531 s)\n",
      "Epoch 4, Step 190 | Training Acc: 0.871 | Test Acc: 0.885 | Test Loss: 0.299 | Test AUC 0.873 (161.30 s) \n",
      "Epoch 4, Step 380 | Training Acc: 0.859 | Test Acc: 0.881 | Test Loss: 0.272 | Test AUC 0.865 (168.09 s) \n",
      "Epoch 4, Step 570 | Training Acc: 0.855 | Test Acc: 0.824 | Test Loss: 0.385 | Test AUC 0.814 (174.89 s) \n",
      "Epoch 4, Step 760 | Training Acc: 0.855 | Test Acc: 0.848 | Test Loss: 0.349 | Test AUC 0.837 (181.68 s) \n",
      "(*) Entering Epoch 5 (189.019 s)\n",
      "Epoch 5, Step 190 | Training Acc: 0.895 | Test Acc: 0.887 | Test Loss: 0.297 | Test AUC 0.884 (195.81 s) \n",
      "Epoch 5, Step 380 | Training Acc: 0.881 | Test Acc: 0.896 | Test Loss: 0.235 | Test AUC 0.886 (202.58 s) \n",
      "Epoch 5, Step 570 | Training Acc: 0.887 | Test Acc: 0.898 | Test Loss: 0.245 | Test AUC 0.880 (209.38 s) \n",
      "Epoch 5, Step 760 | Training Acc: 0.891 | Test Acc: 0.881 | Test Loss: 0.294 | Test AUC 0.879 (216.17 s) \n",
      "(*) Entering Epoch 6 (223.532 s)\n",
      "Epoch 6, Step 190 | Training Acc: 0.859 | Test Acc: 0.793 | Test Loss: 0.419 | Test AUC 0.797 (230.31 s) \n",
      "Epoch 6, Step 380 | Training Acc: 0.869 | Test Acc: 0.842 | Test Loss: 0.366 | Test AUC 0.831 (237.12 s) \n",
      "Epoch 6, Step 570 | Training Acc: 0.859 | Test Acc: 0.871 | Test Loss: 0.321 | Test AUC 0.852 (243.90 s) \n",
      "Epoch 6, Step 760 | Training Acc: 0.881 | Test Acc: 0.877 | Test Loss: 0.270 | Test AUC 0.870 (250.68 s) \n",
      "(*) Entering Epoch 7 (258.028 s)\n",
      "Epoch 7, Step 190 | Training Acc: 0.865 | Test Acc: 0.879 | Test Loss: 0.293 | Test AUC 0.860 (264.84 s) \n",
      "Epoch 7, Step 380 | Training Acc: 0.875 | Test Acc: 0.895 | Test Loss: 0.300 | Test AUC 0.874 (271.63 s) \n",
      "Epoch 7, Step 570 | Training Acc: 0.879 | Test Acc: 0.861 | Test Loss: 0.289 | Test AUC 0.861 (278.41 s) \n",
      "Epoch 7, Step 760 | Training Acc: 0.854 | Test Acc: 0.852 | Test Loss: 0.354 | Test AUC 0.832 (285.24 s) \n",
      "(*) Entering Epoch 8 (292.623 s)\n",
      "Epoch 8, Step 190 | Training Acc: 0.889 | Test Acc: 0.893 | Test Loss: 0.262 | Test AUC 0.880 (299.42 s) \n",
      "Epoch 8, Step 380 | Training Acc: 0.908 | Test Acc: 0.816 | Test Loss: 0.405 | Test AUC 0.837 (306.21 s) \n",
      "Epoch 8, Step 570 | Training Acc: 0.879 | Test Acc: 0.848 | Test Loss: 0.335 | Test AUC 0.845 (312.99 s) \n",
      "Epoch 8, Step 760 | Training Acc: 0.859 | Test Acc: 0.879 | Test Loss: 0.266 | Test AUC 0.860 (319.77 s) \n",
      "(*) Entering Epoch 9 (327.108 s)\n",
      "Epoch 9, Step 190 | Training Acc: 0.891 | Test Acc: 0.885 | Test Loss: 0.279 | Test AUC 0.879 (334.87 s) [*]\n",
      "Epoch 9, Step 380 | Training Acc: 0.879 | Test Acc: 0.871 | Test Loss: 0.320 | Test AUC 0.868 (341.67 s) \n",
      "Epoch 9, Step 570 | Training Acc: 0.891 | Test Acc: 0.873 | Test Loss: 0.293 | Test AUC 0.876 (348.47 s) \n",
      "Epoch 9, Step 760 | Training Acc: 0.855 | Test Acc: 0.857 | Test Loss: 0.307 | Test AUC 0.844 (355.25 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch10.ckpt-10\n",
      "(*) Entering Epoch 10 (363.576 s)\n",
      "Epoch 10, Step 190 | Training Acc: 0.893 | Test Acc: 0.895 | Test Loss: 0.295 | Test AUC 0.877 (371.51 s) [*]\n",
      "Epoch 10, Step 380 | Training Acc: 0.875 | Test Acc: 0.832 | Test Loss: 0.396 | Test AUC 0.823 (378.33 s) \n",
      "Epoch 10, Step 570 | Training Acc: 0.883 | Test Acc: 0.852 | Test Loss: 0.360 | Test AUC 0.855 (385.13 s) \n",
      "Epoch 10, Step 760 | Training Acc: 0.877 | Test Acc: 0.887 | Test Loss: 0.284 | Test AUC 0.871 (391.92 s) \n",
      "(*) Entering Epoch 11 (399.319 s)\n",
      "Epoch 11, Step 190 | Training Acc: 0.904 | Test Acc: 0.904 | Test Loss: 0.243 | Test AUC 0.894 (407.07 s) [*]\n",
      "Epoch 11, Step 380 | Training Acc: 0.885 | Test Acc: 0.881 | Test Loss: 0.275 | Test AUC 0.879 (413.80 s) \n",
      "Epoch 11, Step 570 | Training Acc: 0.893 | Test Acc: 0.871 | Test Loss: 0.336 | Test AUC 0.873 (420.56 s) \n",
      "Epoch 11, Step 760 | Training Acc: 0.887 | Test Acc: 0.881 | Test Loss: 0.268 | Test AUC 0.874 (427.43 s) \n",
      "(*) Entering Epoch 12 (434.786 s)\n",
      "Epoch 12, Step 190 | Training Acc: 0.879 | Test Acc: 0.891 | Test Loss: 0.280 | Test AUC 0.879 (441.58 s) \n",
      "Epoch 12, Step 380 | Training Acc: 0.895 | Test Acc: 0.855 | Test Loss: 0.297 | Test AUC 0.861 (448.34 s) \n",
      "Epoch 12, Step 570 | Training Acc: 0.861 | Test Acc: 0.881 | Test Loss: 0.292 | Test AUC 0.859 (455.08 s) \n",
      "Epoch 12, Step 760 | Training Acc: 0.910 | Test Acc: 0.865 | Test Loss: 0.287 | Test AUC 0.876 (461.80 s) \n",
      "(*) Entering Epoch 13 (469.127 s)\n",
      "Epoch 13, Step 190 | Training Acc: 0.900 | Test Acc: 0.785 | Test Loss: 0.446 | Test AUC 0.813 (475.88 s) \n",
      "Epoch 13, Step 380 | Training Acc: 0.881 | Test Acc: 0.840 | Test Loss: 0.382 | Test AUC 0.841 (482.64 s) \n",
      "Epoch 13, Step 570 | Training Acc: 0.875 | Test Acc: 0.869 | Test Loss: 0.308 | Test AUC 0.859 (489.38 s) \n",
      "Epoch 13, Step 760 | Training Acc: 0.902 | Test Acc: 0.877 | Test Loss: 0.260 | Test AUC 0.882 (496.15 s) \n",
      "(*) Entering Epoch 14 (503.482 s)\n",
      "Epoch 14, Step 190 | Training Acc: 0.883 | Test Acc: 0.869 | Test Loss: 0.317 | Test AUC 0.867 (510.25 s) \n",
      "Epoch 14, Step 380 | Training Acc: 0.904 | Test Acc: 0.906 | Test Loss: 0.253 | Test AUC 0.898 (517.99 s) [*]\n",
      "Epoch 14, Step 570 | Training Acc: 0.900 | Test Acc: 0.893 | Test Loss: 0.259 | Test AUC 0.885 (524.75 s) \n",
      "Epoch 14, Step 760 | Training Acc: 0.879 | Test Acc: 0.885 | Test Loss: 0.284 | Test AUC 0.875 (531.52 s) \n",
      "(*) Entering Epoch 15 (538.837 s)\n",
      "Epoch 15, Step 190 | Training Acc: 0.873 | Test Acc: 0.895 | Test Loss: 0.268 | Test AUC 0.877 (545.57 s) \n",
      "Epoch 15, Step 380 | Training Acc: 0.895 | Test Acc: 0.863 | Test Loss: 0.281 | Test AUC 0.860 (552.31 s) \n",
      "Epoch 15, Step 570 | Training Acc: 0.895 | Test Acc: 0.895 | Test Loss: 0.264 | Test AUC 0.887 (559.04 s) \n",
      "Epoch 15, Step 760 | Training Acc: 0.891 | Test Acc: 0.891 | Test Loss: 0.278 | Test AUC 0.877 (565.77 s) \n",
      "(*) Entering Epoch 16 (573.085 s)\n",
      "Epoch 16, Step 190 | Training Acc: 0.883 | Test Acc: 0.900 | Test Loss: 0.257 | Test AUC 0.881 (579.83 s) \n",
      "Epoch 16, Step 380 | Training Acc: 0.879 | Test Acc: 0.885 | Test Loss: 0.270 | Test AUC 0.869 (586.56 s) \n",
      "Epoch 16, Step 570 | Training Acc: 0.877 | Test Acc: 0.873 | Test Loss: 0.290 | Test AUC 0.868 (593.30 s) \n",
      "Epoch 16, Step 760 | Training Acc: 0.871 | Test Acc: 0.857 | Test Loss: 0.326 | Test AUC 0.850 (600.03 s) \n",
      "(*) Entering Epoch 17 (607.352 s)\n",
      "Epoch 17, Step 190 | Training Acc: 0.889 | Test Acc: 0.879 | Test Loss: 0.282 | Test AUC 0.869 (614.13 s) \n",
      "Epoch 17, Step 380 | Training Acc: 0.879 | Test Acc: 0.873 | Test Loss: 0.309 | Test AUC 0.856 (620.87 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Step 570 | Training Acc: 0.869 | Test Acc: 0.883 | Test Loss: 0.291 | Test AUC 0.869 (627.63 s) \n",
      "Epoch 17, Step 760 | Training Acc: 0.889 | Test Acc: 0.805 | Test Loss: 0.465 | Test AUC 0.818 (634.41 s) \n",
      "(*) Entering Epoch 18 (641.769 s)\n",
      "Epoch 18, Step 190 | Training Acc: 0.895 | Test Acc: 0.816 | Test Loss: 0.430 | Test AUC 0.820 (648.51 s) \n",
      "Epoch 18, Step 380 | Training Acc: 0.896 | Test Acc: 0.883 | Test Loss: 0.304 | Test AUC 0.874 (655.26 s) \n",
      "Epoch 18, Step 570 | Training Acc: 0.895 | Test Acc: 0.865 | Test Loss: 0.304 | Test AUC 0.871 (661.99 s) \n",
      "Epoch 18, Step 760 | Training Acc: 0.852 | Test Acc: 0.873 | Test Loss: 0.285 | Test AUC 0.852 (668.71 s) \n",
      "(*) Entering Epoch 19 (676.046 s)\n",
      "Epoch 19, Step 190 | Training Acc: 0.908 | Test Acc: 0.811 | Test Loss: 0.405 | Test AUC 0.835 (682.78 s) \n",
      "Epoch 19, Step 380 | Training Acc: 0.898 | Test Acc: 0.881 | Test Loss: 0.294 | Test AUC 0.881 (689.51 s) \n",
      "Epoch 19, Step 570 | Training Acc: 0.895 | Test Acc: 0.893 | Test Loss: 0.250 | Test AUC 0.881 (696.26 s) \n",
      "Epoch 19, Step 760 | Training Acc: 0.906 | Test Acc: 0.875 | Test Loss: 0.301 | Test AUC 0.880 (703.04 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch20.ckpt-20\n",
      "(*) Entering Epoch 20 (711.323 s)\n",
      "Epoch 20, Step 190 | Training Acc: 0.900 | Test Acc: 0.861 | Test Loss: 0.331 | Test AUC 0.868 (718.06 s) \n",
      "Epoch 20, Step 380 | Training Acc: 0.885 | Test Acc: 0.838 | Test Loss: 0.397 | Test AUC 0.839 (724.81 s) \n",
      "Epoch 20, Step 570 | Training Acc: 0.891 | Test Acc: 0.826 | Test Loss: 0.366 | Test AUC 0.838 (731.54 s) \n",
      "Epoch 20, Step 760 | Training Acc: 0.891 | Test Acc: 0.885 | Test Loss: 0.300 | Test AUC 0.876 (738.28 s) \n",
      "(*) Entering Epoch 21 (745.591 s)\n",
      "Epoch 21, Step 190 | Training Acc: 0.900 | Test Acc: 0.797 | Test Loss: 0.490 | Test AUC 0.824 (752.33 s) \n",
      "Epoch 21, Step 380 | Training Acc: 0.881 | Test Acc: 0.861 | Test Loss: 0.331 | Test AUC 0.852 (759.08 s) \n",
      "Epoch 21, Step 570 | Training Acc: 0.908 | Test Acc: 0.885 | Test Loss: 0.311 | Test AUC 0.885 (765.85 s) \n",
      "Epoch 21, Step 760 | Training Acc: 0.893 | Test Acc: 0.869 | Test Loss: 0.306 | Test AUC 0.872 (772.56 s) \n",
      "(*) Entering Epoch 22 (779.910 s)\n",
      "Epoch 22, Step 190 | Training Acc: 0.904 | Test Acc: 0.877 | Test Loss: 0.301 | Test AUC 0.878 (786.66 s) \n",
      "Epoch 22, Step 380 | Training Acc: 0.863 | Test Acc: 0.904 | Test Loss: 0.239 | Test AUC 0.869 (793.41 s) \n",
      "Epoch 22, Step 570 | Training Acc: 0.904 | Test Acc: 0.859 | Test Loss: 0.348 | Test AUC 0.865 (800.13 s) \n",
      "Epoch 22, Step 760 | Training Acc: 0.920 | Test Acc: 0.889 | Test Loss: 0.286 | Test AUC 0.895 (806.88 s) \n",
      "(*) Entering Epoch 23 (814.238 s)\n",
      "Epoch 23, Step 190 | Training Acc: 0.898 | Test Acc: 0.793 | Test Loss: 0.424 | Test AUC 0.803 (820.97 s) \n",
      "Epoch 23, Step 380 | Training Acc: 0.895 | Test Acc: 0.818 | Test Loss: 0.432 | Test AUC 0.837 (827.70 s) \n",
      "Epoch 23, Step 570 | Training Acc: 0.922 | Test Acc: 0.783 | Test Loss: 0.494 | Test AUC 0.819 (834.43 s) \n",
      "Epoch 23, Step 760 | Training Acc: 0.891 | Test Acc: 0.811 | Test Loss: 0.404 | Test AUC 0.831 (841.19 s) \n",
      "(*) Entering Epoch 24 (848.515 s)\n",
      "Epoch 24, Step 190 | Training Acc: 0.906 | Test Acc: 0.889 | Test Loss: 0.291 | Test AUC 0.883 (855.24 s) \n",
      "Epoch 24, Step 380 | Training Acc: 0.928 | Test Acc: 0.881 | Test Loss: 0.280 | Test AUC 0.894 (862.03 s) \n",
      "Epoch 24, Step 570 | Training Acc: 0.918 | Test Acc: 0.873 | Test Loss: 0.298 | Test AUC 0.888 (868.77 s) \n",
      "Epoch 24, Step 760 | Training Acc: 0.922 | Test Acc: 0.875 | Test Loss: 0.328 | Test AUC 0.888 (875.51 s) \n",
      "(*) Entering Epoch 25 (882.841 s)\n",
      "Epoch 25, Step 190 | Training Acc: 0.898 | Test Acc: 0.871 | Test Loss: 0.280 | Test AUC 0.870 (889.57 s) \n",
      "Epoch 25, Step 380 | Training Acc: 0.902 | Test Acc: 0.883 | Test Loss: 0.309 | Test AUC 0.886 (896.30 s) \n",
      "Epoch 25, Step 570 | Training Acc: 0.896 | Test Acc: 0.801 | Test Loss: 0.507 | Test AUC 0.822 (903.07 s) \n",
      "Epoch 25, Step 760 | Training Acc: 0.893 | Test Acc: 0.848 | Test Loss: 0.347 | Test AUC 0.851 (909.79 s) \n",
      "(*) Entering Epoch 26 (917.133 s)\n",
      "Epoch 26, Step 190 | Training Acc: 0.891 | Test Acc: 0.785 | Test Loss: 0.527 | Test AUC 0.803 (923.90 s) \n",
      "Epoch 26, Step 380 | Training Acc: 0.902 | Test Acc: 0.850 | Test Loss: 0.373 | Test AUC 0.860 (930.63 s) \n",
      "Epoch 26, Step 570 | Training Acc: 0.887 | Test Acc: 0.891 | Test Loss: 0.296 | Test AUC 0.876 (937.38 s) \n",
      "Epoch 26, Step 760 | Training Acc: 0.881 | Test Acc: 0.799 | Test Loss: 0.468 | Test AUC 0.813 (944.12 s) \n",
      "(*) Entering Epoch 27 (951.452 s)\n",
      "Epoch 27, Step 190 | Training Acc: 0.908 | Test Acc: 0.793 | Test Loss: 0.418 | Test AUC 0.825 (958.21 s) \n",
      "Epoch 27, Step 380 | Training Acc: 0.904 | Test Acc: 0.822 | Test Loss: 0.429 | Test AUC 0.833 (964.96 s) \n",
      "Epoch 27, Step 570 | Training Acc: 0.891 | Test Acc: 0.836 | Test Loss: 0.395 | Test AUC 0.838 (971.73 s) \n",
      "Epoch 27, Step 760 | Training Acc: 0.922 | Test Acc: 0.859 | Test Loss: 0.322 | Test AUC 0.877 (978.53 s) \n",
      "(*) Entering Epoch 28 (985.882 s)\n",
      "Epoch 28, Step 190 | Training Acc: 0.908 | Test Acc: 0.895 | Test Loss: 0.284 | Test AUC 0.889 (992.63 s) \n",
      "Epoch 28, Step 380 | Training Acc: 0.891 | Test Acc: 0.859 | Test Loss: 0.312 | Test AUC 0.863 (999.38 s) \n",
      "Epoch 28, Step 570 | Training Acc: 0.916 | Test Acc: 0.885 | Test Loss: 0.317 | Test AUC 0.895 (1006.11 s) \n",
      "Epoch 28, Step 760 | Training Acc: 0.908 | Test Acc: 0.826 | Test Loss: 0.385 | Test AUC 0.849 (1012.87 s) \n",
      "(*) Entering Epoch 29 (1020.219 s)\n",
      "Epoch 29, Step 190 | Training Acc: 0.904 | Test Acc: 0.904 | Test Loss: 0.281 | Test AUC 0.895 (1026.97 s) \n",
      "Epoch 29, Step 380 | Training Acc: 0.908 | Test Acc: 0.912 | Test Loss: 0.225 | Test AUC 0.902 (1034.68 s) [*]\n",
      "Epoch 29, Step 570 | Training Acc: 0.902 | Test Acc: 0.885 | Test Loss: 0.302 | Test AUC 0.886 (1041.42 s) \n",
      "Epoch 29, Step 760 | Training Acc: 0.887 | Test Acc: 0.879 | Test Loss: 0.291 | Test AUC 0.876 (1048.19 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch30.ckpt-30\n",
      "(*) Entering Epoch 30 (1056.443 s)\n",
      "Epoch 30, Step 190 | Training Acc: 0.920 | Test Acc: 0.803 | Test Loss: 0.479 | Test AUC 0.837 (1063.20 s) \n",
      "Epoch 30, Step 380 | Training Acc: 0.914 | Test Acc: 0.879 | Test Loss: 0.315 | Test AUC 0.886 (1069.96 s) \n",
      "Epoch 30, Step 570 | Training Acc: 0.908 | Test Acc: 0.779 | Test Loss: 0.461 | Test AUC 0.816 (1076.69 s) \n",
      "Epoch 30, Step 760 | Training Acc: 0.918 | Test Acc: 0.799 | Test Loss: 0.442 | Test AUC 0.835 (1083.43 s) \n",
      "(*) Entering Epoch 31 (1090.772 s)\n",
      "Epoch 31, Step 190 | Training Acc: 0.902 | Test Acc: 0.854 | Test Loss: 0.321 | Test AUC 0.868 (1097.52 s) \n",
      "Epoch 31, Step 380 | Training Acc: 0.926 | Test Acc: 0.865 | Test Loss: 0.297 | Test AUC 0.882 (1104.26 s) \n",
      "Epoch 31, Step 570 | Training Acc: 0.916 | Test Acc: 0.785 | Test Loss: 0.491 | Test AUC 0.824 (1111.00 s) \n",
      "Epoch 31, Step 760 | Training Acc: 0.881 | Test Acc: 0.844 | Test Loss: 0.366 | Test AUC 0.842 (1117.73 s) \n",
      "(*) Entering Epoch 32 (1125.067 s)\n",
      "Epoch 32, Step 190 | Training Acc: 0.883 | Test Acc: 0.898 | Test Loss: 0.256 | Test AUC 0.877 (1131.82 s) \n",
      "Epoch 32, Step 380 | Training Acc: 0.930 | Test Acc: 0.854 | Test Loss: 0.332 | Test AUC 0.884 (1138.56 s) \n",
      "Epoch 32, Step 570 | Training Acc: 0.916 | Test Acc: 0.885 | Test Loss: 0.273 | Test AUC 0.891 (1145.28 s) \n",
      "Epoch 32, Step 760 | Training Acc: 0.904 | Test Acc: 0.883 | Test Loss: 0.268 | Test AUC 0.883 (1152.03 s) \n",
      "(*) Entering Epoch 33 (1159.367 s)\n",
      "Epoch 33, Step 190 | Training Acc: 0.908 | Test Acc: 0.834 | Test Loss: 0.433 | Test AUC 0.850 (1166.10 s) \n",
      "Epoch 33, Step 380 | Training Acc: 0.920 | Test Acc: 0.881 | Test Loss: 0.342 | Test AUC 0.893 (1172.84 s) \n",
      "Epoch 33, Step 570 | Training Acc: 0.891 | Test Acc: 0.863 | Test Loss: 0.334 | Test AUC 0.861 (1179.59 s) \n",
      "Epoch 33, Step 760 | Training Acc: 0.895 | Test Acc: 0.904 | Test Loss: 0.229 | Test AUC 0.888 (1186.32 s) \n",
      "(*) Entering Epoch 34 (1193.652 s)\n",
      "Epoch 34, Step 190 | Training Acc: 0.926 | Test Acc: 0.889 | Test Loss: 0.286 | Test AUC 0.902 (1200.41 s) \n",
      "Epoch 34, Step 380 | Training Acc: 0.906 | Test Acc: 0.879 | Test Loss: 0.266 | Test AUC 0.880 (1207.14 s) \n",
      "Epoch 34, Step 570 | Training Acc: 0.896 | Test Acc: 0.875 | Test Loss: 0.315 | Test AUC 0.877 (1213.90 s) \n",
      "Epoch 34, Step 760 | Training Acc: 0.910 | Test Acc: 0.852 | Test Loss: 0.368 | Test AUC 0.870 (1220.63 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) Entering Epoch 35 (1227.974 s)\n",
      "Epoch 35, Step 190 | Training Acc: 0.906 | Test Acc: 0.887 | Test Loss: 0.296 | Test AUC 0.889 (1234.71 s) \n",
      "Epoch 35, Step 380 | Training Acc: 0.902 | Test Acc: 0.902 | Test Loss: 0.243 | Test AUC 0.896 (1241.46 s) \n",
      "Epoch 35, Step 570 | Training Acc: 0.910 | Test Acc: 0.820 | Test Loss: 0.416 | Test AUC 0.849 (1248.19 s) \n",
      "Epoch 35, Step 760 | Training Acc: 0.908 | Test Acc: 0.879 | Test Loss: 0.304 | Test AUC 0.882 (1254.92 s) \n",
      "(*) Entering Epoch 36 (1262.254 s)\n",
      "Epoch 36, Step 190 | Training Acc: 0.900 | Test Acc: 0.881 | Test Loss: 0.271 | Test AUC 0.886 (1269.00 s) \n",
      "Epoch 36, Step 380 | Training Acc: 0.904 | Test Acc: 0.908 | Test Loss: 0.232 | Test AUC 0.898 (1275.75 s) \n",
      "Epoch 36, Step 570 | Training Acc: 0.910 | Test Acc: 0.893 | Test Loss: 0.302 | Test AUC 0.893 (1282.55 s) \n",
      "Epoch 36, Step 760 | Training Acc: 0.885 | Test Acc: 0.879 | Test Loss: 0.313 | Test AUC 0.874 (1289.27 s) \n",
      "(*) Entering Epoch 37 (1296.624 s)\n",
      "Epoch 37, Step 190 | Training Acc: 0.902 | Test Acc: 0.836 | Test Loss: 0.389 | Test AUC 0.843 (1303.35 s) \n",
      "Epoch 37, Step 380 | Training Acc: 0.902 | Test Acc: 0.879 | Test Loss: 0.304 | Test AUC 0.886 (1310.09 s) \n",
      "Epoch 37, Step 570 | Training Acc: 0.893 | Test Acc: 0.867 | Test Loss: 0.302 | Test AUC 0.861 (1316.86 s) \n",
      "Epoch 37, Step 760 | Training Acc: 0.912 | Test Acc: 0.869 | Test Loss: 0.322 | Test AUC 0.884 (1323.62 s) \n",
      "(*) Entering Epoch 38 (1330.940 s)\n",
      "Epoch 38, Step 190 | Training Acc: 0.896 | Test Acc: 0.879 | Test Loss: 0.325 | Test AUC 0.878 (1337.68 s) \n",
      "Epoch 38, Step 380 | Training Acc: 0.906 | Test Acc: 0.898 | Test Loss: 0.258 | Test AUC 0.894 (1344.44 s) \n",
      "Epoch 38, Step 570 | Training Acc: 0.906 | Test Acc: 0.820 | Test Loss: 0.390 | Test AUC 0.837 (1351.19 s) \n",
      "Epoch 38, Step 760 | Training Acc: 0.893 | Test Acc: 0.824 | Test Loss: 0.386 | Test AUC 0.839 (1357.93 s) \n",
      "(*) Entering Epoch 39 (1365.285 s)\n",
      "Epoch 39, Step 190 | Training Acc: 0.912 | Test Acc: 0.865 | Test Loss: 0.314 | Test AUC 0.878 (1372.02 s) \n",
      "Epoch 39, Step 380 | Training Acc: 0.885 | Test Acc: 0.754 | Test Loss: 0.566 | Test AUC 0.788 (1378.78 s) \n",
      "Epoch 39, Step 570 | Training Acc: 0.893 | Test Acc: 0.850 | Test Loss: 0.340 | Test AUC 0.851 (1385.52 s) \n",
      "Epoch 39, Step 760 | Training Acc: 0.930 | Test Acc: 0.885 | Test Loss: 0.303 | Test AUC 0.896 (1392.29 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch40.ckpt-40\n",
      "(*) Entering Epoch 40 (1400.586 s)\n",
      "Epoch 40, Step 190 | Training Acc: 0.922 | Test Acc: 0.877 | Test Loss: 0.297 | Test AUC 0.883 (1407.33 s) \n",
      "Epoch 40, Step 380 | Training Acc: 0.865 | Test Acc: 0.873 | Test Loss: 0.311 | Test AUC 0.855 (1414.07 s) \n",
      "Epoch 40, Step 570 | Training Acc: 0.928 | Test Acc: 0.846 | Test Loss: 0.373 | Test AUC 0.879 (1420.82 s) \n",
      "Epoch 40, Step 760 | Training Acc: 0.908 | Test Acc: 0.896 | Test Loss: 0.251 | Test AUC 0.896 (1427.58 s) \n",
      "(*) Entering Epoch 41 (1434.895 s)\n",
      "Epoch 41, Step 190 | Training Acc: 0.895 | Test Acc: 0.809 | Test Loss: 0.467 | Test AUC 0.836 (1441.65 s) \n",
      "Epoch 41, Step 380 | Training Acc: 0.883 | Test Acc: 0.861 | Test Loss: 0.359 | Test AUC 0.860 (1448.38 s) \n",
      "Epoch 41, Step 570 | Training Acc: 0.908 | Test Acc: 0.867 | Test Loss: 0.299 | Test AUC 0.876 (1455.14 s) \n",
      "Epoch 41, Step 760 | Training Acc: 0.904 | Test Acc: 0.902 | Test Loss: 0.229 | Test AUC 0.895 (1461.88 s) \n",
      "(*) Entering Epoch 42 (1469.246 s)\n",
      "Epoch 42, Step 190 | Training Acc: 0.914 | Test Acc: 0.824 | Test Loss: 0.480 | Test AUC 0.852 (1475.99 s) \n",
      "Epoch 42, Step 380 | Training Acc: 0.900 | Test Acc: 0.822 | Test Loss: 0.436 | Test AUC 0.835 (1482.72 s) \n",
      "Epoch 42, Step 570 | Training Acc: 0.904 | Test Acc: 0.865 | Test Loss: 0.337 | Test AUC 0.870 (1489.48 s) \n",
      "Epoch 42, Step 760 | Training Acc: 0.908 | Test Acc: 0.883 | Test Loss: 0.297 | Test AUC 0.886 (1496.22 s) \n",
      "(*) Entering Epoch 43 (1503.560 s)\n",
      "Epoch 43, Step 190 | Training Acc: 0.898 | Test Acc: 0.902 | Test Loss: 0.253 | Test AUC 0.889 (1510.30 s) \n",
      "Epoch 43, Step 380 | Training Acc: 0.928 | Test Acc: 0.889 | Test Loss: 0.270 | Test AUC 0.903 (1517.05 s) \n",
      "Epoch 43, Step 570 | Training Acc: 0.904 | Test Acc: 0.885 | Test Loss: 0.269 | Test AUC 0.888 (1523.79 s) \n",
      "Epoch 43, Step 760 | Training Acc: 0.912 | Test Acc: 0.867 | Test Loss: 0.319 | Test AUC 0.880 (1530.55 s) \n",
      "(*) Entering Epoch 44 (1537.875 s)\n",
      "Epoch 44, Step 190 | Training Acc: 0.928 | Test Acc: 0.865 | Test Loss: 0.312 | Test AUC 0.891 (1544.62 s) \n",
      "Epoch 44, Step 380 | Training Acc: 0.904 | Test Acc: 0.883 | Test Loss: 0.267 | Test AUC 0.882 (1551.38 s) \n",
      "Epoch 44, Step 570 | Training Acc: 0.924 | Test Acc: 0.887 | Test Loss: 0.261 | Test AUC 0.901 (1558.13 s) \n",
      "Epoch 44, Step 760 | Training Acc: 0.928 | Test Acc: 0.879 | Test Loss: 0.318 | Test AUC 0.891 (1564.88 s) \n",
      "(*) Entering Epoch 45 (1572.229 s)\n",
      "Epoch 45, Step 190 | Training Acc: 0.914 | Test Acc: 0.881 | Test Loss: 0.259 | Test AUC 0.882 (1578.98 s) \n",
      "Epoch 45, Step 380 | Training Acc: 0.889 | Test Acc: 0.857 | Test Loss: 0.352 | Test AUC 0.858 (1585.73 s) \n",
      "Epoch 45, Step 570 | Training Acc: 0.893 | Test Acc: 0.814 | Test Loss: 0.409 | Test AUC 0.831 (1592.50 s) \n",
      "Epoch 45, Step 760 | Training Acc: 0.904 | Test Acc: 0.820 | Test Loss: 0.407 | Test AUC 0.843 (1599.24 s) \n",
      "(*) Entering Epoch 46 (1606.572 s)\n",
      "Epoch 46, Step 190 | Training Acc: 0.910 | Test Acc: 0.871 | Test Loss: 0.292 | Test AUC 0.880 (1613.32 s) \n",
      "Epoch 46, Step 380 | Training Acc: 0.922 | Test Acc: 0.877 | Test Loss: 0.296 | Test AUC 0.889 (1620.06 s) \n",
      "Epoch 46, Step 570 | Training Acc: 0.926 | Test Acc: 0.861 | Test Loss: 0.307 | Test AUC 0.886 (1626.78 s) \n",
      "Epoch 46, Step 760 | Training Acc: 0.906 | Test Acc: 0.752 | Test Loss: 0.540 | Test AUC 0.796 (1633.53 s) \n",
      "(*) Entering Epoch 47 (1640.858 s)\n",
      "Epoch 47, Step 190 | Training Acc: 0.914 | Test Acc: 0.820 | Test Loss: 0.484 | Test AUC 0.841 (1647.62 s) \n",
      "Epoch 47, Step 380 | Training Acc: 0.926 | Test Acc: 0.801 | Test Loss: 0.439 | Test AUC 0.844 (1654.36 s) \n",
      "Epoch 47, Step 570 | Training Acc: 0.908 | Test Acc: 0.842 | Test Loss: 0.341 | Test AUC 0.858 (1661.12 s) \n",
      "Epoch 47, Step 760 | Training Acc: 0.916 | Test Acc: 0.859 | Test Loss: 0.326 | Test AUC 0.876 (1667.87 s) \n",
      "(*) Entering Epoch 48 (1675.207 s)\n",
      "Epoch 48, Step 190 | Training Acc: 0.924 | Test Acc: 0.857 | Test Loss: 0.328 | Test AUC 0.879 (1681.98 s) \n",
      "Epoch 48, Step 380 | Training Acc: 0.926 | Test Acc: 0.879 | Test Loss: 0.323 | Test AUC 0.896 (1688.72 s) \n",
      "Epoch 48, Step 570 | Training Acc: 0.930 | Test Acc: 0.885 | Test Loss: 0.288 | Test AUC 0.902 (1695.45 s) \n",
      "Epoch 48, Step 760 | Training Acc: 0.906 | Test Acc: 0.854 | Test Loss: 0.365 | Test AUC 0.867 (1702.27 s) \n",
      "(*) Entering Epoch 49 (1709.591 s)\n",
      "Epoch 49, Step 190 | Training Acc: 0.889 | Test Acc: 0.854 | Test Loss: 0.340 | Test AUC 0.850 (1716.33 s) \n",
      "Epoch 49, Step 380 | Training Acc: 0.918 | Test Acc: 0.895 | Test Loss: 0.261 | Test AUC 0.895 (1723.07 s) \n",
      "Epoch 49, Step 570 | Training Acc: 0.900 | Test Acc: 0.766 | Test Loss: 0.582 | Test AUC 0.802 (1729.82 s) \n",
      "Epoch 49, Step 760 | Training Acc: 0.920 | Test Acc: 0.850 | Test Loss: 0.397 | Test AUC 0.874 (1736.57 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch50.ckpt-50\n",
      "(*) Entering Epoch 50 (1744.951 s)\n",
      "Epoch 50, Step 190 | Training Acc: 0.922 | Test Acc: 0.871 | Test Loss: 0.327 | Test AUC 0.888 (1751.75 s) \n",
      "Epoch 50, Step 380 | Training Acc: 0.930 | Test Acc: 0.891 | Test Loss: 0.273 | Test AUC 0.899 (1758.51 s) \n",
      "Epoch 50, Step 570 | Training Acc: 0.908 | Test Acc: 0.891 | Test Loss: 0.253 | Test AUC 0.890 (1765.26 s) \n",
      "Epoch 50, Step 760 | Training Acc: 0.920 | Test Acc: 0.895 | Test Loss: 0.269 | Test AUC 0.903 (1772.01 s) \n",
      "(*) Entering Epoch 51 (1779.312 s)\n",
      "Epoch 51, Step 190 | Training Acc: 0.918 | Test Acc: 0.883 | Test Loss: 0.288 | Test AUC 0.893 (1786.05 s) \n",
      "Epoch 51, Step 380 | Training Acc: 0.916 | Test Acc: 0.795 | Test Loss: 0.477 | Test AUC 0.834 (1792.80 s) \n",
      "Epoch 51, Step 570 | Training Acc: 0.902 | Test Acc: 0.789 | Test Loss: 0.507 | Test AUC 0.819 (1799.55 s) \n",
      "Epoch 51, Step 760 | Training Acc: 0.885 | Test Acc: 0.777 | Test Loss: 0.489 | Test AUC 0.799 (1806.29 s) \n",
      "(*) Entering Epoch 52 (1813.656 s)\n",
      "Epoch 52, Step 190 | Training Acc: 0.914 | Test Acc: 0.857 | Test Loss: 0.323 | Test AUC 0.873 (1820.41 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Step 380 | Training Acc: 0.918 | Test Acc: 0.770 | Test Loss: 0.613 | Test AUC 0.800 (1827.17 s) \n",
      "Epoch 52, Step 570 | Training Acc: 0.926 | Test Acc: 0.795 | Test Loss: 0.451 | Test AUC 0.844 (1833.92 s) \n",
      "Epoch 52, Step 760 | Training Acc: 0.912 | Test Acc: 0.820 | Test Loss: 0.460 | Test AUC 0.848 (1840.67 s) \n",
      "(*) Entering Epoch 53 (1847.992 s)\n",
      "Epoch 53, Step 190 | Training Acc: 0.926 | Test Acc: 0.871 | Test Loss: 0.314 | Test AUC 0.887 (1854.71 s) \n",
      "Epoch 53, Step 380 | Training Acc: 0.918 | Test Acc: 0.902 | Test Loss: 0.270 | Test AUC 0.900 (1861.45 s) \n",
      "Epoch 53, Step 570 | Training Acc: 0.924 | Test Acc: 0.916 | Test Loss: 0.257 | Test AUC 0.913 (1869.17 s) [*]\n",
      "Epoch 53, Step 760 | Training Acc: 0.922 | Test Acc: 0.896 | Test Loss: 0.270 | Test AUC 0.900 (1875.92 s) \n",
      "(*) Entering Epoch 54 (1883.230 s)\n",
      "Epoch 54, Step 190 | Training Acc: 0.934 | Test Acc: 0.875 | Test Loss: 0.308 | Test AUC 0.893 (1889.98 s) \n",
      "Epoch 54, Step 380 | Training Acc: 0.928 | Test Acc: 0.885 | Test Loss: 0.276 | Test AUC 0.890 (1896.73 s) \n",
      "Epoch 54, Step 570 | Training Acc: 0.906 | Test Acc: 0.889 | Test Loss: 0.254 | Test AUC 0.892 (1903.47 s) \n",
      "Epoch 54, Step 760 | Training Acc: 0.926 | Test Acc: 0.885 | Test Loss: 0.291 | Test AUC 0.897 (1910.22 s) \n",
      "(*) Entering Epoch 55 (1917.534 s)\n",
      "Epoch 55, Step 190 | Training Acc: 0.922 | Test Acc: 0.877 | Test Loss: 0.314 | Test AUC 0.891 (1924.29 s) \n",
      "Epoch 55, Step 380 | Training Acc: 0.936 | Test Acc: 0.883 | Test Loss: 0.285 | Test AUC 0.903 (1931.05 s) \n",
      "Epoch 55, Step 570 | Training Acc: 0.920 | Test Acc: 0.814 | Test Loss: 0.455 | Test AUC 0.844 (1937.78 s) \n",
      "Epoch 55, Step 760 | Training Acc: 0.910 | Test Acc: 0.775 | Test Loss: 0.561 | Test AUC 0.822 (1944.52 s) \n",
      "(*) Entering Epoch 56 (1951.871 s)\n",
      "Epoch 56, Step 190 | Training Acc: 0.936 | Test Acc: 0.822 | Test Loss: 0.403 | Test AUC 0.862 (1958.61 s) \n",
      "Epoch 56, Step 380 | Training Acc: 0.904 | Test Acc: 0.877 | Test Loss: 0.313 | Test AUC 0.882 (1965.36 s) \n",
      "Epoch 56, Step 570 | Training Acc: 0.918 | Test Acc: 0.879 | Test Loss: 0.315 | Test AUC 0.889 (1972.10 s) \n",
      "Epoch 56, Step 760 | Training Acc: 0.928 | Test Acc: 0.857 | Test Loss: 0.334 | Test AUC 0.887 (1978.84 s) \n",
      "(*) Entering Epoch 57 (1986.161 s)\n",
      "Epoch 57, Step 190 | Training Acc: 0.934 | Test Acc: 0.893 | Test Loss: 0.283 | Test AUC 0.909 (1992.93 s) \n",
      "Epoch 57, Step 380 | Training Acc: 0.920 | Test Acc: 0.869 | Test Loss: 0.334 | Test AUC 0.883 (1999.64 s) \n",
      "Epoch 57, Step 570 | Training Acc: 0.918 | Test Acc: 0.885 | Test Loss: 0.316 | Test AUC 0.889 (2006.38 s) \n",
      "Epoch 57, Step 760 | Training Acc: 0.922 | Test Acc: 0.863 | Test Loss: 0.330 | Test AUC 0.886 (2013.13 s) \n",
      "(*) Entering Epoch 58 (2020.442 s)\n",
      "Epoch 58, Step 190 | Training Acc: 0.930 | Test Acc: 0.803 | Test Loss: 0.467 | Test AUC 0.847 (2027.17 s) \n",
      "Epoch 58, Step 380 | Training Acc: 0.902 | Test Acc: 0.848 | Test Loss: 0.365 | Test AUC 0.865 (2033.91 s) \n",
      "Epoch 58, Step 570 | Training Acc: 0.912 | Test Acc: 0.881 | Test Loss: 0.312 | Test AUC 0.884 (2040.68 s) \n",
      "Epoch 58, Step 760 | Training Acc: 0.912 | Test Acc: 0.877 | Test Loss: 0.272 | Test AUC 0.885 (2047.40 s) \n",
      "(*) Entering Epoch 59 (2054.762 s)\n",
      "Epoch 59, Step 190 | Training Acc: 0.906 | Test Acc: 0.795 | Test Loss: 0.496 | Test AUC 0.821 (2061.55 s) \n",
      "Epoch 59, Step 380 | Training Acc: 0.918 | Test Acc: 0.855 | Test Loss: 0.418 | Test AUC 0.871 (2068.28 s) \n",
      "Epoch 59, Step 570 | Training Acc: 0.922 | Test Acc: 0.861 | Test Loss: 0.348 | Test AUC 0.878 (2075.04 s) \n",
      "Epoch 59, Step 760 | Training Acc: 0.934 | Test Acc: 0.811 | Test Loss: 0.460 | Test AUC 0.846 (2081.78 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch60.ckpt-60\n",
      "(*) Entering Epoch 60 (2090.163 s)\n",
      "Epoch 60, Step 190 | Training Acc: 0.932 | Test Acc: 0.857 | Test Loss: 0.355 | Test AUC 0.884 (2096.90 s) \n",
      "Epoch 60, Step 380 | Training Acc: 0.916 | Test Acc: 0.799 | Test Loss: 0.494 | Test AUC 0.836 (2103.67 s) \n",
      "Epoch 60, Step 570 | Training Acc: 0.900 | Test Acc: 0.879 | Test Loss: 0.291 | Test AUC 0.879 (2110.40 s) \n",
      "Epoch 60, Step 760 | Training Acc: 0.930 | Test Acc: 0.877 | Test Loss: 0.315 | Test AUC 0.896 (2117.16 s) \n",
      "(*) Entering Epoch 61 (2124.483 s)\n",
      "Epoch 61, Step 190 | Training Acc: 0.912 | Test Acc: 0.830 | Test Loss: 0.446 | Test AUC 0.855 (2131.28 s) \n",
      "Epoch 61, Step 380 | Training Acc: 0.922 | Test Acc: 0.838 | Test Loss: 0.411 | Test AUC 0.866 (2138.03 s) \n",
      "Epoch 61, Step 570 | Training Acc: 0.922 | Test Acc: 0.855 | Test Loss: 0.347 | Test AUC 0.881 (2144.81 s) \n",
      "Epoch 61, Step 760 | Training Acc: 0.916 | Test Acc: 0.889 | Test Loss: 0.287 | Test AUC 0.896 (2151.57 s) \n",
      "(*) Entering Epoch 62 (2158.915 s)\n",
      "Epoch 62, Step 190 | Training Acc: 0.924 | Test Acc: 0.852 | Test Loss: 0.351 | Test AUC 0.881 (2165.66 s) \n",
      "Epoch 62, Step 380 | Training Acc: 0.930 | Test Acc: 0.867 | Test Loss: 0.315 | Test AUC 0.891 (2172.41 s) \n",
      "Epoch 62, Step 570 | Training Acc: 0.928 | Test Acc: 0.836 | Test Loss: 0.380 | Test AUC 0.866 (2179.17 s) \n",
      "Epoch 62, Step 760 | Training Acc: 0.920 | Test Acc: 0.863 | Test Loss: 0.311 | Test AUC 0.883 (2185.92 s) \n",
      "(*) Entering Epoch 63 (2193.238 s)\n",
      "Epoch 63, Step 190 | Training Acc: 0.918 | Test Acc: 0.912 | Test Loss: 0.258 | Test AUC 0.911 (2200.00 s) \n",
      "Epoch 63, Step 380 | Training Acc: 0.941 | Test Acc: 0.785 | Test Loss: 0.542 | Test AUC 0.836 (2206.75 s) \n",
      "Epoch 63, Step 570 | Training Acc: 0.936 | Test Acc: 0.805 | Test Loss: 0.468 | Test AUC 0.853 (2213.49 s) \n",
      "Epoch 63, Step 760 | Training Acc: 0.895 | Test Acc: 0.857 | Test Loss: 0.378 | Test AUC 0.859 (2220.23 s) \n",
      "(*) Entering Epoch 64 (2227.562 s)\n",
      "Epoch 64, Step 190 | Training Acc: 0.928 | Test Acc: 0.816 | Test Loss: 0.461 | Test AUC 0.858 (2234.31 s) \n",
      "Epoch 64, Step 380 | Training Acc: 0.908 | Test Acc: 0.824 | Test Loss: 0.472 | Test AUC 0.853 (2241.05 s) \n",
      "Epoch 64, Step 570 | Training Acc: 0.910 | Test Acc: 0.873 | Test Loss: 0.302 | Test AUC 0.884 (2247.79 s) \n",
      "Epoch 64, Step 760 | Training Acc: 0.922 | Test Acc: 0.904 | Test Loss: 0.272 | Test AUC 0.907 (2254.55 s) \n",
      "(*) Entering Epoch 65 (2261.880 s)\n",
      "Epoch 65, Step 190 | Training Acc: 0.918 | Test Acc: 0.893 | Test Loss: 0.296 | Test AUC 0.902 (2268.63 s) \n",
      "Epoch 65, Step 380 | Training Acc: 0.920 | Test Acc: 0.795 | Test Loss: 0.482 | Test AUC 0.834 (2275.37 s) \n",
      "Epoch 65, Step 570 | Training Acc: 0.928 | Test Acc: 0.838 | Test Loss: 0.394 | Test AUC 0.871 (2282.11 s) \n",
      "Epoch 65, Step 760 | Training Acc: 0.910 | Test Acc: 0.887 | Test Loss: 0.269 | Test AUC 0.886 (2288.87 s) \n",
      "(*) Entering Epoch 66 (2296.217 s)\n",
      "Epoch 66, Step 190 | Training Acc: 0.910 | Test Acc: 0.885 | Test Loss: 0.324 | Test AUC 0.891 (2302.96 s) \n",
      "Epoch 66, Step 380 | Training Acc: 0.904 | Test Acc: 0.793 | Test Loss: 0.524 | Test AUC 0.824 (2309.71 s) \n",
      "Epoch 66, Step 570 | Training Acc: 0.941 | Test Acc: 0.836 | Test Loss: 0.401 | Test AUC 0.875 (2316.46 s) \n",
      "Epoch 66, Step 760 | Training Acc: 0.926 | Test Acc: 0.814 | Test Loss: 0.430 | Test AUC 0.849 (2323.21 s) \n",
      "(*) Entering Epoch 67 (2330.584 s)\n",
      "Epoch 67, Step 190 | Training Acc: 0.924 | Test Acc: 0.885 | Test Loss: 0.284 | Test AUC 0.895 (2337.33 s) \n",
      "Epoch 67, Step 380 | Training Acc: 0.930 | Test Acc: 0.883 | Test Loss: 0.325 | Test AUC 0.900 (2344.08 s) \n",
      "Epoch 67, Step 570 | Training Acc: 0.918 | Test Acc: 0.863 | Test Loss: 0.306 | Test AUC 0.885 (2350.85 s) \n",
      "Epoch 67, Step 760 | Training Acc: 0.914 | Test Acc: 0.898 | Test Loss: 0.278 | Test AUC 0.903 (2357.58 s) \n",
      "(*) Entering Epoch 68 (2364.913 s)\n",
      "Epoch 68, Step 190 | Training Acc: 0.930 | Test Acc: 0.869 | Test Loss: 0.333 | Test AUC 0.889 (2371.66 s) \n",
      "Epoch 68, Step 380 | Training Acc: 0.916 | Test Acc: 0.900 | Test Loss: 0.286 | Test AUC 0.900 (2378.43 s) \n",
      "Epoch 68, Step 570 | Training Acc: 0.924 | Test Acc: 0.893 | Test Loss: 0.286 | Test AUC 0.904 (2385.19 s) \n",
      "Epoch 68, Step 760 | Training Acc: 0.918 | Test Acc: 0.861 | Test Loss: 0.386 | Test AUC 0.883 (2391.92 s) \n",
      "(*) Entering Epoch 69 (2399.255 s)\n",
      "Epoch 69, Step 190 | Training Acc: 0.920 | Test Acc: 0.820 | Test Loss: 0.496 | Test AUC 0.851 (2406.02 s) \n",
      "Epoch 69, Step 380 | Training Acc: 0.916 | Test Acc: 0.775 | Test Loss: 0.602 | Test AUC 0.822 (2412.77 s) \n",
      "Epoch 69, Step 570 | Training Acc: 0.943 | Test Acc: 0.848 | Test Loss: 0.361 | Test AUC 0.880 (2419.54 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Step 760 | Training Acc: 0.928 | Test Acc: 0.865 | Test Loss: 0.316 | Test AUC 0.884 (2426.31 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch70.ckpt-70\n",
      "(*) Entering Epoch 70 (2434.598 s)\n",
      "Epoch 70, Step 190 | Training Acc: 0.920 | Test Acc: 0.873 | Test Loss: 0.369 | Test AUC 0.892 (2441.34 s) \n",
      "Epoch 70, Step 380 | Training Acc: 0.920 | Test Acc: 0.809 | Test Loss: 0.472 | Test AUC 0.843 (2448.09 s) \n",
      "Epoch 70, Step 570 | Training Acc: 0.926 | Test Acc: 0.805 | Test Loss: 0.505 | Test AUC 0.841 (2454.84 s) \n",
      "Epoch 70, Step 760 | Training Acc: 0.932 | Test Acc: 0.854 | Test Loss: 0.397 | Test AUC 0.878 (2461.58 s) \n",
      "(*) Entering Epoch 71 (2468.894 s)\n",
      "Epoch 71, Step 190 | Training Acc: 0.926 | Test Acc: 0.867 | Test Loss: 0.342 | Test AUC 0.888 (2475.63 s) \n",
      "Epoch 71, Step 380 | Training Acc: 0.910 | Test Acc: 0.881 | Test Loss: 0.316 | Test AUC 0.885 (2482.37 s) \n",
      "Epoch 71, Step 570 | Training Acc: 0.914 | Test Acc: 0.879 | Test Loss: 0.289 | Test AUC 0.887 (2489.11 s) \n",
      "Epoch 71, Step 760 | Training Acc: 0.900 | Test Acc: 0.867 | Test Loss: 0.300 | Test AUC 0.875 (2495.87 s) \n",
      "(*) Entering Epoch 72 (2503.165 s)\n",
      "Epoch 72, Step 190 | Training Acc: 0.914 | Test Acc: 0.891 | Test Loss: 0.281 | Test AUC 0.898 (2509.92 s) \n",
      "Epoch 72, Step 380 | Training Acc: 0.887 | Test Acc: 0.822 | Test Loss: 0.452 | Test AUC 0.832 (2516.70 s) \n",
      "Epoch 72, Step 570 | Training Acc: 0.939 | Test Acc: 0.873 | Test Loss: 0.341 | Test AUC 0.893 (2523.43 s) \n",
      "Epoch 72, Step 760 | Training Acc: 0.922 | Test Acc: 0.779 | Test Loss: 0.547 | Test AUC 0.827 (2530.17 s) \n",
      "(*) Entering Epoch 73 (2537.525 s)\n",
      "Epoch 73, Step 190 | Training Acc: 0.908 | Test Acc: 0.811 | Test Loss: 0.455 | Test AUC 0.838 (2544.27 s) \n",
      "Epoch 73, Step 380 | Training Acc: 0.936 | Test Acc: 0.865 | Test Loss: 0.388 | Test AUC 0.893 (2551.08 s) \n",
      "Epoch 73, Step 570 | Training Acc: 0.943 | Test Acc: 0.820 | Test Loss: 0.474 | Test AUC 0.858 (2557.85 s) \n",
      "Epoch 73, Step 760 | Training Acc: 0.918 | Test Acc: 0.811 | Test Loss: 0.446 | Test AUC 0.844 (2564.61 s) \n",
      "(*) Entering Epoch 74 (2571.953 s)\n",
      "Epoch 74, Step 190 | Training Acc: 0.922 | Test Acc: 0.889 | Test Loss: 0.313 | Test AUC 0.899 (2578.70 s) \n",
      "Epoch 74, Step 380 | Training Acc: 0.938 | Test Acc: 0.889 | Test Loss: 0.279 | Test AUC 0.905 (2585.44 s) \n",
      "Epoch 74, Step 570 | Training Acc: 0.920 | Test Acc: 0.879 | Test Loss: 0.320 | Test AUC 0.888 (2592.19 s) \n",
      "Epoch 74, Step 760 | Training Acc: 0.916 | Test Acc: 0.869 | Test Loss: 0.329 | Test AUC 0.885 (2598.91 s) \n",
      "(*) Entering Epoch 75 (2606.265 s)\n",
      "Epoch 75, Step 190 | Training Acc: 0.941 | Test Acc: 0.891 | Test Loss: 0.325 | Test AUC 0.911 (2613.04 s) \n",
      "Epoch 75, Step 380 | Training Acc: 0.922 | Test Acc: 0.867 | Test Loss: 0.334 | Test AUC 0.890 (2619.79 s) \n",
      "Epoch 75, Step 570 | Training Acc: 0.918 | Test Acc: 0.871 | Test Loss: 0.337 | Test AUC 0.884 (2626.54 s) \n",
      "Epoch 75, Step 760 | Training Acc: 0.934 | Test Acc: 0.883 | Test Loss: 0.272 | Test AUC 0.905 (2633.28 s) \n",
      "(*) Entering Epoch 76 (2640.612 s)\n",
      "Epoch 76, Step 190 | Training Acc: 0.926 | Test Acc: 0.875 | Test Loss: 0.367 | Test AUC 0.894 (2647.35 s) \n",
      "Epoch 76, Step 380 | Training Acc: 0.939 | Test Acc: 0.863 | Test Loss: 0.350 | Test AUC 0.891 (2654.08 s) \n",
      "Epoch 76, Step 570 | Training Acc: 0.926 | Test Acc: 0.865 | Test Loss: 0.367 | Test AUC 0.886 (2660.82 s) \n",
      "Epoch 76, Step 760 | Training Acc: 0.934 | Test Acc: 0.893 | Test Loss: 0.284 | Test AUC 0.907 (2667.57 s) \n",
      "(*) Entering Epoch 77 (2674.908 s)\n",
      "Epoch 77, Step 190 | Training Acc: 0.926 | Test Acc: 0.887 | Test Loss: 0.298 | Test AUC 0.901 (2681.68 s) \n",
      "Epoch 77, Step 380 | Training Acc: 0.916 | Test Acc: 0.871 | Test Loss: 0.316 | Test AUC 0.884 (2688.44 s) \n",
      "Epoch 77, Step 570 | Training Acc: 0.922 | Test Acc: 0.902 | Test Loss: 0.274 | Test AUC 0.905 (2695.21 s) \n",
      "Epoch 77, Step 760 | Training Acc: 0.930 | Test Acc: 0.875 | Test Loss: 0.328 | Test AUC 0.895 (2701.96 s) \n",
      "(*) Entering Epoch 78 (2709.293 s)\n",
      "Epoch 78, Step 190 | Training Acc: 0.912 | Test Acc: 0.900 | Test Loss: 0.251 | Test AUC 0.899 (2716.03 s) \n",
      "Epoch 78, Step 380 | Training Acc: 0.938 | Test Acc: 0.838 | Test Loss: 0.417 | Test AUC 0.875 (2722.76 s) \n",
      "Epoch 78, Step 570 | Training Acc: 0.922 | Test Acc: 0.852 | Test Loss: 0.355 | Test AUC 0.873 (2729.52 s) \n",
      "Epoch 78, Step 760 | Training Acc: 0.939 | Test Acc: 0.779 | Test Loss: 0.545 | Test AUC 0.835 (2736.29 s) \n",
      "(*) Entering Epoch 79 (2743.630 s)\n",
      "Epoch 79, Step 190 | Training Acc: 0.924 | Test Acc: 0.848 | Test Loss: 0.415 | Test AUC 0.873 (2750.40 s) \n",
      "Epoch 79, Step 380 | Training Acc: 0.930 | Test Acc: 0.820 | Test Loss: 0.482 | Test AUC 0.861 (2757.14 s) \n",
      "Epoch 79, Step 570 | Training Acc: 0.916 | Test Acc: 0.846 | Test Loss: 0.418 | Test AUC 0.866 (2763.88 s) \n",
      "Epoch 79, Step 760 | Training Acc: 0.916 | Test Acc: 0.803 | Test Loss: 0.532 | Test AUC 0.832 (2770.63 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch80.ckpt-80\n",
      "(*) Entering Epoch 80 (2778.948 s)\n",
      "Epoch 80, Step 190 | Training Acc: 0.951 | Test Acc: 0.871 | Test Loss: 0.402 | Test AUC 0.904 (2785.68 s) \n",
      "Epoch 80, Step 380 | Training Acc: 0.938 | Test Acc: 0.863 | Test Loss: 0.318 | Test AUC 0.881 (2792.43 s) \n",
      "Epoch 80, Step 570 | Training Acc: 0.920 | Test Acc: 0.879 | Test Loss: 0.310 | Test AUC 0.893 (2799.18 s) \n",
      "Epoch 80, Step 760 | Training Acc: 0.924 | Test Acc: 0.889 | Test Loss: 0.283 | Test AUC 0.898 (2805.93 s) \n",
      "(*) Entering Epoch 81 (2813.251 s)\n",
      "Epoch 81, Step 190 | Training Acc: 0.916 | Test Acc: 0.770 | Test Loss: 0.521 | Test AUC 0.815 (2819.98 s) \n",
      "Epoch 81, Step 380 | Training Acc: 0.936 | Test Acc: 0.816 | Test Loss: 0.479 | Test AUC 0.858 (2826.72 s) \n",
      "Epoch 81, Step 570 | Training Acc: 0.930 | Test Acc: 0.848 | Test Loss: 0.358 | Test AUC 0.877 (2833.49 s) \n",
      "Epoch 81, Step 760 | Training Acc: 0.910 | Test Acc: 0.859 | Test Loss: 0.375 | Test AUC 0.877 (2840.22 s) \n",
      "(*) Entering Epoch 82 (2847.582 s)\n",
      "Epoch 82, Step 190 | Training Acc: 0.939 | Test Acc: 0.797 | Test Loss: 0.526 | Test AUC 0.855 (2854.34 s) \n",
      "Epoch 82, Step 380 | Training Acc: 0.930 | Test Acc: 0.844 | Test Loss: 0.448 | Test AUC 0.873 (2861.10 s) \n",
      "Epoch 82, Step 570 | Training Acc: 0.898 | Test Acc: 0.896 | Test Loss: 0.294 | Test AUC 0.888 (2867.85 s) \n",
      "Epoch 82, Step 760 | Training Acc: 0.924 | Test Acc: 0.842 | Test Loss: 0.382 | Test AUC 0.872 (2874.59 s) \n",
      "(*) Entering Epoch 83 (2881.920 s)\n",
      "Epoch 83, Step 190 | Training Acc: 0.924 | Test Acc: 0.857 | Test Loss: 0.391 | Test AUC 0.887 (2888.64 s) \n",
      "Epoch 83, Step 380 | Training Acc: 0.947 | Test Acc: 0.898 | Test Loss: 0.275 | Test AUC 0.919 (2895.38 s) \n",
      "Epoch 83, Step 570 | Training Acc: 0.920 | Test Acc: 0.879 | Test Loss: 0.333 | Test AUC 0.890 (2902.14 s) \n",
      "Epoch 83, Step 760 | Training Acc: 0.926 | Test Acc: 0.883 | Test Loss: 0.341 | Test AUC 0.898 (2908.88 s) \n",
      "(*) Entering Epoch 84 (2916.186 s)\n",
      "Epoch 84, Step 190 | Training Acc: 0.920 | Test Acc: 0.801 | Test Loss: 0.504 | Test AUC 0.843 (2922.91 s) \n",
      "Epoch 84, Step 380 | Training Acc: 0.945 | Test Acc: 0.838 | Test Loss: 0.459 | Test AUC 0.873 (2929.65 s) \n",
      "Epoch 84, Step 570 | Training Acc: 0.920 | Test Acc: 0.857 | Test Loss: 0.386 | Test AUC 0.879 (2936.38 s) \n",
      "Epoch 84, Step 760 | Training Acc: 0.938 | Test Acc: 0.883 | Test Loss: 0.319 | Test AUC 0.904 (2943.12 s) \n",
      "(*) Entering Epoch 85 (2950.432 s)\n",
      "Epoch 85, Step 190 | Training Acc: 0.926 | Test Acc: 0.893 | Test Loss: 0.279 | Test AUC 0.904 (2957.19 s) \n",
      "Epoch 85, Step 380 | Training Acc: 0.939 | Test Acc: 0.865 | Test Loss: 0.340 | Test AUC 0.896 (2963.93 s) \n",
      "Epoch 85, Step 570 | Training Acc: 0.936 | Test Acc: 0.898 | Test Loss: 0.293 | Test AUC 0.912 (2970.73 s) \n",
      "Epoch 85, Step 760 | Training Acc: 0.932 | Test Acc: 0.877 | Test Loss: 0.297 | Test AUC 0.887 (2977.47 s) \n",
      "(*) Entering Epoch 86 (2984.771 s)\n",
      "Epoch 86, Step 190 | Training Acc: 0.932 | Test Acc: 0.777 | Test Loss: 0.613 | Test AUC 0.833 (2991.51 s) \n",
      "Epoch 86, Step 380 | Training Acc: 0.934 | Test Acc: 0.830 | Test Loss: 0.466 | Test AUC 0.856 (2998.26 s) \n",
      "Epoch 86, Step 570 | Training Acc: 0.939 | Test Acc: 0.844 | Test Loss: 0.397 | Test AUC 0.879 (3005.01 s) \n",
      "Epoch 86, Step 760 | Training Acc: 0.914 | Test Acc: 0.824 | Test Loss: 0.386 | Test AUC 0.851 (3011.77 s) \n",
      "(*) Entering Epoch 87 (3019.089 s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Step 190 | Training Acc: 0.916 | Test Acc: 0.857 | Test Loss: 0.369 | Test AUC 0.880 (3025.86 s) \n",
      "Epoch 87, Step 380 | Training Acc: 0.941 | Test Acc: 0.836 | Test Loss: 0.424 | Test AUC 0.874 (3032.61 s) \n",
      "Epoch 87, Step 570 | Training Acc: 0.910 | Test Acc: 0.867 | Test Loss: 0.380 | Test AUC 0.879 (3039.34 s) \n",
      "Epoch 87, Step 760 | Training Acc: 0.920 | Test Acc: 0.885 | Test Loss: 0.330 | Test AUC 0.897 (3046.10 s) \n",
      "(*) Entering Epoch 88 (3053.411 s)\n",
      "Epoch 88, Step 190 | Training Acc: 0.941 | Test Acc: 0.877 | Test Loss: 0.353 | Test AUC 0.901 (3060.16 s) \n",
      "Epoch 88, Step 380 | Training Acc: 0.928 | Test Acc: 0.879 | Test Loss: 0.356 | Test AUC 0.897 (3066.91 s) \n",
      "Epoch 88, Step 570 | Training Acc: 0.943 | Test Acc: 0.871 | Test Loss: 0.332 | Test AUC 0.901 (3073.65 s) \n",
      "Epoch 88, Step 760 | Training Acc: 0.932 | Test Acc: 0.875 | Test Loss: 0.277 | Test AUC 0.893 (3080.39 s) \n",
      "(*) Entering Epoch 89 (3087.730 s)\n",
      "Epoch 89, Step 190 | Training Acc: 0.914 | Test Acc: 0.793 | Test Loss: 0.561 | Test AUC 0.834 (3094.47 s) \n",
      "Epoch 89, Step 380 | Training Acc: 0.934 | Test Acc: 0.848 | Test Loss: 0.395 | Test AUC 0.876 (3101.19 s) \n",
      "Epoch 89, Step 570 | Training Acc: 0.916 | Test Acc: 0.852 | Test Loss: 0.445 | Test AUC 0.866 (3107.92 s) \n",
      "Epoch 89, Step 760 | Training Acc: 0.941 | Test Acc: 0.863 | Test Loss: 0.346 | Test AUC 0.894 (3114.67 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch90.ckpt-90\n",
      "(*) Entering Epoch 90 (3122.997 s)\n",
      "Epoch 90, Step 190 | Training Acc: 0.934 | Test Acc: 0.895 | Test Loss: 0.291 | Test AUC 0.908 (3129.74 s) \n",
      "Epoch 90, Step 380 | Training Acc: 0.920 | Test Acc: 0.902 | Test Loss: 0.279 | Test AUC 0.908 (3136.50 s) \n",
      "Epoch 90, Step 570 | Training Acc: 0.920 | Test Acc: 0.875 | Test Loss: 0.333 | Test AUC 0.893 (3143.25 s) \n",
      "Epoch 90, Step 760 | Training Acc: 0.938 | Test Acc: 0.896 | Test Loss: 0.335 | Test AUC 0.914 (3150.00 s) \n",
      "(*) Entering Epoch 91 (3157.314 s)\n",
      "Epoch 91, Step 190 | Training Acc: 0.936 | Test Acc: 0.889 | Test Loss: 0.314 | Test AUC 0.906 (3164.05 s) \n",
      "Epoch 91, Step 380 | Training Acc: 0.928 | Test Acc: 0.893 | Test Loss: 0.317 | Test AUC 0.900 (3170.82 s) \n",
      "Epoch 91, Step 570 | Training Acc: 0.932 | Test Acc: 0.801 | Test Loss: 0.529 | Test AUC 0.854 (3177.59 s) \n",
      "Epoch 91, Step 760 | Training Acc: 0.920 | Test Acc: 0.850 | Test Loss: 0.381 | Test AUC 0.870 (3184.36 s) \n",
      "(*) Entering Epoch 92 (3191.714 s)\n",
      "Epoch 92, Step 190 | Training Acc: 0.910 | Test Acc: 0.883 | Test Loss: 0.307 | Test AUC 0.892 (3198.47 s) \n",
      "Epoch 92, Step 380 | Training Acc: 0.934 | Test Acc: 0.887 | Test Loss: 0.313 | Test AUC 0.906 (3205.21 s) \n",
      "Epoch 92, Step 570 | Training Acc: 0.920 | Test Acc: 0.875 | Test Loss: 0.317 | Test AUC 0.886 (3211.96 s) \n",
      "Epoch 92, Step 760 | Training Acc: 0.930 | Test Acc: 0.795 | Test Loss: 0.523 | Test AUC 0.840 (3218.71 s) \n",
      "(*) Entering Epoch 93 (3226.008 s)\n",
      "Epoch 93, Step 190 | Training Acc: 0.938 | Test Acc: 0.787 | Test Loss: 0.591 | Test AUC 0.839 (3232.75 s) \n",
      "Epoch 93, Step 380 | Training Acc: 0.928 | Test Acc: 0.795 | Test Loss: 0.605 | Test AUC 0.842 (3239.52 s) \n",
      "Epoch 93, Step 570 | Training Acc: 0.953 | Test Acc: 0.832 | Test Loss: 0.531 | Test AUC 0.882 (3246.27 s) \n",
      "Epoch 93, Step 760 | Training Acc: 0.891 | Test Acc: 0.863 | Test Loss: 0.317 | Test AUC 0.868 (3253.02 s) \n",
      "(*) Entering Epoch 94 (3260.367 s)\n",
      "Epoch 94, Step 190 | Training Acc: 0.938 | Test Acc: 0.881 | Test Loss: 0.332 | Test AUC 0.902 (3267.12 s) \n",
      "Epoch 94, Step 380 | Training Acc: 0.957 | Test Acc: 0.867 | Test Loss: 0.387 | Test AUC 0.908 (3273.86 s) \n",
      "Epoch 94, Step 570 | Training Acc: 0.939 | Test Acc: 0.859 | Test Loss: 0.364 | Test AUC 0.891 (3280.62 s) \n",
      "Epoch 94, Step 760 | Training Acc: 0.945 | Test Acc: 0.863 | Test Loss: 0.358 | Test AUC 0.900 (3287.37 s) \n",
      "(*) Entering Epoch 95 (3294.687 s)\n",
      "Epoch 95, Step 190 | Training Acc: 0.945 | Test Acc: 0.830 | Test Loss: 0.417 | Test AUC 0.864 (3301.46 s) \n",
      "Epoch 95, Step 380 | Training Acc: 0.920 | Test Acc: 0.814 | Test Loss: 0.505 | Test AUC 0.850 (3308.20 s) \n",
      "Epoch 95, Step 570 | Training Acc: 0.938 | Test Acc: 0.871 | Test Loss: 0.418 | Test AUC 0.892 (3314.95 s) \n",
      "Epoch 95, Step 760 | Training Acc: 0.926 | Test Acc: 0.875 | Test Loss: 0.356 | Test AUC 0.894 (3321.72 s) \n",
      "(*) Entering Epoch 96 (3329.034 s)\n",
      "Epoch 96, Step 190 | Training Acc: 0.932 | Test Acc: 0.838 | Test Loss: 0.432 | Test AUC 0.869 (3335.75 s) \n",
      "Epoch 96, Step 380 | Training Acc: 0.928 | Test Acc: 0.855 | Test Loss: 0.448 | Test AUC 0.878 (3342.51 s) \n",
      "Epoch 96, Step 570 | Training Acc: 0.928 | Test Acc: 0.867 | Test Loss: 0.338 | Test AUC 0.886 (3349.25 s) \n",
      "Epoch 96, Step 760 | Training Acc: 0.939 | Test Acc: 0.805 | Test Loss: 0.491 | Test AUC 0.856 (3356.01 s) \n",
      "(*) Entering Epoch 97 (3363.366 s)\n",
      "Epoch 97, Step 190 | Training Acc: 0.961 | Test Acc: 0.838 | Test Loss: 0.381 | Test AUC 0.881 (3370.12 s) \n",
      "Epoch 97, Step 380 | Training Acc: 0.934 | Test Acc: 0.859 | Test Loss: 0.382 | Test AUC 0.888 (3376.88 s) \n",
      "Epoch 97, Step 570 | Training Acc: 0.938 | Test Acc: 0.877 | Test Loss: 0.304 | Test AUC 0.898 (3383.61 s) \n",
      "Epoch 97, Step 760 | Training Acc: 0.945 | Test Acc: 0.852 | Test Loss: 0.367 | Test AUC 0.888 (3390.38 s) \n",
      "(*) Entering Epoch 98 (3397.712 s)\n",
      "Epoch 98, Step 190 | Training Acc: 0.910 | Test Acc: 0.854 | Test Loss: 0.435 | Test AUC 0.873 (3404.46 s) \n",
      "Epoch 98, Step 380 | Training Acc: 0.926 | Test Acc: 0.896 | Test Loss: 0.314 | Test AUC 0.903 (3411.21 s) \n",
      "Epoch 98, Step 570 | Training Acc: 0.941 | Test Acc: 0.857 | Test Loss: 0.433 | Test AUC 0.889 (3417.96 s) \n",
      "Epoch 98, Step 760 | Training Acc: 0.936 | Test Acc: 0.842 | Test Loss: 0.377 | Test AUC 0.883 (3424.72 s) \n",
      "(*) Entering Epoch 99 (3432.046 s)\n",
      "Epoch 99, Step 190 | Training Acc: 0.947 | Test Acc: 0.814 | Test Loss: 0.500 | Test AUC 0.864 (3438.79 s) \n",
      "Epoch 99, Step 380 | Training Acc: 0.938 | Test Acc: 0.867 | Test Loss: 0.395 | Test AUC 0.894 (3445.55 s) \n",
      "Epoch 99, Step 570 | Training Acc: 0.934 | Test Acc: 0.818 | Test Loss: 0.438 | Test AUC 0.859 (3452.29 s) \n",
      "Epoch 99, Step 760 | Training Acc: 0.943 | Test Acc: 0.865 | Test Loss: 0.424 | Test AUC 0.891 (3459.04 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch100.ckpt-100\n",
      "(*) Entering Epoch 100 (3467.380 s)\n",
      "Epoch 100, Step 190 | Training Acc: 0.947 | Test Acc: 0.873 | Test Loss: 0.385 | Test AUC 0.904 (3474.12 s) \n",
      "Epoch 100, Step 380 | Training Acc: 0.926 | Test Acc: 0.879 | Test Loss: 0.333 | Test AUC 0.899 (3480.90 s) \n",
      "Epoch 100, Step 570 | Training Acc: 0.936 | Test Acc: 0.795 | Test Loss: 0.520 | Test AUC 0.843 (3487.65 s) \n",
      "Epoch 100, Step 760 | Training Acc: 0.920 | Test Acc: 0.852 | Test Loss: 0.406 | Test AUC 0.871 (3494.39 s) \n",
      "(*) Entering Epoch 101 (3501.742 s)\n",
      "Epoch 101, Step 190 | Training Acc: 0.934 | Test Acc: 0.902 | Test Loss: 0.265 | Test AUC 0.913 (3508.46 s) \n",
      "Epoch 101, Step 380 | Training Acc: 0.930 | Test Acc: 0.812 | Test Loss: 0.470 | Test AUC 0.851 (3515.23 s) \n",
      "Epoch 101, Step 570 | Training Acc: 0.922 | Test Acc: 0.863 | Test Loss: 0.454 | Test AUC 0.879 (3521.96 s) \n",
      "Epoch 101, Step 760 | Training Acc: 0.938 | Test Acc: 0.850 | Test Loss: 0.374 | Test AUC 0.886 (3528.71 s) \n",
      "(*) Entering Epoch 102 (3536.077 s)\n",
      "Epoch 102, Step 190 | Training Acc: 0.932 | Test Acc: 0.768 | Test Loss: 0.616 | Test AUC 0.821 (3542.85 s) \n",
      "Epoch 102, Step 380 | Training Acc: 0.924 | Test Acc: 0.834 | Test Loss: 0.439 | Test AUC 0.868 (3549.59 s) \n",
      "Epoch 102, Step 570 | Training Acc: 0.934 | Test Acc: 0.873 | Test Loss: 0.321 | Test AUC 0.895 (3556.35 s) \n",
      "Epoch 102, Step 760 | Training Acc: 0.938 | Test Acc: 0.834 | Test Loss: 0.456 | Test AUC 0.869 (3563.09 s) \n",
      "(*) Entering Epoch 103 (3570.415 s)\n",
      "Epoch 103, Step 190 | Training Acc: 0.939 | Test Acc: 0.885 | Test Loss: 0.314 | Test AUC 0.908 (3577.16 s) \n",
      "Epoch 103, Step 380 | Training Acc: 0.939 | Test Acc: 0.889 | Test Loss: 0.328 | Test AUC 0.910 (3583.91 s) \n",
      "Epoch 103, Step 570 | Training Acc: 0.941 | Test Acc: 0.881 | Test Loss: 0.302 | Test AUC 0.901 (3590.69 s) \n",
      "Epoch 103, Step 760 | Training Acc: 0.943 | Test Acc: 0.879 | Test Loss: 0.351 | Test AUC 0.902 (3597.48 s) \n",
      "(*) Entering Epoch 104 (3604.826 s)\n",
      "Epoch 104, Step 190 | Training Acc: 0.941 | Test Acc: 0.869 | Test Loss: 0.388 | Test AUC 0.901 (3611.59 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104, Step 380 | Training Acc: 0.928 | Test Acc: 0.797 | Test Loss: 0.583 | Test AUC 0.835 (3618.35 s) \n",
      "Epoch 104, Step 570 | Training Acc: 0.926 | Test Acc: 0.830 | Test Loss: 0.437 | Test AUC 0.862 (3625.11 s) \n",
      "Epoch 104, Step 760 | Training Acc: 0.947 | Test Acc: 0.828 | Test Loss: 0.476 | Test AUC 0.868 (3631.88 s) \n",
      "(*) Entering Epoch 105 (3639.205 s)\n",
      "Epoch 105, Step 190 | Training Acc: 0.922 | Test Acc: 0.855 | Test Loss: 0.395 | Test AUC 0.878 (3645.96 s) \n",
      "Epoch 105, Step 380 | Training Acc: 0.928 | Test Acc: 0.855 | Test Loss: 0.378 | Test AUC 0.882 (3652.74 s) \n",
      "Epoch 105, Step 570 | Training Acc: 0.906 | Test Acc: 0.883 | Test Loss: 0.374 | Test AUC 0.888 (3659.54 s) \n",
      "Epoch 105, Step 760 | Training Acc: 0.924 | Test Acc: 0.836 | Test Loss: 0.479 | Test AUC 0.867 (3666.33 s) \n",
      "(*) Entering Epoch 106 (3673.706 s)\n",
      "Epoch 106, Step 190 | Training Acc: 0.922 | Test Acc: 0.793 | Test Loss: 0.632 | Test AUC 0.836 (3680.49 s) \n",
      "Epoch 106, Step 380 | Training Acc: 0.926 | Test Acc: 0.828 | Test Loss: 0.512 | Test AUC 0.866 (3687.31 s) \n",
      "Epoch 106, Step 570 | Training Acc: 0.910 | Test Acc: 0.842 | Test Loss: 0.402 | Test AUC 0.857 (3694.04 s) \n",
      "Epoch 106, Step 760 | Training Acc: 0.908 | Test Acc: 0.877 | Test Loss: 0.300 | Test AUC 0.887 (3700.78 s) \n",
      "(*) Entering Epoch 107 (3708.135 s)\n",
      "Epoch 107, Step 190 | Training Acc: 0.918 | Test Acc: 0.842 | Test Loss: 0.436 | Test AUC 0.870 (3714.93 s) \n",
      "Epoch 107, Step 380 | Training Acc: 0.934 | Test Acc: 0.832 | Test Loss: 0.407 | Test AUC 0.875 (3721.71 s) \n",
      "Epoch 107, Step 570 | Training Acc: 0.967 | Test Acc: 0.879 | Test Loss: 0.323 | Test AUC 0.920 (3728.47 s) \n",
      "Epoch 107, Step 760 | Training Acc: 0.930 | Test Acc: 0.855 | Test Loss: 0.338 | Test AUC 0.882 (3735.20 s) \n",
      "(*) Entering Epoch 108 (3742.529 s)\n",
      "Epoch 108, Step 190 | Training Acc: 0.957 | Test Acc: 0.896 | Test Loss: 0.323 | Test AUC 0.919 (3749.29 s) \n",
      "Epoch 108, Step 380 | Training Acc: 0.949 | Test Acc: 0.807 | Test Loss: 0.536 | Test AUC 0.857 (3756.02 s) \n",
      "Epoch 108, Step 570 | Training Acc: 0.943 | Test Acc: 0.811 | Test Loss: 0.461 | Test AUC 0.866 (3762.78 s) \n",
      "Epoch 108, Step 760 | Training Acc: 0.943 | Test Acc: 0.852 | Test Loss: 0.370 | Test AUC 0.892 (3769.56 s) \n",
      "(*) Entering Epoch 109 (3776.883 s)\n",
      "Epoch 109, Step 190 | Training Acc: 0.938 | Test Acc: 0.875 | Test Loss: 0.344 | Test AUC 0.898 (3783.62 s) \n",
      "Epoch 109, Step 380 | Training Acc: 0.930 | Test Acc: 0.885 | Test Loss: 0.307 | Test AUC 0.901 (3790.36 s) \n",
      "Epoch 109, Step 570 | Training Acc: 0.934 | Test Acc: 0.834 | Test Loss: 0.458 | Test AUC 0.866 (3797.10 s) \n",
      "Epoch 109, Step 760 | Training Acc: 0.938 | Test Acc: 0.844 | Test Loss: 0.428 | Test AUC 0.880 (3803.84 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch110.ckpt-110\n",
      "(*) Entering Epoch 110 (3812.146 s)\n",
      "Epoch 110, Step 190 | Training Acc: 0.945 | Test Acc: 0.863 | Test Loss: 0.412 | Test AUC 0.899 (3818.94 s) \n",
      "Epoch 110, Step 380 | Training Acc: 0.936 | Test Acc: 0.877 | Test Loss: 0.365 | Test AUC 0.900 (3825.71 s) \n",
      "Epoch 110, Step 570 | Training Acc: 0.934 | Test Acc: 0.887 | Test Loss: 0.296 | Test AUC 0.901 (3832.53 s) \n",
      "Epoch 110, Step 760 | Training Acc: 0.939 | Test Acc: 0.779 | Test Loss: 0.577 | Test AUC 0.842 (3839.29 s) \n",
      "(*) Entering Epoch 111 (3846.633 s)\n",
      "Epoch 111, Step 190 | Training Acc: 0.936 | Test Acc: 0.863 | Test Loss: 0.376 | Test AUC 0.893 (3853.39 s) \n",
      "Epoch 111, Step 380 | Training Acc: 0.945 | Test Acc: 0.863 | Test Loss: 0.370 | Test AUC 0.900 (3860.13 s) \n",
      "Epoch 111, Step 570 | Training Acc: 0.924 | Test Acc: 0.859 | Test Loss: 0.347 | Test AUC 0.883 (3866.87 s) \n",
      "Epoch 111, Step 760 | Training Acc: 0.936 | Test Acc: 0.873 | Test Loss: 0.320 | Test AUC 0.894 (3873.62 s) \n",
      "(*) Entering Epoch 112 (3880.952 s)\n",
      "Epoch 112, Step 190 | Training Acc: 0.930 | Test Acc: 0.852 | Test Loss: 0.417 | Test AUC 0.886 (3887.68 s) \n",
      "Epoch 112, Step 380 | Training Acc: 0.928 | Test Acc: 0.867 | Test Loss: 0.361 | Test AUC 0.895 (3894.44 s) \n",
      "Epoch 112, Step 570 | Training Acc: 0.941 | Test Acc: 0.869 | Test Loss: 0.378 | Test AUC 0.900 (3901.17 s) \n",
      "Epoch 112, Step 760 | Training Acc: 0.949 | Test Acc: 0.828 | Test Loss: 0.525 | Test AUC 0.874 (3907.93 s) \n",
      "(*) Entering Epoch 113 (3915.256 s)\n",
      "Epoch 113, Step 190 | Training Acc: 0.939 | Test Acc: 0.859 | Test Loss: 0.394 | Test AUC 0.891 (3922.01 s) \n",
      "Epoch 113, Step 380 | Training Acc: 0.941 | Test Acc: 0.900 | Test Loss: 0.296 | Test AUC 0.914 (3928.78 s) \n",
      "Epoch 113, Step 570 | Training Acc: 0.949 | Test Acc: 0.859 | Test Loss: 0.369 | Test AUC 0.896 (3935.50 s) \n",
      "Epoch 113, Step 760 | Training Acc: 0.898 | Test Acc: 0.883 | Test Loss: 0.357 | Test AUC 0.880 (3942.26 s) \n",
      "(*) Entering Epoch 114 (3949.557 s)\n",
      "Epoch 114, Step 190 | Training Acc: 0.928 | Test Acc: 0.885 | Test Loss: 0.326 | Test AUC 0.899 (3956.31 s) \n",
      "Epoch 114, Step 380 | Training Acc: 0.945 | Test Acc: 0.873 | Test Loss: 0.352 | Test AUC 0.906 (3963.04 s) \n",
      "Epoch 114, Step 570 | Training Acc: 0.947 | Test Acc: 0.873 | Test Loss: 0.369 | Test AUC 0.904 (3969.83 s) \n",
      "Epoch 114, Step 760 | Training Acc: 0.902 | Test Acc: 0.867 | Test Loss: 0.375 | Test AUC 0.879 (3976.59 s) \n",
      "(*) Entering Epoch 115 (3983.957 s)\n",
      "Epoch 115, Step 190 | Training Acc: 0.953 | Test Acc: 0.824 | Test Loss: 0.466 | Test AUC 0.865 (3990.72 s) \n",
      "Epoch 115, Step 380 | Training Acc: 0.955 | Test Acc: 0.834 | Test Loss: 0.499 | Test AUC 0.880 (3997.48 s) \n",
      "Epoch 115, Step 570 | Training Acc: 0.938 | Test Acc: 0.871 | Test Loss: 0.339 | Test AUC 0.898 (4004.23 s) \n",
      "Epoch 115, Step 760 | Training Acc: 0.934 | Test Acc: 0.879 | Test Loss: 0.327 | Test AUC 0.902 (4010.97 s) \n",
      "(*) Entering Epoch 116 (4018.312 s)\n",
      "Epoch 116, Step 190 | Training Acc: 0.922 | Test Acc: 0.898 | Test Loss: 0.306 | Test AUC 0.903 (4025.08 s) \n",
      "Epoch 116, Step 380 | Training Acc: 0.953 | Test Acc: 0.861 | Test Loss: 0.401 | Test AUC 0.903 (4031.84 s) \n",
      "Epoch 116, Step 570 | Training Acc: 0.945 | Test Acc: 0.891 | Test Loss: 0.309 | Test AUC 0.911 (4038.60 s) \n",
      "Epoch 116, Step 760 | Training Acc: 0.928 | Test Acc: 0.863 | Test Loss: 0.454 | Test AUC 0.889 (4045.34 s) \n",
      "(*) Entering Epoch 117 (4052.681 s)\n",
      "Epoch 117, Step 190 | Training Acc: 0.939 | Test Acc: 0.852 | Test Loss: 0.446 | Test AUC 0.888 (4059.43 s) \n",
      "Epoch 117, Step 380 | Training Acc: 0.938 | Test Acc: 0.895 | Test Loss: 0.321 | Test AUC 0.914 (4066.24 s) \n",
      "Epoch 117, Step 570 | Training Acc: 0.936 | Test Acc: 0.873 | Test Loss: 0.326 | Test AUC 0.900 (4073.03 s) \n",
      "Epoch 117, Step 760 | Training Acc: 0.953 | Test Acc: 0.877 | Test Loss: 0.314 | Test AUC 0.907 (4079.76 s) \n",
      "(*) Entering Epoch 118 (4087.113 s)\n",
      "Epoch 118, Step 190 | Training Acc: 0.926 | Test Acc: 0.867 | Test Loss: 0.389 | Test AUC 0.893 (4093.88 s) \n",
      "Epoch 118, Step 380 | Training Acc: 0.945 | Test Acc: 0.836 | Test Loss: 0.436 | Test AUC 0.873 (4100.62 s) \n",
      "Epoch 118, Step 570 | Training Acc: 0.936 | Test Acc: 0.869 | Test Loss: 0.350 | Test AUC 0.891 (4107.39 s) \n",
      "Epoch 118, Step 760 | Training Acc: 0.934 | Test Acc: 0.879 | Test Loss: 0.361 | Test AUC 0.901 (4114.16 s) \n",
      "(*) Entering Epoch 119 (4121.486 s)\n",
      "Epoch 119, Step 190 | Training Acc: 0.965 | Test Acc: 0.881 | Test Loss: 0.330 | Test AUC 0.914 (4128.21 s) \n",
      "Epoch 119, Step 380 | Training Acc: 0.943 | Test Acc: 0.873 | Test Loss: 0.384 | Test AUC 0.903 (4134.96 s) \n",
      "Epoch 119, Step 570 | Training Acc: 0.941 | Test Acc: 0.826 | Test Loss: 0.468 | Test AUC 0.864 (4141.73 s) \n",
      "Epoch 119, Step 760 | Training Acc: 0.943 | Test Acc: 0.887 | Test Loss: 0.320 | Test AUC 0.907 (4148.48 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch120.ckpt-120\n",
      "(*) Entering Epoch 120 (4156.803 s)\n",
      "Epoch 120, Step 190 | Training Acc: 0.967 | Test Acc: 0.875 | Test Loss: 0.342 | Test AUC 0.916 (4163.57 s) \n",
      "Epoch 120, Step 380 | Training Acc: 0.949 | Test Acc: 0.871 | Test Loss: 0.369 | Test AUC 0.902 (4170.33 s) \n",
      "Epoch 120, Step 570 | Training Acc: 0.930 | Test Acc: 0.883 | Test Loss: 0.305 | Test AUC 0.900 (4177.09 s) \n",
      "Epoch 120, Step 760 | Training Acc: 0.932 | Test Acc: 0.877 | Test Loss: 0.350 | Test AUC 0.898 (4183.87 s) \n",
      "(*) Entering Epoch 121 (4191.195 s)\n",
      "Epoch 121, Step 190 | Training Acc: 0.936 | Test Acc: 0.867 | Test Loss: 0.349 | Test AUC 0.890 (4197.94 s) \n",
      "Epoch 121, Step 380 | Training Acc: 0.926 | Test Acc: 0.873 | Test Loss: 0.317 | Test AUC 0.893 (4204.69 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121, Step 570 | Training Acc: 0.947 | Test Acc: 0.906 | Test Loss: 0.255 | Test AUC 0.920 (4211.45 s) \n",
      "Epoch 121, Step 760 | Training Acc: 0.930 | Test Acc: 0.883 | Test Loss: 0.319 | Test AUC 0.902 (4218.21 s) \n",
      "(*) Entering Epoch 122 (4225.564 s)\n",
      "Epoch 122, Step 190 | Training Acc: 0.938 | Test Acc: 0.855 | Test Loss: 0.477 | Test AUC 0.884 (4232.31 s) \n",
      "Epoch 122, Step 380 | Training Acc: 0.936 | Test Acc: 0.838 | Test Loss: 0.556 | Test AUC 0.876 (4239.10 s) \n",
      "Epoch 122, Step 570 | Training Acc: 0.930 | Test Acc: 0.887 | Test Loss: 0.329 | Test AUC 0.898 (4245.85 s) \n",
      "Epoch 122, Step 760 | Training Acc: 0.939 | Test Acc: 0.879 | Test Loss: 0.368 | Test AUC 0.902 (4252.59 s) \n",
      "(*) Entering Epoch 123 (4259.898 s)\n",
      "Epoch 123, Step 190 | Training Acc: 0.934 | Test Acc: 0.832 | Test Loss: 0.522 | Test AUC 0.873 (4266.65 s) \n",
      "Epoch 123, Step 380 | Training Acc: 0.951 | Test Acc: 0.875 | Test Loss: 0.344 | Test AUC 0.908 (4273.42 s) \n",
      "Epoch 123, Step 570 | Training Acc: 0.951 | Test Acc: 0.893 | Test Loss: 0.333 | Test AUC 0.915 (4280.16 s) \n",
      "Epoch 123, Step 760 | Training Acc: 0.934 | Test Acc: 0.904 | Test Loss: 0.304 | Test AUC 0.919 (4286.90 s) \n",
      "(*) Entering Epoch 124 (4294.219 s)\n",
      "Epoch 124, Step 190 | Training Acc: 0.926 | Test Acc: 0.824 | Test Loss: 0.486 | Test AUC 0.860 (4300.97 s) \n",
      "Epoch 124, Step 380 | Training Acc: 0.941 | Test Acc: 0.834 | Test Loss: 0.462 | Test AUC 0.872 (4307.71 s) \n",
      "Epoch 124, Step 570 | Training Acc: 0.906 | Test Acc: 0.828 | Test Loss: 0.432 | Test AUC 0.852 (4314.45 s) \n",
      "Epoch 124, Step 760 | Training Acc: 0.932 | Test Acc: 0.828 | Test Loss: 0.478 | Test AUC 0.866 (4321.20 s) \n",
      "(*) Entering Epoch 125 (4328.536 s)\n",
      "Epoch 125, Step 190 | Training Acc: 0.943 | Test Acc: 0.867 | Test Loss: 0.368 | Test AUC 0.897 (4335.32 s) \n",
      "Epoch 125, Step 380 | Training Acc: 0.949 | Test Acc: 0.855 | Test Loss: 0.367 | Test AUC 0.897 (4342.07 s) \n",
      "Epoch 125, Step 570 | Training Acc: 0.936 | Test Acc: 0.873 | Test Loss: 0.315 | Test AUC 0.893 (4348.85 s) \n",
      "Epoch 125, Step 760 | Training Acc: 0.928 | Test Acc: 0.812 | Test Loss: 0.514 | Test AUC 0.853 (4355.62 s) \n",
      "(*) Entering Epoch 126 (4362.957 s)\n",
      "Epoch 126, Step 190 | Training Acc: 0.932 | Test Acc: 0.807 | Test Loss: 0.592 | Test AUC 0.847 (4369.71 s) \n",
      "Epoch 126, Step 380 | Training Acc: 0.936 | Test Acc: 0.818 | Test Loss: 0.533 | Test AUC 0.860 (4376.46 s) \n",
      "Epoch 126, Step 570 | Training Acc: 0.928 | Test Acc: 0.846 | Test Loss: 0.463 | Test AUC 0.866 (4383.21 s) \n",
      "Epoch 126, Step 760 | Training Acc: 0.945 | Test Acc: 0.844 | Test Loss: 0.444 | Test AUC 0.883 (4389.97 s) \n",
      "(*) Entering Epoch 127 (4397.296 s)\n",
      "Epoch 127, Step 190 | Training Acc: 0.953 | Test Acc: 0.891 | Test Loss: 0.330 | Test AUC 0.916 (4404.03 s) \n",
      "Epoch 127, Step 380 | Training Acc: 0.934 | Test Acc: 0.879 | Test Loss: 0.300 | Test AUC 0.900 (4410.80 s) \n",
      "Epoch 127, Step 570 | Training Acc: 0.939 | Test Acc: 0.887 | Test Loss: 0.338 | Test AUC 0.907 (4417.59 s) \n",
      "Epoch 127, Step 760 | Training Acc: 0.943 | Test Acc: 0.875 | Test Loss: 0.346 | Test AUC 0.904 (4424.35 s) \n",
      "(*) Entering Epoch 128 (4431.666 s)\n",
      "Epoch 128, Step 190 | Training Acc: 0.939 | Test Acc: 0.881 | Test Loss: 0.383 | Test AUC 0.902 (4438.41 s) \n",
      "Epoch 128, Step 380 | Training Acc: 0.957 | Test Acc: 0.879 | Test Loss: 0.357 | Test AUC 0.915 (4445.17 s) \n",
      "Epoch 128, Step 570 | Training Acc: 0.951 | Test Acc: 0.889 | Test Loss: 0.358 | Test AUC 0.914 (4451.94 s) \n",
      "Epoch 128, Step 760 | Training Acc: 0.951 | Test Acc: 0.877 | Test Loss: 0.322 | Test AUC 0.906 (4458.73 s) \n",
      "(*) Entering Epoch 129 (4466.072 s)\n",
      "Epoch 129, Step 190 | Training Acc: 0.959 | Test Acc: 0.902 | Test Loss: 0.282 | Test AUC 0.927 (4472.83 s) \n",
      "Epoch 129, Step 380 | Training Acc: 0.928 | Test Acc: 0.850 | Test Loss: 0.350 | Test AUC 0.873 (4479.59 s) \n",
      "Epoch 129, Step 570 | Training Acc: 0.928 | Test Acc: 0.865 | Test Loss: 0.338 | Test AUC 0.890 (4486.33 s) \n",
      "Epoch 129, Step 760 | Training Acc: 0.930 | Test Acc: 0.832 | Test Loss: 0.457 | Test AUC 0.869 (4493.11 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch130.ckpt-130\n",
      "(*) Entering Epoch 130 (4501.454 s)\n",
      "Epoch 130, Step 190 | Training Acc: 0.941 | Test Acc: 0.832 | Test Loss: 0.545 | Test AUC 0.878 (4508.20 s) \n",
      "Epoch 130, Step 380 | Training Acc: 0.947 | Test Acc: 0.873 | Test Loss: 0.418 | Test AUC 0.903 (4514.96 s) \n",
      "Epoch 130, Step 570 | Training Acc: 0.963 | Test Acc: 0.811 | Test Loss: 0.532 | Test AUC 0.869 (4521.72 s) \n",
      "Epoch 130, Step 760 | Training Acc: 0.932 | Test Acc: 0.838 | Test Loss: 0.456 | Test AUC 0.876 (4528.47 s) \n",
      "(*) Entering Epoch 131 (4535.815 s)\n",
      "Epoch 131, Step 190 | Training Acc: 0.961 | Test Acc: 0.791 | Test Loss: 0.564 | Test AUC 0.863 (4542.57 s) \n",
      "Epoch 131, Step 380 | Training Acc: 0.938 | Test Acc: 0.838 | Test Loss: 0.448 | Test AUC 0.874 (4549.31 s) \n",
      "Epoch 131, Step 570 | Training Acc: 0.938 | Test Acc: 0.857 | Test Loss: 0.416 | Test AUC 0.888 (4556.07 s) \n",
      "Epoch 131, Step 760 | Training Acc: 0.941 | Test Acc: 0.887 | Test Loss: 0.325 | Test AUC 0.906 (4562.85 s) \n",
      "(*) Entering Epoch 132 (4570.193 s)\n",
      "Epoch 132, Step 190 | Training Acc: 0.930 | Test Acc: 0.852 | Test Loss: 0.418 | Test AUC 0.880 (4576.96 s) \n",
      "Epoch 132, Step 380 | Training Acc: 0.938 | Test Acc: 0.863 | Test Loss: 0.369 | Test AUC 0.896 (4583.71 s) \n",
      "Epoch 132, Step 570 | Training Acc: 0.936 | Test Acc: 0.867 | Test Loss: 0.388 | Test AUC 0.893 (4590.44 s) \n",
      "Epoch 132, Step 760 | Training Acc: 0.949 | Test Acc: 0.893 | Test Loss: 0.301 | Test AUC 0.917 (4597.23 s) \n",
      "(*) Entering Epoch 133 (4604.540 s)\n",
      "Epoch 133, Step 190 | Training Acc: 0.930 | Test Acc: 0.822 | Test Loss: 0.484 | Test AUC 0.854 (4611.31 s) \n",
      "Epoch 133, Step 380 | Training Acc: 0.930 | Test Acc: 0.811 | Test Loss: 0.583 | Test AUC 0.860 (4618.04 s) \n",
      "Epoch 133, Step 570 | Training Acc: 0.939 | Test Acc: 0.854 | Test Loss: 0.432 | Test AUC 0.886 (4625.02 s) \n",
      "Epoch 133, Step 760 | Training Acc: 0.949 | Test Acc: 0.869 | Test Loss: 0.365 | Test AUC 0.899 (4631.78 s) \n",
      "(*) Entering Epoch 134 (4639.118 s)\n",
      "Epoch 134, Step 190 | Training Acc: 0.938 | Test Acc: 0.842 | Test Loss: 0.424 | Test AUC 0.881 (4645.88 s) \n",
      "Epoch 134, Step 380 | Training Acc: 0.924 | Test Acc: 0.834 | Test Loss: 0.517 | Test AUC 0.865 (4652.63 s) \n",
      "Epoch 134, Step 570 | Training Acc: 0.945 | Test Acc: 0.863 | Test Loss: 0.338 | Test AUC 0.899 (4659.42 s) \n",
      "Epoch 134, Step 760 | Training Acc: 0.947 | Test Acc: 0.852 | Test Loss: 0.461 | Test AUC 0.887 (4666.17 s) \n",
      "(*) Entering Epoch 135 (4673.476 s)\n",
      "Epoch 135, Step 190 | Training Acc: 0.947 | Test Acc: 0.859 | Test Loss: 0.402 | Test AUC 0.892 (4680.23 s) \n",
      "Epoch 135, Step 380 | Training Acc: 0.939 | Test Acc: 0.883 | Test Loss: 0.338 | Test AUC 0.902 (4686.96 s) \n",
      "Epoch 135, Step 570 | Training Acc: 0.951 | Test Acc: 0.869 | Test Loss: 0.415 | Test AUC 0.907 (4693.71 s) \n",
      "Epoch 135, Step 760 | Training Acc: 0.934 | Test Acc: 0.867 | Test Loss: 0.386 | Test AUC 0.895 (4700.46 s) \n",
      "(*) Entering Epoch 136 (4707.767 s)\n",
      "Epoch 136, Step 190 | Training Acc: 0.955 | Test Acc: 0.877 | Test Loss: 0.340 | Test AUC 0.912 (4714.54 s) \n",
      "Epoch 136, Step 380 | Training Acc: 0.936 | Test Acc: 0.865 | Test Loss: 0.402 | Test AUC 0.891 (4721.27 s) \n",
      "Epoch 136, Step 570 | Training Acc: 0.939 | Test Acc: 0.896 | Test Loss: 0.387 | Test AUC 0.913 (4728.05 s) \n",
      "Epoch 136, Step 760 | Training Acc: 0.951 | Test Acc: 0.885 | Test Loss: 0.316 | Test AUC 0.916 (4734.79 s) \n",
      "(*) Entering Epoch 137 (4742.107 s)\n",
      "Epoch 137, Step 190 | Training Acc: 0.945 | Test Acc: 0.820 | Test Loss: 0.502 | Test AUC 0.870 (4748.86 s) \n",
      "Epoch 137, Step 380 | Training Acc: 0.947 | Test Acc: 0.859 | Test Loss: 0.424 | Test AUC 0.896 (4755.62 s) \n",
      "Epoch 137, Step 570 | Training Acc: 0.924 | Test Acc: 0.824 | Test Loss: 0.489 | Test AUC 0.861 (4762.37 s) \n",
      "Epoch 137, Step 760 | Training Acc: 0.955 | Test Acc: 0.836 | Test Loss: 0.431 | Test AUC 0.883 (4769.12 s) \n",
      "(*) Entering Epoch 138 (4776.431 s)\n",
      "Epoch 138, Step 190 | Training Acc: 0.936 | Test Acc: 0.854 | Test Loss: 0.392 | Test AUC 0.885 (4783.18 s) \n",
      "Epoch 138, Step 380 | Training Acc: 0.945 | Test Acc: 0.865 | Test Loss: 0.372 | Test AUC 0.899 (4789.92 s) \n",
      "Epoch 138, Step 570 | Training Acc: 0.936 | Test Acc: 0.838 | Test Loss: 0.433 | Test AUC 0.877 (4796.69 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 138, Step 760 | Training Acc: 0.930 | Test Acc: 0.846 | Test Loss: 0.431 | Test AUC 0.879 (4803.44 s) \n",
      "(*) Entering Epoch 139 (4810.729 s)\n",
      "Epoch 139, Step 190 | Training Acc: 0.928 | Test Acc: 0.854 | Test Loss: 0.423 | Test AUC 0.879 (4817.47 s) \n",
      "Epoch 139, Step 380 | Training Acc: 0.941 | Test Acc: 0.824 | Test Loss: 0.479 | Test AUC 0.873 (4824.23 s) \n",
      "Epoch 139, Step 570 | Training Acc: 0.934 | Test Acc: 0.840 | Test Loss: 0.483 | Test AUC 0.872 (4830.99 s) \n",
      "Epoch 139, Step 760 | Training Acc: 0.936 | Test Acc: 0.863 | Test Loss: 0.391 | Test AUC 0.888 (4837.74 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch140.ckpt-140\n",
      "(*) Entering Epoch 140 (4846.012 s)\n",
      "Epoch 140, Step 190 | Training Acc: 0.939 | Test Acc: 0.869 | Test Loss: 0.397 | Test AUC 0.900 (4852.77 s) \n",
      "Epoch 140, Step 380 | Training Acc: 0.934 | Test Acc: 0.896 | Test Loss: 0.289 | Test AUC 0.906 (4859.52 s) \n",
      "Epoch 140, Step 570 | Training Acc: 0.938 | Test Acc: 0.875 | Test Loss: 0.347 | Test AUC 0.903 (4866.28 s) \n",
      "Epoch 140, Step 760 | Training Acc: 0.943 | Test Acc: 0.877 | Test Loss: 0.370 | Test AUC 0.901 (4873.04 s) \n",
      "(*) Entering Epoch 141 (4880.366 s)\n",
      "Epoch 141, Step 190 | Training Acc: 0.936 | Test Acc: 0.885 | Test Loss: 0.340 | Test AUC 0.903 (4887.13 s) \n",
      "Epoch 141, Step 380 | Training Acc: 0.969 | Test Acc: 0.836 | Test Loss: 0.478 | Test AUC 0.889 (4893.90 s) \n",
      "Epoch 141, Step 570 | Training Acc: 0.953 | Test Acc: 0.877 | Test Loss: 0.384 | Test AUC 0.906 (4900.65 s) \n",
      "Epoch 141, Step 760 | Training Acc: 0.939 | Test Acc: 0.865 | Test Loss: 0.400 | Test AUC 0.888 (4907.41 s) \n",
      "(*) Entering Epoch 142 (4914.737 s)\n",
      "Epoch 142, Step 190 | Training Acc: 0.924 | Test Acc: 0.896 | Test Loss: 0.317 | Test AUC 0.904 (4921.47 s) \n",
      "Epoch 142, Step 380 | Training Acc: 0.951 | Test Acc: 0.883 | Test Loss: 0.296 | Test AUC 0.911 (4928.24 s) \n",
      "Epoch 142, Step 570 | Training Acc: 0.936 | Test Acc: 0.873 | Test Loss: 0.322 | Test AUC 0.899 (4934.97 s) \n",
      "Epoch 142, Step 760 | Training Acc: 0.938 | Test Acc: 0.865 | Test Loss: 0.424 | Test AUC 0.891 (4941.73 s) \n",
      "(*) Entering Epoch 143 (4949.062 s)\n",
      "Epoch 143, Step 190 | Training Acc: 0.918 | Test Acc: 0.877 | Test Loss: 0.373 | Test AUC 0.890 (4955.89 s) \n",
      "Epoch 143, Step 380 | Training Acc: 0.947 | Test Acc: 0.895 | Test Loss: 0.345 | Test AUC 0.916 (4962.65 s) \n",
      "Epoch 143, Step 570 | Training Acc: 0.945 | Test Acc: 0.850 | Test Loss: 0.403 | Test AUC 0.879 (4969.40 s) \n",
      "Epoch 143, Step 760 | Training Acc: 0.961 | Test Acc: 0.865 | Test Loss: 0.355 | Test AUC 0.903 (4976.15 s) \n",
      "(*) Entering Epoch 144 (4983.481 s)\n",
      "Epoch 144, Step 190 | Training Acc: 0.939 | Test Acc: 0.824 | Test Loss: 0.509 | Test AUC 0.865 (4990.22 s) \n",
      "Epoch 144, Step 380 | Training Acc: 0.949 | Test Acc: 0.824 | Test Loss: 0.452 | Test AUC 0.872 (4996.96 s) \n",
      "Epoch 144, Step 570 | Training Acc: 0.928 | Test Acc: 0.875 | Test Loss: 0.353 | Test AUC 0.890 (5003.73 s) \n",
      "Epoch 144, Step 760 | Training Acc: 0.953 | Test Acc: 0.859 | Test Loss: 0.380 | Test AUC 0.894 (5010.47 s) \n",
      "(*) Entering Epoch 145 (5017.821 s)\n",
      "Epoch 145, Step 190 | Training Acc: 0.945 | Test Acc: 0.875 | Test Loss: 0.374 | Test AUC 0.902 (5024.56 s) \n",
      "Epoch 145, Step 380 | Training Acc: 0.953 | Test Acc: 0.861 | Test Loss: 0.426 | Test AUC 0.900 (5031.32 s) \n",
      "Epoch 145, Step 570 | Training Acc: 0.943 | Test Acc: 0.855 | Test Loss: 0.443 | Test AUC 0.885 (5038.08 s) \n",
      "Epoch 145, Step 760 | Training Acc: 0.945 | Test Acc: 0.812 | Test Loss: 0.519 | Test AUC 0.865 (5044.87 s) \n",
      "(*) Entering Epoch 146 (5052.189 s)\n",
      "Epoch 146, Step 190 | Training Acc: 0.961 | Test Acc: 0.881 | Test Loss: 0.320 | Test AUC 0.915 (5058.99 s) \n",
      "Epoch 146, Step 380 | Training Acc: 0.967 | Test Acc: 0.861 | Test Loss: 0.414 | Test AUC 0.909 (5065.80 s) \n",
      "Epoch 146, Step 570 | Training Acc: 0.959 | Test Acc: 0.850 | Test Loss: 0.425 | Test AUC 0.896 (5072.61 s) \n",
      "Epoch 146, Step 760 | Training Acc: 0.922 | Test Acc: 0.906 | Test Loss: 0.281 | Test AUC 0.907 (5079.46 s) \n",
      "(*) Entering Epoch 147 (5086.805 s)\n",
      "Epoch 147, Step 190 | Training Acc: 0.961 | Test Acc: 0.844 | Test Loss: 0.548 | Test AUC 0.886 (5093.63 s) \n",
      "Epoch 147, Step 380 | Training Acc: 0.963 | Test Acc: 0.840 | Test Loss: 0.422 | Test AUC 0.889 (5100.46 s) \n",
      "Epoch 147, Step 570 | Training Acc: 0.943 | Test Acc: 0.859 | Test Loss: 0.426 | Test AUC 0.891 (5107.23 s) \n",
      "Epoch 147, Step 760 | Training Acc: 0.963 | Test Acc: 0.812 | Test Loss: 0.510 | Test AUC 0.875 (5114.03 s) \n",
      "(*) Entering Epoch 148 (5121.337 s)\n",
      "Epoch 148, Step 190 | Training Acc: 0.943 | Test Acc: 0.846 | Test Loss: 0.499 | Test AUC 0.881 (5128.16 s) \n",
      "Epoch 148, Step 380 | Training Acc: 0.934 | Test Acc: 0.863 | Test Loss: 0.416 | Test AUC 0.886 (5134.99 s) \n",
      "Epoch 148, Step 570 | Training Acc: 0.945 | Test Acc: 0.869 | Test Loss: 0.367 | Test AUC 0.905 (5141.88 s) \n",
      "Epoch 148, Step 760 | Training Acc: 0.943 | Test Acc: 0.869 | Test Loss: 0.397 | Test AUC 0.899 (5148.72 s) \n",
      "(*) Entering Epoch 149 (5156.063 s)\n",
      "Epoch 149, Step 190 | Training Acc: 0.961 | Test Acc: 0.873 | Test Loss: 0.427 | Test AUC 0.914 (5162.83 s) \n",
      "Epoch 149, Step 380 | Training Acc: 0.934 | Test Acc: 0.893 | Test Loss: 0.342 | Test AUC 0.908 (5169.60 s) \n",
      "Epoch 149, Step 570 | Training Acc: 0.943 | Test Acc: 0.857 | Test Loss: 0.373 | Test AUC 0.888 (5176.37 s) \n",
      "Epoch 149, Step 760 | Training Acc: 0.951 | Test Acc: 0.850 | Test Loss: 0.457 | Test AUC 0.895 (5183.18 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch150.ckpt-150\n",
      "(*) Entering Epoch 150 (5191.530 s)\n",
      "Epoch 150, Step 190 | Training Acc: 0.934 | Test Acc: 0.869 | Test Loss: 0.356 | Test AUC 0.896 (5198.27 s) \n",
      "Epoch 150, Step 380 | Training Acc: 0.938 | Test Acc: 0.883 | Test Loss: 0.408 | Test AUC 0.903 (5205.03 s) \n",
      "Epoch 150, Step 570 | Training Acc: 0.936 | Test Acc: 0.885 | Test Loss: 0.394 | Test AUC 0.907 (5211.77 s) \n",
      "Epoch 150, Step 760 | Training Acc: 0.945 | Test Acc: 0.887 | Test Loss: 0.309 | Test AUC 0.909 (5218.51 s) \n",
      "(*) Entering Epoch 151 (5225.826 s)\n",
      "Epoch 151, Step 190 | Training Acc: 0.951 | Test Acc: 0.893 | Test Loss: 0.331 | Test AUC 0.920 (5232.56 s) \n",
      "Epoch 151, Step 380 | Training Acc: 0.955 | Test Acc: 0.881 | Test Loss: 0.394 | Test AUC 0.912 (5239.32 s) \n",
      "Epoch 151, Step 570 | Training Acc: 0.957 | Test Acc: 0.857 | Test Loss: 0.397 | Test AUC 0.905 (5246.05 s) \n",
      "Epoch 151, Step 760 | Training Acc: 0.947 | Test Acc: 0.830 | Test Loss: 0.441 | Test AUC 0.871 (5252.79 s) \n",
      "(*) Entering Epoch 152 (5260.122 s)\n",
      "Epoch 152, Step 190 | Training Acc: 0.939 | Test Acc: 0.822 | Test Loss: 0.483 | Test AUC 0.871 (5266.87 s) \n",
      "Epoch 152, Step 380 | Training Acc: 0.936 | Test Acc: 0.844 | Test Loss: 0.448 | Test AUC 0.880 (5273.63 s) \n",
      "Epoch 152, Step 570 | Training Acc: 0.949 | Test Acc: 0.885 | Test Loss: 0.342 | Test AUC 0.909 (5280.40 s) \n",
      "Epoch 152, Step 760 | Training Acc: 0.951 | Test Acc: 0.873 | Test Loss: 0.352 | Test AUC 0.908 (5287.17 s) \n",
      "(*) Entering Epoch 153 (5294.495 s)\n",
      "Epoch 153, Step 190 | Training Acc: 0.953 | Test Acc: 0.895 | Test Loss: 0.305 | Test AUC 0.915 (5301.24 s) \n",
      "Epoch 153, Step 380 | Training Acc: 0.959 | Test Acc: 0.865 | Test Loss: 0.381 | Test AUC 0.904 (5307.99 s) \n",
      "Epoch 153, Step 570 | Training Acc: 0.951 | Test Acc: 0.883 | Test Loss: 0.370 | Test AUC 0.915 (5314.75 s) \n",
      "Epoch 153, Step 760 | Training Acc: 0.949 | Test Acc: 0.869 | Test Loss: 0.370 | Test AUC 0.902 (5321.52 s) \n",
      "(*) Entering Epoch 154 (5328.819 s)\n",
      "Epoch 154, Step 190 | Training Acc: 0.957 | Test Acc: 0.869 | Test Loss: 0.348 | Test AUC 0.905 (5335.58 s) \n",
      "Epoch 154, Step 380 | Training Acc: 0.953 | Test Acc: 0.857 | Test Loss: 0.413 | Test AUC 0.897 (5342.33 s) \n",
      "Epoch 154, Step 570 | Training Acc: 0.945 | Test Acc: 0.871 | Test Loss: 0.370 | Test AUC 0.905 (5349.07 s) \n",
      "Epoch 154, Step 760 | Training Acc: 0.934 | Test Acc: 0.814 | Test Loss: 0.509 | Test AUC 0.861 (5355.82 s) \n",
      "(*) Entering Epoch 155 (5363.127 s)\n",
      "Epoch 155, Step 190 | Training Acc: 0.941 | Test Acc: 0.875 | Test Loss: 0.400 | Test AUC 0.898 (5369.90 s) \n",
      "Epoch 155, Step 380 | Training Acc: 0.945 | Test Acc: 0.846 | Test Loss: 0.455 | Test AUC 0.888 (5376.64 s) \n",
      "Epoch 155, Step 570 | Training Acc: 0.939 | Test Acc: 0.863 | Test Loss: 0.404 | Test AUC 0.892 (5383.40 s) \n",
      "Epoch 155, Step 760 | Training Acc: 0.926 | Test Acc: 0.873 | Test Loss: 0.365 | Test AUC 0.890 (5390.13 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) Entering Epoch 156 (5397.472 s)\n",
      "Epoch 156, Step 190 | Training Acc: 0.959 | Test Acc: 0.861 | Test Loss: 0.435 | Test AUC 0.895 (5404.20 s) \n",
      "Epoch 156, Step 380 | Training Acc: 0.941 | Test Acc: 0.852 | Test Loss: 0.397 | Test AUC 0.889 (5410.95 s) \n",
      "Epoch 156, Step 570 | Training Acc: 0.953 | Test Acc: 0.854 | Test Loss: 0.431 | Test AUC 0.895 (5417.69 s) \n",
      "Epoch 156, Step 760 | Training Acc: 0.951 | Test Acc: 0.875 | Test Loss: 0.362 | Test AUC 0.908 (5424.42 s) \n",
      "(*) Entering Epoch 157 (5431.718 s)\n",
      "Epoch 157, Step 190 | Training Acc: 0.943 | Test Acc: 0.865 | Test Loss: 0.346 | Test AUC 0.898 (5438.47 s) \n",
      "Epoch 157, Step 380 | Training Acc: 0.943 | Test Acc: 0.877 | Test Loss: 0.366 | Test AUC 0.901 (5445.20 s) \n",
      "Epoch 157, Step 570 | Training Acc: 0.953 | Test Acc: 0.848 | Test Loss: 0.463 | Test AUC 0.887 (5451.95 s) \n",
      "Epoch 157, Step 760 | Training Acc: 0.924 | Test Acc: 0.865 | Test Loss: 0.415 | Test AUC 0.886 (5458.72 s) \n",
      "(*) Entering Epoch 158 (5466.063 s)\n",
      "Epoch 158, Step 190 | Training Acc: 0.963 | Test Acc: 0.848 | Test Loss: 0.408 | Test AUC 0.899 (5472.89 s) \n",
      "Epoch 158, Step 380 | Training Acc: 0.934 | Test Acc: 0.881 | Test Loss: 0.392 | Test AUC 0.900 (5479.67 s) \n",
      "Epoch 158, Step 570 | Training Acc: 0.934 | Test Acc: 0.877 | Test Loss: 0.373 | Test AUC 0.898 (5486.43 s) \n",
      "Epoch 158, Step 760 | Training Acc: 0.953 | Test Acc: 0.885 | Test Loss: 0.346 | Test AUC 0.910 (5493.20 s) \n",
      "(*) Entering Epoch 159 (5500.512 s)\n",
      "Epoch 159, Step 190 | Training Acc: 0.967 | Test Acc: 0.877 | Test Loss: 0.358 | Test AUC 0.915 (5507.30 s) \n",
      "Epoch 159, Step 380 | Training Acc: 0.939 | Test Acc: 0.873 | Test Loss: 0.322 | Test AUC 0.896 (5514.09 s) \n",
      "Epoch 159, Step 570 | Training Acc: 0.943 | Test Acc: 0.883 | Test Loss: 0.352 | Test AUC 0.913 (5520.86 s) \n",
      "Epoch 159, Step 760 | Training Acc: 0.955 | Test Acc: 0.896 | Test Loss: 0.350 | Test AUC 0.924 (5527.61 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch160.ckpt-160\n",
      "(*) Entering Epoch 160 (5535.889 s)\n",
      "Epoch 160, Step 190 | Training Acc: 0.955 | Test Acc: 0.848 | Test Loss: 0.395 | Test AUC 0.890 (5542.65 s) \n",
      "Epoch 160, Step 380 | Training Acc: 0.939 | Test Acc: 0.861 | Test Loss: 0.418 | Test AUC 0.892 (5549.43 s) \n",
      "Epoch 160, Step 570 | Training Acc: 0.939 | Test Acc: 0.863 | Test Loss: 0.372 | Test AUC 0.900 (5556.20 s) \n",
      "Epoch 160, Step 760 | Training Acc: 0.957 | Test Acc: 0.836 | Test Loss: 0.482 | Test AUC 0.878 (5562.98 s) \n",
      "(*) Entering Epoch 161 (5570.294 s)\n",
      "Epoch 161, Step 190 | Training Acc: 0.936 | Test Acc: 0.854 | Test Loss: 0.468 | Test AUC 0.880 (5577.04 s) \n",
      "Epoch 161, Step 380 | Training Acc: 0.945 | Test Acc: 0.840 | Test Loss: 0.507 | Test AUC 0.871 (5583.86 s) \n",
      "Epoch 161, Step 570 | Training Acc: 0.932 | Test Acc: 0.814 | Test Loss: 0.523 | Test AUC 0.857 (5590.62 s) \n",
      "Epoch 161, Step 760 | Training Acc: 0.953 | Test Acc: 0.865 | Test Loss: 0.380 | Test AUC 0.901 (5597.39 s) \n",
      "(*) Entering Epoch 162 (5604.716 s)\n",
      "Epoch 162, Step 190 | Training Acc: 0.953 | Test Acc: 0.891 | Test Loss: 0.322 | Test AUC 0.916 (5611.49 s) \n",
      "Epoch 162, Step 380 | Training Acc: 0.959 | Test Acc: 0.854 | Test Loss: 0.426 | Test AUC 0.897 (5618.23 s) \n",
      "Epoch 162, Step 570 | Training Acc: 0.961 | Test Acc: 0.867 | Test Loss: 0.388 | Test AUC 0.910 (5625.00 s) \n",
      "Epoch 162, Step 760 | Training Acc: 0.947 | Test Acc: 0.836 | Test Loss: 0.481 | Test AUC 0.878 (5631.75 s) \n",
      "(*) Entering Epoch 163 (5639.082 s)\n",
      "Epoch 163, Step 190 | Training Acc: 0.951 | Test Acc: 0.883 | Test Loss: 0.421 | Test AUC 0.909 (5645.87 s) \n",
      "Epoch 163, Step 380 | Training Acc: 0.957 | Test Acc: 0.873 | Test Loss: 0.376 | Test AUC 0.909 (5652.63 s) \n",
      "Epoch 163, Step 570 | Training Acc: 0.957 | Test Acc: 0.844 | Test Loss: 0.511 | Test AUC 0.888 (5659.42 s) \n",
      "Epoch 163, Step 760 | Training Acc: 0.959 | Test Acc: 0.834 | Test Loss: 0.472 | Test AUC 0.887 (5666.16 s) \n",
      "(*) Entering Epoch 164 (5673.491 s)\n",
      "Epoch 164, Step 190 | Training Acc: 0.951 | Test Acc: 0.871 | Test Loss: 0.429 | Test AUC 0.903 (5680.24 s) \n",
      "Epoch 164, Step 380 | Training Acc: 0.951 | Test Acc: 0.818 | Test Loss: 0.576 | Test AUC 0.867 (5687.04 s) \n",
      "Epoch 164, Step 570 | Training Acc: 0.949 | Test Acc: 0.811 | Test Loss: 0.501 | Test AUC 0.861 (5693.85 s) \n",
      "Epoch 164, Step 760 | Training Acc: 0.945 | Test Acc: 0.867 | Test Loss: 0.392 | Test AUC 0.887 (5700.61 s) \n",
      "(*) Entering Epoch 165 (5707.925 s)\n",
      "Epoch 165, Step 190 | Training Acc: 0.949 | Test Acc: 0.898 | Test Loss: 0.336 | Test AUC 0.919 (5714.67 s) \n",
      "Epoch 165, Step 380 | Training Acc: 0.951 | Test Acc: 0.861 | Test Loss: 0.400 | Test AUC 0.902 (5721.42 s) \n",
      "Epoch 165, Step 570 | Training Acc: 0.951 | Test Acc: 0.889 | Test Loss: 0.361 | Test AUC 0.914 (5728.16 s) \n",
      "Epoch 165, Step 760 | Training Acc: 0.938 | Test Acc: 0.879 | Test Loss: 0.322 | Test AUC 0.904 (5734.93 s) \n",
      "(*) Entering Epoch 166 (5742.284 s)\n",
      "Epoch 166, Step 190 | Training Acc: 0.916 | Test Acc: 0.865 | Test Loss: 0.385 | Test AUC 0.882 (5749.06 s) \n",
      "Epoch 166, Step 380 | Training Acc: 0.957 | Test Acc: 0.857 | Test Loss: 0.394 | Test AUC 0.902 (5755.83 s) \n",
      "Epoch 166, Step 570 | Training Acc: 0.949 | Test Acc: 0.811 | Test Loss: 0.502 | Test AUC 0.867 (5762.59 s) \n",
      "Epoch 166, Step 760 | Training Acc: 0.955 | Test Acc: 0.826 | Test Loss: 0.568 | Test AUC 0.875 (5769.36 s) \n",
      "(*) Entering Epoch 167 (5776.679 s)\n",
      "Epoch 167, Step 190 | Training Acc: 0.963 | Test Acc: 0.857 | Test Loss: 0.371 | Test AUC 0.901 (5783.43 s) \n",
      "Epoch 167, Step 380 | Training Acc: 0.947 | Test Acc: 0.850 | Test Loss: 0.438 | Test AUC 0.894 (5790.20 s) \n",
      "Epoch 167, Step 570 | Training Acc: 0.951 | Test Acc: 0.916 | Test Loss: 0.273 | Test AUC 0.928 (5796.97 s) \n",
      "Epoch 167, Step 760 | Training Acc: 0.947 | Test Acc: 0.900 | Test Loss: 0.352 | Test AUC 0.915 (5803.72 s) \n",
      "(*) Entering Epoch 168 (5811.033 s)\n",
      "Epoch 168, Step 190 | Training Acc: 0.951 | Test Acc: 0.887 | Test Loss: 0.382 | Test AUC 0.913 (5817.79 s) \n",
      "Epoch 168, Step 380 | Training Acc: 0.955 | Test Acc: 0.869 | Test Loss: 0.364 | Test AUC 0.910 (5824.61 s) \n",
      "Epoch 168, Step 570 | Training Acc: 0.936 | Test Acc: 0.867 | Test Loss: 0.412 | Test AUC 0.893 (5831.38 s) \n",
      "Epoch 168, Step 760 | Training Acc: 0.955 | Test Acc: 0.885 | Test Loss: 0.322 | Test AUC 0.918 (5838.15 s) \n",
      "(*) Entering Epoch 169 (5845.492 s)\n",
      "Epoch 169, Step 190 | Training Acc: 0.951 | Test Acc: 0.830 | Test Loss: 0.482 | Test AUC 0.880 (5852.28 s) \n",
      "Epoch 169, Step 380 | Training Acc: 0.953 | Test Acc: 0.867 | Test Loss: 0.398 | Test AUC 0.902 (5859.06 s) \n",
      "Epoch 169, Step 570 | Training Acc: 0.955 | Test Acc: 0.812 | Test Loss: 0.618 | Test AUC 0.864 (5865.83 s) \n",
      "Epoch 169, Step 760 | Training Acc: 0.957 | Test Acc: 0.826 | Test Loss: 0.491 | Test AUC 0.872 (5872.58 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch170.ckpt-170\n",
      "(*) Entering Epoch 170 (5880.872 s)\n",
      "Epoch 170, Step 190 | Training Acc: 0.949 | Test Acc: 0.811 | Test Loss: 0.605 | Test AUC 0.865 (5887.64 s) \n",
      "Epoch 170, Step 380 | Training Acc: 0.955 | Test Acc: 0.844 | Test Loss: 0.493 | Test AUC 0.889 (5894.40 s) \n",
      "Epoch 170, Step 570 | Training Acc: 0.936 | Test Acc: 0.854 | Test Loss: 0.428 | Test AUC 0.888 (5901.18 s) \n",
      "Epoch 170, Step 760 | Training Acc: 0.947 | Test Acc: 0.875 | Test Loss: 0.359 | Test AUC 0.901 (5907.93 s) \n",
      "(*) Entering Epoch 171 (5915.243 s)\n",
      "Epoch 171, Step 190 | Training Acc: 0.943 | Test Acc: 0.855 | Test Loss: 0.400 | Test AUC 0.893 (5922.02 s) \n",
      "Epoch 171, Step 380 | Training Acc: 0.955 | Test Acc: 0.875 | Test Loss: 0.353 | Test AUC 0.912 (5928.87 s) \n",
      "Epoch 171, Step 570 | Training Acc: 0.949 | Test Acc: 0.848 | Test Loss: 0.443 | Test AUC 0.895 (5935.62 s) \n",
      "Epoch 171, Step 760 | Training Acc: 0.953 | Test Acc: 0.893 | Test Loss: 0.333 | Test AUC 0.921 (5942.40 s) \n",
      "(*) Entering Epoch 172 (5949.712 s)\n",
      "Epoch 172, Step 190 | Training Acc: 0.965 | Test Acc: 0.869 | Test Loss: 0.422 | Test AUC 0.915 (5956.48 s) \n",
      "Epoch 172, Step 380 | Training Acc: 0.934 | Test Acc: 0.859 | Test Loss: 0.359 | Test AUC 0.891 (5963.24 s) \n",
      "Epoch 172, Step 570 | Training Acc: 0.934 | Test Acc: 0.852 | Test Loss: 0.414 | Test AUC 0.887 (5970.01 s) \n",
      "Epoch 172, Step 760 | Training Acc: 0.959 | Test Acc: 0.840 | Test Loss: 0.493 | Test AUC 0.887 (5976.77 s) \n",
      "(*) Entering Epoch 173 (5984.087 s)\n",
      "Epoch 173, Step 190 | Training Acc: 0.961 | Test Acc: 0.842 | Test Loss: 0.486 | Test AUC 0.884 (5990.85 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173, Step 380 | Training Acc: 0.953 | Test Acc: 0.830 | Test Loss: 0.495 | Test AUC 0.880 (5997.62 s) \n",
      "Epoch 173, Step 570 | Training Acc: 0.953 | Test Acc: 0.885 | Test Loss: 0.348 | Test AUC 0.907 (6004.40 s) \n",
      "Epoch 173, Step 760 | Training Acc: 0.922 | Test Acc: 0.834 | Test Loss: 0.493 | Test AUC 0.863 (6011.13 s) \n",
      "(*) Entering Epoch 174 (6018.467 s)\n",
      "Epoch 174, Step 190 | Training Acc: 0.941 | Test Acc: 0.898 | Test Loss: 0.339 | Test AUC 0.917 (6025.22 s) \n",
      "Epoch 174, Step 380 | Training Acc: 0.957 | Test Acc: 0.879 | Test Loss: 0.404 | Test AUC 0.915 (6032.13 s) \n",
      "Epoch 174, Step 570 | Training Acc: 0.961 | Test Acc: 0.857 | Test Loss: 0.452 | Test AUC 0.902 (6038.90 s) \n",
      "Epoch 174, Step 760 | Training Acc: 0.957 | Test Acc: 0.865 | Test Loss: 0.354 | Test AUC 0.906 (6045.67 s) \n",
      "(*) Entering Epoch 175 (6052.971 s)\n",
      "Epoch 175, Step 190 | Training Acc: 0.951 | Test Acc: 0.842 | Test Loss: 0.434 | Test AUC 0.882 (6059.74 s) \n",
      "Epoch 175, Step 380 | Training Acc: 0.955 | Test Acc: 0.859 | Test Loss: 0.429 | Test AUC 0.906 (6066.48 s) \n",
      "Epoch 175, Step 570 | Training Acc: 0.934 | Test Acc: 0.838 | Test Loss: 0.489 | Test AUC 0.874 (6073.24 s) \n",
      "Epoch 175, Step 760 | Training Acc: 0.953 | Test Acc: 0.920 | Test Loss: 0.291 | Test AUC 0.932 (6081.02 s) [*]\n",
      "(*) Entering Epoch 176 (6088.348 s)\n",
      "Epoch 176, Step 190 | Training Acc: 0.945 | Test Acc: 0.871 | Test Loss: 0.368 | Test AUC 0.905 (6095.12 s) \n",
      "Epoch 176, Step 380 | Training Acc: 0.959 | Test Acc: 0.865 | Test Loss: 0.401 | Test AUC 0.906 (6101.88 s) \n",
      "Epoch 176, Step 570 | Training Acc: 0.955 | Test Acc: 0.844 | Test Loss: 0.460 | Test AUC 0.889 (6108.72 s) \n",
      "Epoch 176, Step 760 | Training Acc: 0.945 | Test Acc: 0.836 | Test Loss: 0.491 | Test AUC 0.881 (6115.51 s) \n",
      "(*) Entering Epoch 177 (6122.838 s)\n",
      "Epoch 177, Step 190 | Training Acc: 0.953 | Test Acc: 0.875 | Test Loss: 0.438 | Test AUC 0.910 (6129.61 s) \n",
      "Epoch 177, Step 380 | Training Acc: 0.934 | Test Acc: 0.893 | Test Loss: 0.298 | Test AUC 0.911 (6136.40 s) \n",
      "Epoch 177, Step 570 | Training Acc: 0.959 | Test Acc: 0.865 | Test Loss: 0.394 | Test AUC 0.907 (6143.18 s) \n",
      "Epoch 177, Step 760 | Training Acc: 0.953 | Test Acc: 0.852 | Test Loss: 0.389 | Test AUC 0.892 (6149.93 s) \n",
      "(*) Entering Epoch 178 (6157.240 s)\n",
      "Epoch 178, Step 190 | Training Acc: 0.951 | Test Acc: 0.902 | Test Loss: 0.301 | Test AUC 0.922 (6163.99 s) \n",
      "Epoch 178, Step 380 | Training Acc: 0.912 | Test Acc: 0.889 | Test Loss: 0.317 | Test AUC 0.894 (6170.76 s) \n",
      "Epoch 178, Step 570 | Training Acc: 0.967 | Test Acc: 0.809 | Test Loss: 0.442 | Test AUC 0.867 (6177.53 s) \n",
      "Epoch 178, Step 760 | Training Acc: 0.949 | Test Acc: 0.832 | Test Loss: 0.506 | Test AUC 0.874 (6184.28 s) \n",
      "(*) Entering Epoch 179 (6191.607 s)\n",
      "Epoch 179, Step 190 | Training Acc: 0.959 | Test Acc: 0.875 | Test Loss: 0.364 | Test AUC 0.912 (6198.37 s) \n",
      "Epoch 179, Step 380 | Training Acc: 0.961 | Test Acc: 0.867 | Test Loss: 0.414 | Test AUC 0.905 (6205.14 s) \n",
      "Epoch 179, Step 570 | Training Acc: 0.943 | Test Acc: 0.885 | Test Loss: 0.286 | Test AUC 0.907 (6211.90 s) \n",
      "Epoch 179, Step 760 | Training Acc: 0.932 | Test Acc: 0.877 | Test Loss: 0.392 | Test AUC 0.893 (6218.64 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch180.ckpt-180\n",
      "(*) Entering Epoch 180 (6226.943 s)\n",
      "Epoch 180, Step 190 | Training Acc: 0.961 | Test Acc: 0.885 | Test Loss: 0.367 | Test AUC 0.914 (6233.70 s) \n",
      "Epoch 180, Step 380 | Training Acc: 0.949 | Test Acc: 0.875 | Test Loss: 0.326 | Test AUC 0.904 (6240.44 s) \n",
      "Epoch 180, Step 570 | Training Acc: 0.947 | Test Acc: 0.834 | Test Loss: 0.468 | Test AUC 0.875 (6247.19 s) \n",
      "Epoch 180, Step 760 | Training Acc: 0.949 | Test Acc: 0.826 | Test Loss: 0.458 | Test AUC 0.875 (6253.95 s) \n",
      "(*) Entering Epoch 181 (6261.270 s)\n",
      "Epoch 181, Step 190 | Training Acc: 0.939 | Test Acc: 0.879 | Test Loss: 0.345 | Test AUC 0.902 (6268.01 s) \n",
      "Epoch 181, Step 380 | Training Acc: 0.949 | Test Acc: 0.867 | Test Loss: 0.428 | Test AUC 0.904 (6274.76 s) \n",
      "Epoch 181, Step 570 | Training Acc: 0.957 | Test Acc: 0.848 | Test Loss: 0.428 | Test AUC 0.894 (6281.51 s) \n",
      "Epoch 181, Step 760 | Training Acc: 0.957 | Test Acc: 0.828 | Test Loss: 0.579 | Test AUC 0.876 (6288.27 s) \n",
      "(*) Entering Epoch 182 (6295.605 s)\n",
      "Epoch 182, Step 190 | Training Acc: 0.959 | Test Acc: 0.873 | Test Loss: 0.385 | Test AUC 0.905 (6302.36 s) \n",
      "Epoch 182, Step 380 | Training Acc: 0.965 | Test Acc: 0.869 | Test Loss: 0.420 | Test AUC 0.912 (6309.13 s) \n",
      "Epoch 182, Step 570 | Training Acc: 0.936 | Test Acc: 0.883 | Test Loss: 0.345 | Test AUC 0.903 (6315.87 s) \n",
      "Epoch 182, Step 760 | Training Acc: 0.949 | Test Acc: 0.846 | Test Loss: 0.454 | Test AUC 0.885 (6322.61 s) \n",
      "(*) Entering Epoch 183 (6329.918 s)\n",
      "Epoch 183, Step 190 | Training Acc: 0.957 | Test Acc: 0.818 | Test Loss: 0.497 | Test AUC 0.875 (6336.67 s) \n",
      "Epoch 183, Step 380 | Training Acc: 0.945 | Test Acc: 0.846 | Test Loss: 0.482 | Test AUC 0.882 (6343.41 s) \n",
      "Epoch 183, Step 570 | Training Acc: 0.949 | Test Acc: 0.822 | Test Loss: 0.471 | Test AUC 0.875 (6350.20 s) \n",
      "Epoch 183, Step 760 | Training Acc: 0.945 | Test Acc: 0.826 | Test Loss: 0.522 | Test AUC 0.877 (6356.95 s) \n",
      "(*) Entering Epoch 184 (6364.273 s)\n",
      "Epoch 184, Step 190 | Training Acc: 0.961 | Test Acc: 0.881 | Test Loss: 0.407 | Test AUC 0.915 (6371.03 s) \n",
      "Epoch 184, Step 380 | Training Acc: 0.947 | Test Acc: 0.805 | Test Loss: 0.557 | Test AUC 0.863 (6377.79 s) \n",
      "Epoch 184, Step 570 | Training Acc: 0.947 | Test Acc: 0.828 | Test Loss: 0.441 | Test AUC 0.881 (6384.57 s) \n",
      "Epoch 184, Step 760 | Training Acc: 0.959 | Test Acc: 0.875 | Test Loss: 0.336 | Test AUC 0.909 (6391.32 s) \n",
      "(*) Entering Epoch 185 (6398.650 s)\n",
      "Epoch 185, Step 190 | Training Acc: 0.949 | Test Acc: 0.875 | Test Loss: 0.400 | Test AUC 0.905 (6405.40 s) \n",
      "Epoch 185, Step 380 | Training Acc: 0.939 | Test Acc: 0.812 | Test Loss: 0.578 | Test AUC 0.856 (6412.17 s) \n",
      "Epoch 185, Step 570 | Training Acc: 0.949 | Test Acc: 0.844 | Test Loss: 0.409 | Test AUC 0.886 (6418.94 s) \n",
      "Epoch 185, Step 760 | Training Acc: 0.945 | Test Acc: 0.809 | Test Loss: 0.542 | Test AUC 0.869 (6425.71 s) \n",
      "(*) Entering Epoch 186 (6433.060 s)\n",
      "Epoch 186, Step 190 | Training Acc: 0.965 | Test Acc: 0.855 | Test Loss: 0.414 | Test AUC 0.904 (6439.86 s) \n",
      "Epoch 186, Step 380 | Training Acc: 0.941 | Test Acc: 0.861 | Test Loss: 0.373 | Test AUC 0.889 (6446.63 s) \n",
      "Epoch 186, Step 570 | Training Acc: 0.953 | Test Acc: 0.871 | Test Loss: 0.409 | Test AUC 0.906 (6453.40 s) \n",
      "Epoch 186, Step 760 | Training Acc: 0.947 | Test Acc: 0.857 | Test Loss: 0.454 | Test AUC 0.893 (6460.15 s) \n",
      "(*) Entering Epoch 187 (6467.491 s)\n",
      "Epoch 187, Step 190 | Training Acc: 0.955 | Test Acc: 0.861 | Test Loss: 0.419 | Test AUC 0.899 (6474.26 s) \n",
      "Epoch 187, Step 380 | Training Acc: 0.941 | Test Acc: 0.832 | Test Loss: 0.516 | Test AUC 0.875 (6481.01 s) \n",
      "Epoch 187, Step 570 | Training Acc: 0.947 | Test Acc: 0.883 | Test Loss: 0.369 | Test AUC 0.906 (6487.77 s) \n",
      "Epoch 187, Step 760 | Training Acc: 0.961 | Test Acc: 0.881 | Test Loss: 0.383 | Test AUC 0.914 (6494.51 s) \n",
      "(*) Entering Epoch 188 (6501.854 s)\n",
      "Epoch 188, Step 190 | Training Acc: 0.963 | Test Acc: 0.879 | Test Loss: 0.403 | Test AUC 0.915 (6508.61 s) \n",
      "Epoch 188, Step 380 | Training Acc: 0.945 | Test Acc: 0.822 | Test Loss: 0.582 | Test AUC 0.872 (6515.40 s) \n",
      "Epoch 188, Step 570 | Training Acc: 0.934 | Test Acc: 0.816 | Test Loss: 0.543 | Test AUC 0.863 (6522.18 s) \n",
      "Epoch 188, Step 760 | Training Acc: 0.916 | Test Acc: 0.871 | Test Loss: 0.426 | Test AUC 0.888 (6528.93 s) \n",
      "(*) Entering Epoch 189 (6536.267 s)\n",
      "Epoch 189, Step 190 | Training Acc: 0.941 | Test Acc: 0.859 | Test Loss: 0.394 | Test AUC 0.888 (6543.02 s) \n",
      "Epoch 189, Step 380 | Training Acc: 0.945 | Test Acc: 0.826 | Test Loss: 0.548 | Test AUC 0.869 (6549.76 s) \n",
      "Epoch 189, Step 570 | Training Acc: 0.955 | Test Acc: 0.852 | Test Loss: 0.461 | Test AUC 0.893 (6556.52 s) \n",
      "Epoch 189, Step 760 | Training Acc: 0.965 | Test Acc: 0.859 | Test Loss: 0.403 | Test AUC 0.906 (6563.26 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch190.ckpt-190\n",
      "(*) Entering Epoch 190 (6571.559 s)\n",
      "Epoch 190, Step 190 | Training Acc: 0.965 | Test Acc: 0.838 | Test Loss: 0.526 | Test AUC 0.891 (6578.31 s) \n",
      "Epoch 190, Step 380 | Training Acc: 0.955 | Test Acc: 0.863 | Test Loss: 0.374 | Test AUC 0.900 (6585.06 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 190, Step 570 | Training Acc: 0.955 | Test Acc: 0.875 | Test Loss: 0.373 | Test AUC 0.908 (6591.80 s) \n",
      "Epoch 190, Step 760 | Training Acc: 0.953 | Test Acc: 0.895 | Test Loss: 0.329 | Test AUC 0.913 (6598.54 s) \n",
      "(*) Entering Epoch 191 (6605.891 s)\n",
      "Epoch 191, Step 190 | Training Acc: 0.965 | Test Acc: 0.893 | Test Loss: 0.350 | Test AUC 0.928 (6612.66 s) \n",
      "Epoch 191, Step 380 | Training Acc: 0.938 | Test Acc: 0.873 | Test Loss: 0.332 | Test AUC 0.901 (6619.42 s) \n",
      "Epoch 191, Step 570 | Training Acc: 0.959 | Test Acc: 0.904 | Test Loss: 0.286 | Test AUC 0.929 (6626.18 s) \n",
      "Epoch 191, Step 760 | Training Acc: 0.963 | Test Acc: 0.848 | Test Loss: 0.489 | Test AUC 0.898 (6632.92 s) \n",
      "(*) Entering Epoch 192 (6640.228 s)\n",
      "Epoch 192, Step 190 | Training Acc: 0.938 | Test Acc: 0.912 | Test Loss: 0.343 | Test AUC 0.919 (6646.99 s) \n",
      "Epoch 192, Step 380 | Training Acc: 0.951 | Test Acc: 0.877 | Test Loss: 0.416 | Test AUC 0.908 (6653.78 s) \n",
      "Epoch 192, Step 570 | Training Acc: 0.957 | Test Acc: 0.883 | Test Loss: 0.361 | Test AUC 0.915 (6660.55 s) \n",
      "Epoch 192, Step 760 | Training Acc: 0.926 | Test Acc: 0.891 | Test Loss: 0.360 | Test AUC 0.902 (6667.29 s) \n",
      "(*) Entering Epoch 193 (6674.612 s)\n",
      "Epoch 193, Step 190 | Training Acc: 0.961 | Test Acc: 0.859 | Test Loss: 0.468 | Test AUC 0.902 (6681.39 s) \n",
      "Epoch 193, Step 380 | Training Acc: 0.943 | Test Acc: 0.879 | Test Loss: 0.394 | Test AUC 0.901 (6688.17 s) \n",
      "Epoch 193, Step 570 | Training Acc: 0.953 | Test Acc: 0.861 | Test Loss: 0.368 | Test AUC 0.906 (6694.93 s) \n",
      "Epoch 193, Step 760 | Training Acc: 0.959 | Test Acc: 0.818 | Test Loss: 0.495 | Test AUC 0.870 (6701.68 s) \n",
      "(*) Entering Epoch 194 (6708.988 s)\n",
      "Epoch 194, Step 190 | Training Acc: 0.959 | Test Acc: 0.846 | Test Loss: 0.478 | Test AUC 0.895 (6715.75 s) \n",
      "Epoch 194, Step 380 | Training Acc: 0.959 | Test Acc: 0.865 | Test Loss: 0.383 | Test AUC 0.908 (6722.50 s) \n",
      "Epoch 194, Step 570 | Training Acc: 0.953 | Test Acc: 0.863 | Test Loss: 0.408 | Test AUC 0.902 (6729.26 s) \n",
      "Epoch 194, Step 760 | Training Acc: 0.947 | Test Acc: 0.885 | Test Loss: 0.330 | Test AUC 0.908 (6736.03 s) \n",
      "(*) Entering Epoch 195 (6743.369 s)\n",
      "Epoch 195, Step 190 | Training Acc: 0.939 | Test Acc: 0.863 | Test Loss: 0.374 | Test AUC 0.898 (6750.13 s) \n",
      "Epoch 195, Step 380 | Training Acc: 0.951 | Test Acc: 0.863 | Test Loss: 0.394 | Test AUC 0.899 (6756.91 s) \n",
      "Epoch 195, Step 570 | Training Acc: 0.953 | Test Acc: 0.863 | Test Loss: 0.396 | Test AUC 0.905 (6763.66 s) \n",
      "Epoch 195, Step 760 | Training Acc: 0.953 | Test Acc: 0.895 | Test Loss: 0.327 | Test AUC 0.917 (6770.46 s) \n",
      "(*) Entering Epoch 196 (6777.807 s)\n",
      "Epoch 196, Step 190 | Training Acc: 0.961 | Test Acc: 0.875 | Test Loss: 0.368 | Test AUC 0.913 (6784.56 s) \n",
      "Epoch 196, Step 380 | Training Acc: 0.951 | Test Acc: 0.926 | Test Loss: 0.202 | Test AUC 0.932 (6792.25 s) [*]\n",
      "Epoch 196, Step 570 | Training Acc: 0.943 | Test Acc: 0.861 | Test Loss: 0.411 | Test AUC 0.896 (6799.02 s) \n",
      "Epoch 196, Step 760 | Training Acc: 0.953 | Test Acc: 0.887 | Test Loss: 0.374 | Test AUC 0.915 (6805.78 s) \n",
      "(*) Entering Epoch 197 (6813.101 s)\n",
      "Epoch 197, Step 190 | Training Acc: 0.947 | Test Acc: 0.855 | Test Loss: 0.519 | Test AUC 0.885 (6819.87 s) \n",
      "Epoch 197, Step 380 | Training Acc: 0.949 | Test Acc: 0.840 | Test Loss: 0.526 | Test AUC 0.880 (6826.64 s) \n",
      "Epoch 197, Step 570 | Training Acc: 0.957 | Test Acc: 0.854 | Test Loss: 0.472 | Test AUC 0.898 (6833.40 s) \n",
      "Epoch 197, Step 760 | Training Acc: 0.939 | Test Acc: 0.877 | Test Loss: 0.345 | Test AUC 0.899 (6840.18 s) \n",
      "(*) Entering Epoch 198 (6847.544 s)\n",
      "Epoch 198, Step 190 | Training Acc: 0.951 | Test Acc: 0.859 | Test Loss: 0.380 | Test AUC 0.897 (6854.30 s) \n",
      "Epoch 198, Step 380 | Training Acc: 0.955 | Test Acc: 0.885 | Test Loss: 0.430 | Test AUC 0.914 (6861.05 s) \n",
      "Epoch 198, Step 570 | Training Acc: 0.943 | Test Acc: 0.867 | Test Loss: 0.396 | Test AUC 0.898 (6867.80 s) \n",
      "Epoch 198, Step 760 | Training Acc: 0.959 | Test Acc: 0.865 | Test Loss: 0.412 | Test AUC 0.907 (6874.57 s) \n",
      "(*) Entering Epoch 199 (6881.914 s)\n",
      "Epoch 199, Step 190 | Training Acc: 0.932 | Test Acc: 0.918 | Test Loss: 0.282 | Test AUC 0.920 (6888.67 s) \n",
      "Epoch 199, Step 380 | Training Acc: 0.949 | Test Acc: 0.906 | Test Loss: 0.327 | Test AUC 0.922 (6895.43 s) \n",
      "Epoch 199, Step 570 | Training Acc: 0.955 | Test Acc: 0.863 | Test Loss: 0.457 | Test AUC 0.899 (6902.19 s) \n",
      "Epoch 199, Step 760 | Training Acc: 0.945 | Test Acc: 0.877 | Test Loss: 0.388 | Test AUC 0.907 (6908.97 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch200.ckpt-200\n",
      "(*) Entering Epoch 200 (6917.454 s)\n",
      "Epoch 200, Step 190 | Training Acc: 0.949 | Test Acc: 0.852 | Test Loss: 0.504 | Test AUC 0.889 (6924.21 s) \n",
      "Epoch 200, Step 380 | Training Acc: 0.951 | Test Acc: 0.855 | Test Loss: 0.432 | Test AUC 0.900 (6930.97 s) \n",
      "Epoch 200, Step 570 | Training Acc: 0.947 | Test Acc: 0.859 | Test Loss: 0.426 | Test AUC 0.892 (6937.72 s) \n",
      "Epoch 200, Step 760 | Training Acc: 0.957 | Test Acc: 0.783 | Test Loss: 0.600 | Test AUC 0.858 (6944.46 s) \n",
      "(*) Entering Epoch 201 (6951.810 s)\n",
      "Epoch 201, Step 190 | Training Acc: 0.961 | Test Acc: 0.852 | Test Loss: 0.442 | Test AUC 0.899 (6958.57 s) \n",
      "Epoch 201, Step 380 | Training Acc: 0.955 | Test Acc: 0.879 | Test Loss: 0.344 | Test AUC 0.914 (6965.32 s) \n",
      "Epoch 201, Step 570 | Training Acc: 0.945 | Test Acc: 0.891 | Test Loss: 0.347 | Test AUC 0.910 (6972.06 s) \n",
      "Epoch 201, Step 760 | Training Acc: 0.947 | Test Acc: 0.863 | Test Loss: 0.510 | Test AUC 0.895 (6978.82 s) \n",
      "(*) Entering Epoch 202 (6986.136 s)\n",
      "Epoch 202, Step 190 | Training Acc: 0.957 | Test Acc: 0.893 | Test Loss: 0.301 | Test AUC 0.914 (6992.89 s) \n",
      "Epoch 202, Step 380 | Training Acc: 0.926 | Test Acc: 0.895 | Test Loss: 0.339 | Test AUC 0.906 (6999.63 s) \n",
      "Epoch 202, Step 570 | Training Acc: 0.951 | Test Acc: 0.861 | Test Loss: 0.433 | Test AUC 0.900 (7006.43 s) \n",
      "Epoch 202, Step 760 | Training Acc: 0.936 | Test Acc: 0.850 | Test Loss: 0.399 | Test AUC 0.882 (7013.18 s) \n",
      "(*) Entering Epoch 203 (7020.505 s)\n",
      "Epoch 203, Step 190 | Training Acc: 0.951 | Test Acc: 0.861 | Test Loss: 0.435 | Test AUC 0.899 (7027.26 s) \n",
      "Epoch 203, Step 380 | Training Acc: 0.934 | Test Acc: 0.893 | Test Loss: 0.332 | Test AUC 0.909 (7034.05 s) \n",
      "Epoch 203, Step 570 | Training Acc: 0.951 | Test Acc: 0.854 | Test Loss: 0.448 | Test AUC 0.898 (7040.80 s) \n",
      "Epoch 203, Step 760 | Training Acc: 0.957 | Test Acc: 0.854 | Test Loss: 0.471 | Test AUC 0.893 (7047.56 s) \n",
      "(*) Entering Epoch 204 (7054.901 s)\n",
      "Epoch 204, Step 190 | Training Acc: 0.959 | Test Acc: 0.863 | Test Loss: 0.415 | Test AUC 0.908 (7061.66 s) \n",
      "Epoch 204, Step 380 | Training Acc: 0.963 | Test Acc: 0.865 | Test Loss: 0.396 | Test AUC 0.911 (7068.43 s) \n",
      "Epoch 204, Step 570 | Training Acc: 0.963 | Test Acc: 0.844 | Test Loss: 0.522 | Test AUC 0.898 (7075.20 s) \n",
      "Epoch 204, Step 760 | Training Acc: 0.961 | Test Acc: 0.861 | Test Loss: 0.408 | Test AUC 0.909 (7081.99 s) \n",
      "(*) Entering Epoch 205 (7089.305 s)\n",
      "Epoch 205, Step 190 | Training Acc: 0.957 | Test Acc: 0.887 | Test Loss: 0.377 | Test AUC 0.916 (7096.08 s) \n",
      "Epoch 205, Step 380 | Training Acc: 0.975 | Test Acc: 0.889 | Test Loss: 0.305 | Test AUC 0.929 (7102.87 s) \n",
      "Epoch 205, Step 570 | Training Acc: 0.965 | Test Acc: 0.830 | Test Loss: 0.456 | Test AUC 0.886 (7109.65 s) \n",
      "Epoch 205, Step 760 | Training Acc: 0.939 | Test Acc: 0.838 | Test Loss: 0.493 | Test AUC 0.875 (7116.43 s) \n",
      "(*) Entering Epoch 206 (7123.745 s)\n",
      "Epoch 206, Step 190 | Training Acc: 0.936 | Test Acc: 0.877 | Test Loss: 0.372 | Test AUC 0.901 (7130.50 s) \n",
      "Epoch 206, Step 380 | Training Acc: 0.953 | Test Acc: 0.867 | Test Loss: 0.438 | Test AUC 0.903 (7137.30 s) \n",
      "Epoch 206, Step 570 | Training Acc: 0.961 | Test Acc: 0.877 | Test Loss: 0.400 | Test AUC 0.914 (7144.07 s) \n",
      "Epoch 206, Step 760 | Training Acc: 0.941 | Test Acc: 0.867 | Test Loss: 0.381 | Test AUC 0.896 (7150.84 s) \n",
      "(*) Entering Epoch 207 (7158.167 s)\n",
      "Epoch 207, Step 190 | Training Acc: 0.951 | Test Acc: 0.891 | Test Loss: 0.313 | Test AUC 0.914 (7164.94 s) \n",
      "Epoch 207, Step 380 | Training Acc: 0.963 | Test Acc: 0.877 | Test Loss: 0.402 | Test AUC 0.910 (7171.69 s) \n",
      "Epoch 207, Step 570 | Training Acc: 0.965 | Test Acc: 0.846 | Test Loss: 0.419 | Test AUC 0.898 (7178.46 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 207, Step 760 | Training Acc: 0.959 | Test Acc: 0.863 | Test Loss: 0.386 | Test AUC 0.902 (7185.21 s) \n",
      "(*) Entering Epoch 208 (7192.535 s)\n",
      "Epoch 208, Step 190 | Training Acc: 0.949 | Test Acc: 0.877 | Test Loss: 0.405 | Test AUC 0.901 (7199.33 s) \n",
      "Epoch 208, Step 380 | Training Acc: 0.971 | Test Acc: 0.869 | Test Loss: 0.404 | Test AUC 0.914 (7206.07 s) \n",
      "Epoch 208, Step 570 | Training Acc: 0.977 | Test Acc: 0.873 | Test Loss: 0.413 | Test AUC 0.918 (7212.82 s) \n",
      "Epoch 208, Step 760 | Training Acc: 0.943 | Test Acc: 0.781 | Test Loss: 0.660 | Test AUC 0.845 (7219.59 s) \n",
      "(*) Entering Epoch 209 (7226.930 s)\n",
      "Epoch 209, Step 190 | Training Acc: 0.959 | Test Acc: 0.877 | Test Loss: 0.331 | Test AUC 0.910 (7233.67 s) \n",
      "Epoch 209, Step 380 | Training Acc: 0.943 | Test Acc: 0.801 | Test Loss: 0.556 | Test AUC 0.858 (7240.44 s) \n",
      "Epoch 209, Step 570 | Training Acc: 0.955 | Test Acc: 0.852 | Test Loss: 0.442 | Test AUC 0.895 (7247.21 s) \n",
      "Epoch 209, Step 760 | Training Acc: 0.941 | Test Acc: 0.822 | Test Loss: 0.513 | Test AUC 0.866 (7253.95 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch210.ckpt-210\n",
      "(*) Entering Epoch 210 (7262.291 s)\n",
      "Epoch 210, Step 190 | Training Acc: 0.951 | Test Acc: 0.850 | Test Loss: 0.448 | Test AUC 0.888 (7269.07 s) \n",
      "Epoch 210, Step 380 | Training Acc: 0.971 | Test Acc: 0.875 | Test Loss: 0.361 | Test AUC 0.917 (7275.84 s) \n",
      "Epoch 210, Step 570 | Training Acc: 0.961 | Test Acc: 0.840 | Test Loss: 0.549 | Test AUC 0.882 (7282.59 s) \n",
      "Epoch 210, Step 760 | Training Acc: 0.961 | Test Acc: 0.852 | Test Loss: 0.479 | Test AUC 0.899 (7289.34 s) \n",
      "(*) Entering Epoch 211 (7296.703 s)\n",
      "Epoch 211, Step 190 | Training Acc: 0.975 | Test Acc: 0.895 | Test Loss: 0.304 | Test AUC 0.929 (7303.45 s) \n",
      "Epoch 211, Step 380 | Training Acc: 0.951 | Test Acc: 0.846 | Test Loss: 0.415 | Test AUC 0.893 (7310.20 s) \n",
      "Epoch 211, Step 570 | Training Acc: 0.943 | Test Acc: 0.885 | Test Loss: 0.298 | Test AUC 0.906 (7316.99 s) \n",
      "Epoch 211, Step 760 | Training Acc: 0.947 | Test Acc: 0.891 | Test Loss: 0.280 | Test AUC 0.911 (7323.72 s) \n",
      "(*) Entering Epoch 212 (7331.041 s)\n",
      "Epoch 212, Step 190 | Training Acc: 0.967 | Test Acc: 0.850 | Test Loss: 0.500 | Test AUC 0.895 (7337.78 s) \n",
      "Epoch 212, Step 380 | Training Acc: 0.959 | Test Acc: 0.818 | Test Loss: 0.516 | Test AUC 0.869 (7344.55 s) \n",
      "Epoch 212, Step 570 | Training Acc: 0.959 | Test Acc: 0.840 | Test Loss: 0.466 | Test AUC 0.884 (7351.30 s) \n",
      "Epoch 212, Step 760 | Training Acc: 0.961 | Test Acc: 0.832 | Test Loss: 0.512 | Test AUC 0.886 (7358.06 s) \n",
      "(*) Entering Epoch 213 (7365.373 s)\n",
      "Epoch 213, Step 190 | Training Acc: 0.945 | Test Acc: 0.891 | Test Loss: 0.373 | Test AUC 0.910 (7372.14 s) \n",
      "Epoch 213, Step 380 | Training Acc: 0.959 | Test Acc: 0.883 | Test Loss: 0.393 | Test AUC 0.918 (7378.92 s) \n",
      "Epoch 213, Step 570 | Training Acc: 0.963 | Test Acc: 0.859 | Test Loss: 0.397 | Test AUC 0.907 (7385.69 s) \n",
      "Epoch 213, Step 760 | Training Acc: 0.959 | Test Acc: 0.896 | Test Loss: 0.333 | Test AUC 0.924 (7392.44 s) \n",
      "(*) Entering Epoch 214 (7399.799 s)\n",
      "Epoch 214, Step 190 | Training Acc: 0.938 | Test Acc: 0.871 | Test Loss: 0.406 | Test AUC 0.891 (7406.57 s) \n",
      "Epoch 214, Step 380 | Training Acc: 0.945 | Test Acc: 0.865 | Test Loss: 0.371 | Test AUC 0.896 (7413.35 s) \n",
      "Epoch 214, Step 570 | Training Acc: 0.953 | Test Acc: 0.879 | Test Loss: 0.372 | Test AUC 0.910 (7420.13 s) \n",
      "Epoch 214, Step 760 | Training Acc: 0.957 | Test Acc: 0.855 | Test Loss: 0.409 | Test AUC 0.899 (7426.89 s) \n",
      "(*) Entering Epoch 215 (7434.232 s)\n",
      "Epoch 215, Step 190 | Training Acc: 0.959 | Test Acc: 0.875 | Test Loss: 0.410 | Test AUC 0.914 (7441.02 s) \n",
      "Epoch 215, Step 380 | Training Acc: 0.953 | Test Acc: 0.865 | Test Loss: 0.416 | Test AUC 0.900 (7447.77 s) \n",
      "Epoch 215, Step 570 | Training Acc: 0.971 | Test Acc: 0.881 | Test Loss: 0.417 | Test AUC 0.921 (7454.54 s) \n",
      "Epoch 215, Step 760 | Training Acc: 0.963 | Test Acc: 0.865 | Test Loss: 0.426 | Test AUC 0.910 (7461.30 s) \n",
      "(*) Entering Epoch 216 (7468.671 s)\n",
      "Epoch 216, Step 190 | Training Acc: 0.953 | Test Acc: 0.826 | Test Loss: 0.524 | Test AUC 0.877 (7475.44 s) \n",
      "Epoch 216, Step 380 | Training Acc: 0.967 | Test Acc: 0.854 | Test Loss: 0.461 | Test AUC 0.901 (7482.20 s) \n",
      "Epoch 216, Step 570 | Training Acc: 0.969 | Test Acc: 0.896 | Test Loss: 0.337 | Test AUC 0.929 (7488.94 s) \n",
      "Epoch 216, Step 760 | Training Acc: 0.949 | Test Acc: 0.861 | Test Loss: 0.354 | Test AUC 0.898 (7495.70 s) \n",
      "(*) Entering Epoch 217 (7503.030 s)\n",
      "Epoch 217, Step 190 | Training Acc: 0.963 | Test Acc: 0.877 | Test Loss: 0.332 | Test AUC 0.912 (7509.82 s) \n",
      "Epoch 217, Step 380 | Training Acc: 0.951 | Test Acc: 0.879 | Test Loss: 0.390 | Test AUC 0.912 (7516.58 s) \n",
      "Epoch 217, Step 570 | Training Acc: 0.963 | Test Acc: 0.811 | Test Loss: 0.582 | Test AUC 0.862 (7523.36 s) \n",
      "Epoch 217, Step 760 | Training Acc: 0.928 | Test Acc: 0.809 | Test Loss: 0.545 | Test AUC 0.842 (7530.14 s) \n",
      "(*) Entering Epoch 218 (7537.485 s)\n",
      "Epoch 218, Step 190 | Training Acc: 0.957 | Test Acc: 0.834 | Test Loss: 0.581 | Test AUC 0.880 (7544.24 s) \n",
      "Epoch 218, Step 380 | Training Acc: 0.953 | Test Acc: 0.848 | Test Loss: 0.484 | Test AUC 0.888 (7551.04 s) \n",
      "Epoch 218, Step 570 | Training Acc: 0.955 | Test Acc: 0.846 | Test Loss: 0.466 | Test AUC 0.890 (7557.84 s) \n",
      "Epoch 218, Step 760 | Training Acc: 0.973 | Test Acc: 0.873 | Test Loss: 0.360 | Test AUC 0.917 (7564.60 s) \n",
      "(*) Entering Epoch 219 (7571.929 s)\n",
      "Epoch 219, Step 190 | Training Acc: 0.939 | Test Acc: 0.852 | Test Loss: 0.462 | Test AUC 0.878 (7578.69 s) \n",
      "Epoch 219, Step 380 | Training Acc: 0.961 | Test Acc: 0.834 | Test Loss: 0.548 | Test AUC 0.886 (7585.46 s) \n",
      "Epoch 219, Step 570 | Training Acc: 0.949 | Test Acc: 0.803 | Test Loss: 0.620 | Test AUC 0.857 (7592.23 s) \n",
      "Epoch 219, Step 760 | Training Acc: 0.951 | Test Acc: 0.842 | Test Loss: 0.464 | Test AUC 0.882 (7599.00 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch220.ckpt-220\n",
      "(*) Entering Epoch 220 (7607.317 s)\n",
      "Epoch 220, Step 190 | Training Acc: 0.949 | Test Acc: 0.869 | Test Loss: 0.441 | Test AUC 0.903 (7614.09 s) \n",
      "Epoch 220, Step 380 | Training Acc: 0.947 | Test Acc: 0.873 | Test Loss: 0.368 | Test AUC 0.907 (7620.92 s) \n",
      "Epoch 220, Step 570 | Training Acc: 0.967 | Test Acc: 0.811 | Test Loss: 0.500 | Test AUC 0.876 (7627.67 s) \n",
      "Epoch 220, Step 760 | Training Acc: 0.949 | Test Acc: 0.850 | Test Loss: 0.415 | Test AUC 0.891 (7634.42 s) \n",
      "(*) Entering Epoch 221 (7641.760 s)\n",
      "Epoch 221, Step 190 | Training Acc: 0.957 | Test Acc: 0.875 | Test Loss: 0.411 | Test AUC 0.910 (7648.53 s) \n",
      "Epoch 221, Step 380 | Training Acc: 0.959 | Test Acc: 0.861 | Test Loss: 0.394 | Test AUC 0.903 (7655.29 s) \n",
      "Epoch 221, Step 570 | Training Acc: 0.955 | Test Acc: 0.859 | Test Loss: 0.395 | Test AUC 0.897 (7662.04 s) \n",
      "Epoch 221, Step 760 | Training Acc: 0.949 | Test Acc: 0.844 | Test Loss: 0.429 | Test AUC 0.891 (7668.80 s) \n",
      "(*) Entering Epoch 222 (7676.117 s)\n",
      "Epoch 222, Step 190 | Training Acc: 0.957 | Test Acc: 0.844 | Test Loss: 0.508 | Test AUC 0.889 (7682.88 s) \n",
      "Epoch 222, Step 380 | Training Acc: 0.967 | Test Acc: 0.855 | Test Loss: 0.424 | Test AUC 0.902 (7689.63 s) \n",
      "Epoch 222, Step 570 | Training Acc: 0.943 | Test Acc: 0.871 | Test Loss: 0.382 | Test AUC 0.901 (7696.39 s) \n",
      "Epoch 222, Step 760 | Training Acc: 0.957 | Test Acc: 0.869 | Test Loss: 0.399 | Test AUC 0.907 (7703.12 s) \n",
      "(*) Entering Epoch 223 (7710.449 s)\n",
      "Epoch 223, Step 190 | Training Acc: 0.969 | Test Acc: 0.812 | Test Loss: 0.536 | Test AUC 0.875 (7717.20 s) \n",
      "Epoch 223, Step 380 | Training Acc: 0.963 | Test Acc: 0.848 | Test Loss: 0.458 | Test AUC 0.888 (7723.96 s) \n",
      "Epoch 223, Step 570 | Training Acc: 0.951 | Test Acc: 0.838 | Test Loss: 0.522 | Test AUC 0.885 (7730.71 s) \n",
      "Epoch 223, Step 760 | Training Acc: 0.967 | Test Acc: 0.859 | Test Loss: 0.451 | Test AUC 0.903 (7737.49 s) \n",
      "(*) Entering Epoch 224 (7744.821 s)\n",
      "Epoch 224, Step 190 | Training Acc: 0.957 | Test Acc: 0.867 | Test Loss: 0.375 | Test AUC 0.908 (7751.57 s) \n",
      "Epoch 224, Step 380 | Training Acc: 0.949 | Test Acc: 0.875 | Test Loss: 0.366 | Test AUC 0.907 (7758.32 s) \n",
      "Epoch 224, Step 570 | Training Acc: 0.965 | Test Acc: 0.881 | Test Loss: 0.320 | Test AUC 0.918 (7765.06 s) \n",
      "Epoch 224, Step 760 | Training Acc: 0.961 | Test Acc: 0.816 | Test Loss: 0.555 | Test AUC 0.875 (7771.83 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(*) Entering Epoch 225 (7779.157 s)\n",
      "Epoch 225, Step 190 | Training Acc: 0.945 | Test Acc: 0.838 | Test Loss: 0.418 | Test AUC 0.878 (7785.94 s) \n",
      "Epoch 225, Step 380 | Training Acc: 0.965 | Test Acc: 0.865 | Test Loss: 0.411 | Test AUC 0.906 (7792.68 s) \n",
      "Epoch 225, Step 570 | Training Acc: 0.969 | Test Acc: 0.846 | Test Loss: 0.486 | Test AUC 0.899 (7799.44 s) \n",
      "Epoch 225, Step 760 | Training Acc: 0.941 | Test Acc: 0.834 | Test Loss: 0.625 | Test AUC 0.877 (7806.19 s) \n",
      "(*) Entering Epoch 226 (7813.496 s)\n",
      "Epoch 226, Step 190 | Training Acc: 0.969 | Test Acc: 0.848 | Test Loss: 0.467 | Test AUC 0.898 (7820.27 s) \n",
      "Epoch 226, Step 380 | Training Acc: 0.951 | Test Acc: 0.855 | Test Loss: 0.458 | Test AUC 0.889 (7827.03 s) \n",
      "Epoch 226, Step 570 | Training Acc: 0.953 | Test Acc: 0.885 | Test Loss: 0.365 | Test AUC 0.912 (7833.79 s) \n",
      "Epoch 226, Step 760 | Training Acc: 0.951 | Test Acc: 0.850 | Test Loss: 0.501 | Test AUC 0.896 (7840.58 s) \n",
      "(*) Entering Epoch 227 (7847.904 s)\n",
      "Epoch 227, Step 190 | Training Acc: 0.963 | Test Acc: 0.895 | Test Loss: 0.388 | Test AUC 0.922 (7854.66 s) \n",
      "Epoch 227, Step 380 | Training Acc: 0.979 | Test Acc: 0.861 | Test Loss: 0.449 | Test AUC 0.911 (7861.43 s) \n",
      "Epoch 227, Step 570 | Training Acc: 0.957 | Test Acc: 0.895 | Test Loss: 0.383 | Test AUC 0.919 (7868.20 s) \n",
      "Epoch 227, Step 760 | Training Acc: 0.951 | Test Acc: 0.893 | Test Loss: 0.359 | Test AUC 0.917 (7874.95 s) \n",
      "(*) Entering Epoch 228 (7882.273 s)\n",
      "Epoch 228, Step 190 | Training Acc: 0.957 | Test Acc: 0.881 | Test Loss: 0.466 | Test AUC 0.911 (7889.05 s) \n",
      "Epoch 228, Step 380 | Training Acc: 0.953 | Test Acc: 0.877 | Test Loss: 0.490 | Test AUC 0.909 (7895.81 s) \n",
      "Epoch 228, Step 570 | Training Acc: 0.936 | Test Acc: 0.857 | Test Loss: 0.409 | Test AUC 0.886 (7902.57 s) \n",
      "Epoch 228, Step 760 | Training Acc: 0.961 | Test Acc: 0.861 | Test Loss: 0.423 | Test AUC 0.897 (7909.32 s) \n",
      "(*) Entering Epoch 229 (7916.615 s)\n",
      "Epoch 229, Step 190 | Training Acc: 0.975 | Test Acc: 0.863 | Test Loss: 0.420 | Test AUC 0.910 (7923.36 s) \n",
      "Epoch 229, Step 380 | Training Acc: 0.951 | Test Acc: 0.871 | Test Loss: 0.396 | Test AUC 0.905 (7930.10 s) \n",
      "Epoch 229, Step 570 | Training Acc: 0.957 | Test Acc: 0.865 | Test Loss: 0.420 | Test AUC 0.905 (7936.87 s) \n",
      "Epoch 229, Step 760 | Training Acc: 0.957 | Test Acc: 0.893 | Test Loss: 0.342 | Test AUC 0.920 (7943.62 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch230.ckpt-230\n",
      "(*) Entering Epoch 230 (7951.933 s)\n",
      "Epoch 230, Step 190 | Training Acc: 0.947 | Test Acc: 0.898 | Test Loss: 0.280 | Test AUC 0.918 (7958.70 s) \n",
      "Epoch 230, Step 380 | Training Acc: 0.945 | Test Acc: 0.854 | Test Loss: 0.396 | Test AUC 0.890 (7965.46 s) \n",
      "Epoch 230, Step 570 | Training Acc: 0.945 | Test Acc: 0.863 | Test Loss: 0.386 | Test AUC 0.898 (7972.22 s) \n",
      "Epoch 230, Step 760 | Training Acc: 0.959 | Test Acc: 0.887 | Test Loss: 0.343 | Test AUC 0.919 (7979.01 s) \n",
      "(*) Entering Epoch 231 (7986.374 s)\n",
      "Epoch 231, Step 190 | Training Acc: 0.955 | Test Acc: 0.852 | Test Loss: 0.519 | Test AUC 0.897 (7993.13 s) \n",
      "Epoch 231, Step 380 | Training Acc: 0.961 | Test Acc: 0.828 | Test Loss: 0.464 | Test AUC 0.877 (7999.90 s) \n",
      "Epoch 231, Step 570 | Training Acc: 0.955 | Test Acc: 0.848 | Test Loss: 0.477 | Test AUC 0.890 (8006.66 s) \n",
      "Epoch 231, Step 760 | Training Acc: 0.963 | Test Acc: 0.816 | Test Loss: 0.576 | Test AUC 0.874 (8013.45 s) \n",
      "(*) Entering Epoch 232 (8020.809 s)\n",
      "Epoch 232, Step 190 | Training Acc: 0.957 | Test Acc: 0.844 | Test Loss: 0.453 | Test AUC 0.890 (8027.56 s) \n",
      "Epoch 232, Step 380 | Training Acc: 0.951 | Test Acc: 0.838 | Test Loss: 0.526 | Test AUC 0.882 (8034.32 s) \n",
      "Epoch 232, Step 570 | Training Acc: 0.951 | Test Acc: 0.832 | Test Loss: 0.544 | Test AUC 0.882 (8041.16 s) \n",
      "Epoch 232, Step 760 | Training Acc: 0.951 | Test Acc: 0.863 | Test Loss: 0.436 | Test AUC 0.897 (8047.93 s) \n",
      "(*) Entering Epoch 233 (8055.246 s)\n",
      "Epoch 233, Step 190 | Training Acc: 0.938 | Test Acc: 0.881 | Test Loss: 0.353 | Test AUC 0.906 (8062.00 s) \n",
      "Epoch 233, Step 380 | Training Acc: 0.947 | Test Acc: 0.871 | Test Loss: 0.444 | Test AUC 0.905 (8068.76 s) \n",
      "Epoch 233, Step 570 | Training Acc: 0.961 | Test Acc: 0.848 | Test Loss: 0.501 | Test AUC 0.899 (8075.52 s) \n",
      "Epoch 233, Step 760 | Training Acc: 0.961 | Test Acc: 0.848 | Test Loss: 0.505 | Test AUC 0.897 (8082.28 s) \n",
      "(*) Entering Epoch 234 (8089.648 s)\n",
      "Epoch 234, Step 190 | Training Acc: 0.965 | Test Acc: 0.854 | Test Loss: 0.439 | Test AUC 0.903 (8096.41 s) \n",
      "Epoch 234, Step 380 | Training Acc: 0.965 | Test Acc: 0.887 | Test Loss: 0.380 | Test AUC 0.922 (8103.17 s) \n",
      "Epoch 234, Step 570 | Training Acc: 0.961 | Test Acc: 0.879 | Test Loss: 0.429 | Test AUC 0.915 (8109.92 s) \n",
      "Epoch 234, Step 760 | Training Acc: 0.961 | Test Acc: 0.844 | Test Loss: 0.410 | Test AUC 0.887 (8116.67 s) \n",
      "(*) Entering Epoch 235 (8123.989 s)\n",
      "Epoch 235, Step 190 | Training Acc: 0.955 | Test Acc: 0.883 | Test Loss: 0.334 | Test AUC 0.913 (8130.77 s) \n",
      "Epoch 235, Step 380 | Training Acc: 0.961 | Test Acc: 0.898 | Test Loss: 0.339 | Test AUC 0.921 (8137.51 s) \n",
      "Epoch 235, Step 570 | Training Acc: 0.938 | Test Acc: 0.871 | Test Loss: 0.405 | Test AUC 0.895 (8144.26 s) \n",
      "Epoch 235, Step 760 | Training Acc: 0.941 | Test Acc: 0.883 | Test Loss: 0.436 | Test AUC 0.903 (8151.02 s) \n",
      "(*) Entering Epoch 236 (8158.361 s)\n",
      "Epoch 236, Step 190 | Training Acc: 0.953 | Test Acc: 0.885 | Test Loss: 0.370 | Test AUC 0.913 (8165.11 s) \n",
      "Epoch 236, Step 380 | Training Acc: 0.953 | Test Acc: 0.873 | Test Loss: 0.370 | Test AUC 0.910 (8171.89 s) \n",
      "Epoch 236, Step 570 | Training Acc: 0.961 | Test Acc: 0.863 | Test Loss: 0.379 | Test AUC 0.909 (8178.66 s) \n",
      "Epoch 236, Step 760 | Training Acc: 0.924 | Test Acc: 0.848 | Test Loss: 0.482 | Test AUC 0.870 (8185.44 s) \n",
      "(*) Entering Epoch 237 (8192.777 s)\n",
      "Epoch 237, Step 190 | Training Acc: 0.967 | Test Acc: 0.789 | Test Loss: 0.621 | Test AUC 0.858 (8199.55 s) \n",
      "Epoch 237, Step 380 | Training Acc: 0.959 | Test Acc: 0.846 | Test Loss: 0.465 | Test AUC 0.889 (8206.31 s) \n",
      "Epoch 237, Step 570 | Training Acc: 0.938 | Test Acc: 0.887 | Test Loss: 0.380 | Test AUC 0.900 (8213.06 s) \n",
      "Epoch 237, Step 760 | Training Acc: 0.957 | Test Acc: 0.859 | Test Loss: 0.431 | Test AUC 0.901 (8219.82 s) \n",
      "(*) Entering Epoch 238 (8227.182 s)\n",
      "Epoch 238, Step 190 | Training Acc: 0.955 | Test Acc: 0.871 | Test Loss: 0.358 | Test AUC 0.908 (8233.95 s) \n",
      "Epoch 238, Step 380 | Training Acc: 0.971 | Test Acc: 0.863 | Test Loss: 0.415 | Test AUC 0.913 (8240.72 s) \n",
      "Epoch 238, Step 570 | Training Acc: 0.947 | Test Acc: 0.893 | Test Loss: 0.264 | Test AUC 0.913 (8247.48 s) \n",
      "Epoch 238, Step 760 | Training Acc: 0.965 | Test Acc: 0.811 | Test Loss: 0.535 | Test AUC 0.870 (8254.23 s) \n",
      "(*) Entering Epoch 239 (8261.563 s)\n",
      "Epoch 239, Step 190 | Training Acc: 0.949 | Test Acc: 0.807 | Test Loss: 0.673 | Test AUC 0.859 (8268.33 s) \n",
      "Epoch 239, Step 380 | Training Acc: 0.939 | Test Acc: 0.852 | Test Loss: 0.403 | Test AUC 0.885 (8275.14 s) \n",
      "Epoch 239, Step 570 | Training Acc: 0.955 | Test Acc: 0.879 | Test Loss: 0.446 | Test AUC 0.910 (8281.90 s) \n",
      "Epoch 239, Step 760 | Training Acc: 0.959 | Test Acc: 0.871 | Test Loss: 0.382 | Test AUC 0.911 (8288.69 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch240.ckpt-240\n",
      "(*) Entering Epoch 240 (8297.068 s)\n",
      "Epoch 240, Step 190 | Training Acc: 0.959 | Test Acc: 0.871 | Test Loss: 0.407 | Test AUC 0.907 (8303.85 s) \n",
      "Epoch 240, Step 380 | Training Acc: 0.967 | Test Acc: 0.877 | Test Loss: 0.332 | Test AUC 0.918 (8310.63 s) \n",
      "Epoch 240, Step 570 | Training Acc: 0.934 | Test Acc: 0.869 | Test Loss: 0.414 | Test AUC 0.895 (8317.40 s) \n",
      "Epoch 240, Step 760 | Training Acc: 0.949 | Test Acc: 0.840 | Test Loss: 0.508 | Test AUC 0.878 (8324.15 s) \n",
      "(*) Entering Epoch 241 (8331.478 s)\n",
      "Epoch 241, Step 190 | Training Acc: 0.951 | Test Acc: 0.840 | Test Loss: 0.500 | Test AUC 0.881 (8338.25 s) \n",
      "Epoch 241, Step 380 | Training Acc: 0.959 | Test Acc: 0.783 | Test Loss: 0.724 | Test AUC 0.854 (8345.03 s) \n",
      "Epoch 241, Step 570 | Training Acc: 0.963 | Test Acc: 0.832 | Test Loss: 0.524 | Test AUC 0.887 (8351.80 s) \n",
      "Epoch 241, Step 760 | Training Acc: 0.943 | Test Acc: 0.830 | Test Loss: 0.455 | Test AUC 0.873 (8358.58 s) \n",
      "(*) Entering Epoch 242 (8365.904 s)\n",
      "Epoch 242, Step 190 | Training Acc: 0.959 | Test Acc: 0.838 | Test Loss: 0.484 | Test AUC 0.888 (8372.66 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 242, Step 380 | Training Acc: 0.961 | Test Acc: 0.832 | Test Loss: 0.508 | Test AUC 0.883 (8379.44 s) \n",
      "Epoch 242, Step 570 | Training Acc: 0.953 | Test Acc: 0.863 | Test Loss: 0.471 | Test AUC 0.899 (8386.20 s) \n",
      "Epoch 242, Step 760 | Training Acc: 0.965 | Test Acc: 0.881 | Test Loss: 0.347 | Test AUC 0.914 (8393.02 s) \n",
      "(*) Entering Epoch 243 (8400.371 s)\n",
      "Epoch 243, Step 190 | Training Acc: 0.967 | Test Acc: 0.895 | Test Loss: 0.355 | Test AUC 0.925 (8407.14 s) \n",
      "Epoch 243, Step 380 | Training Acc: 0.957 | Test Acc: 0.852 | Test Loss: 0.422 | Test AUC 0.887 (8413.89 s) \n",
      "Epoch 243, Step 570 | Training Acc: 0.953 | Test Acc: 0.830 | Test Loss: 0.485 | Test AUC 0.881 (8420.63 s) \n",
      "Epoch 243, Step 760 | Training Acc: 0.951 | Test Acc: 0.869 | Test Loss: 0.390 | Test AUC 0.903 (8427.39 s) \n",
      "(*) Entering Epoch 244 (8434.707 s)\n",
      "Epoch 244, Step 190 | Training Acc: 0.951 | Test Acc: 0.865 | Test Loss: 0.393 | Test AUC 0.901 (8441.45 s) \n",
      "Epoch 244, Step 380 | Training Acc: 0.930 | Test Acc: 0.871 | Test Loss: 0.368 | Test AUC 0.896 (8448.24 s) \n",
      "Epoch 244, Step 570 | Training Acc: 0.971 | Test Acc: 0.789 | Test Loss: 0.628 | Test AUC 0.864 (8455.00 s) \n",
      "Epoch 244, Step 760 | Training Acc: 0.953 | Test Acc: 0.848 | Test Loss: 0.382 | Test AUC 0.890 (8461.85 s) \n",
      "(*) Entering Epoch 245 (8469.170 s)\n",
      "Epoch 245, Step 190 | Training Acc: 0.947 | Test Acc: 0.900 | Test Loss: 0.314 | Test AUC 0.920 (8475.95 s) \n",
      "Epoch 245, Step 380 | Training Acc: 0.967 | Test Acc: 0.906 | Test Loss: 0.267 | Test AUC 0.930 (8482.75 s) \n",
      "Epoch 245, Step 570 | Training Acc: 0.957 | Test Acc: 0.902 | Test Loss: 0.268 | Test AUC 0.923 (8489.53 s) \n",
      "Epoch 245, Step 760 | Training Acc: 0.947 | Test Acc: 0.861 | Test Loss: 0.436 | Test AUC 0.899 (8496.34 s) \n",
      "(*) Entering Epoch 246 (8503.663 s)\n",
      "Epoch 246, Step 190 | Training Acc: 0.951 | Test Acc: 0.838 | Test Loss: 0.493 | Test AUC 0.883 (8510.44 s) \n",
      "Epoch 246, Step 380 | Training Acc: 0.973 | Test Acc: 0.857 | Test Loss: 0.470 | Test AUC 0.908 (8517.24 s) \n",
      "Epoch 246, Step 570 | Training Acc: 0.963 | Test Acc: 0.848 | Test Loss: 0.439 | Test AUC 0.901 (8523.98 s) \n",
      "Epoch 246, Step 760 | Training Acc: 0.955 | Test Acc: 0.787 | Test Loss: 0.601 | Test AUC 0.850 (8530.75 s) \n",
      "(*) Entering Epoch 247 (8538.061 s)\n",
      "Epoch 247, Step 190 | Training Acc: 0.955 | Test Acc: 0.873 | Test Loss: 0.405 | Test AUC 0.903 (8544.88 s) \n",
      "Epoch 247, Step 380 | Training Acc: 0.949 | Test Acc: 0.857 | Test Loss: 0.494 | Test AUC 0.892 (8551.70 s) \n",
      "Epoch 247, Step 570 | Training Acc: 0.953 | Test Acc: 0.871 | Test Loss: 0.365 | Test AUC 0.906 (8558.44 s) \n",
      "Epoch 247, Step 760 | Training Acc: 0.959 | Test Acc: 0.857 | Test Loss: 0.424 | Test AUC 0.899 (8565.22 s) \n",
      "(*) Entering Epoch 248 (8572.534 s)\n",
      "Epoch 248, Step 190 | Training Acc: 0.969 | Test Acc: 0.863 | Test Loss: 0.405 | Test AUC 0.906 (8579.31 s) \n",
      "Epoch 248, Step 380 | Training Acc: 0.941 | Test Acc: 0.869 | Test Loss: 0.394 | Test AUC 0.900 (8586.06 s) \n",
      "Epoch 248, Step 570 | Training Acc: 0.967 | Test Acc: 0.854 | Test Loss: 0.520 | Test AUC 0.905 (8592.83 s) \n",
      "Epoch 248, Step 760 | Training Acc: 0.965 | Test Acc: 0.812 | Test Loss: 0.648 | Test AUC 0.872 (8599.57 s) \n",
      "(*) Entering Epoch 249 (8606.908 s)\n",
      "Epoch 249, Step 190 | Training Acc: 0.969 | Test Acc: 0.854 | Test Loss: 0.512 | Test AUC 0.903 (8613.68 s) \n",
      "Epoch 249, Step 380 | Training Acc: 0.959 | Test Acc: 0.873 | Test Loss: 0.409 | Test AUC 0.910 (8620.44 s) \n",
      "Epoch 249, Step 570 | Training Acc: 0.959 | Test Acc: 0.844 | Test Loss: 0.468 | Test AUC 0.896 (8627.23 s) \n",
      "Epoch 249, Step 760 | Training Acc: 0.949 | Test Acc: 0.885 | Test Loss: 0.353 | Test AUC 0.912 (8634.04 s) \n",
      "Graph saved to file: checkpoints/vDNN_kst_rho0_epoch250.ckpt-250\n",
      "(*) Entering Epoch 250 (8642.400 s)\n",
      "Epoch 250, Step 190 | Training Acc: 0.955 | Test Acc: 0.898 | Test Loss: 0.319 | Test AUC 0.922 (8649.16 s) \n",
      "Epoch 250, Step 380 | Training Acc: 0.965 | Test Acc: 0.832 | Test Loss: 0.539 | Test AUC 0.883 (8655.94 s) \n",
      "Epoch 250, Step 570 | Training Acc: 0.969 | Test Acc: 0.826 | Test Loss: 0.556 | Test AUC 0.885 (8662.73 s) \n",
      "Epoch 250, Step 760 | Training Acc: 0.955 | Test Acc: 0.838 | Test Loss: 0.485 | Test AUC 0.888 (8669.75 s) \n",
      "(*) Entering Epoch 251 (8677.076 s)\n",
      "Epoch 251, Step 190 | Training Acc: 0.963 | Test Acc: 0.816 | Test Loss: 0.587 | Test AUC 0.868 (8683.80 s) \n",
      "Epoch 251, Step 380 | Training Acc: 0.967 | Test Acc: 0.850 | Test Loss: 0.365 | Test AUC 0.894 (8690.58 s) \n",
      "Epoch 251, Step 570 | Training Acc: 0.953 | Test Acc: 0.877 | Test Loss: 0.417 | Test AUC 0.904 (8697.36 s) \n",
      "Epoch 251, Step 760 | Training Acc: 0.973 | Test Acc: 0.889 | Test Loss: 0.406 | Test AUC 0.925 (8704.15 s) \n",
      "(*) Entering Epoch 252 (8711.508 s)\n",
      "Epoch 252, Step 190 | Training Acc: 0.949 | Test Acc: 0.852 | Test Loss: 0.516 | Test AUC 0.892 (8718.28 s) \n",
      "Epoch 252, Step 380 | Training Acc: 0.971 | Test Acc: 0.844 | Test Loss: 0.545 | Test AUC 0.900 (8725.09 s) \n",
      "Epoch 252, Step 570 | Training Acc: 0.957 | Test Acc: 0.826 | Test Loss: 0.539 | Test AUC 0.875 (8731.88 s) \n",
      "Epoch 252, Step 760 | Training Acc: 0.957 | Test Acc: 0.854 | Test Loss: 0.468 | Test AUC 0.899 (8738.65 s) \n",
      "(*) Entering Epoch 253 (8745.964 s)\n",
      "Epoch 253, Step 190 | Training Acc: 0.959 | Test Acc: 0.836 | Test Loss: 0.584 | Test AUC 0.886 (8752.73 s) \n",
      "Epoch 253, Step 380 | Training Acc: 0.963 | Test Acc: 0.867 | Test Loss: 0.398 | Test AUC 0.908 (8759.52 s) \n",
      "Epoch 253, Step 570 | Training Acc: 0.941 | Test Acc: 0.861 | Test Loss: 0.445 | Test AUC 0.897 (8766.36 s) \n",
      "Epoch 253, Step 760 | Training Acc: 0.953 | Test Acc: 0.877 | Test Loss: 0.325 | Test AUC 0.908 (8773.12 s) \n",
      "(*) Entering Epoch 254 (8780.488 s)\n",
      "Epoch 254, Step 190 | Training Acc: 0.943 | Test Acc: 0.848 | Test Loss: 0.442 | Test AUC 0.884 (8787.26 s) \n",
      "Epoch 254, Step 380 | Training Acc: 0.945 | Test Acc: 0.875 | Test Loss: 0.376 | Test AUC 0.902 (8794.05 s) \n",
      "Epoch 254, Step 570 | Training Acc: 0.959 | Test Acc: 0.893 | Test Loss: 0.307 | Test AUC 0.922 (8800.87 s) \n",
      "Epoch 254, Step 760 | Training Acc: 0.959 | Test Acc: 0.848 | Test Loss: 0.462 | Test AUC 0.898 (8807.66 s) \n",
      "(*) Entering Epoch 255 (8814.996 s)\n",
      "Epoch 255, Step 190 | Training Acc: 0.941 | Test Acc: 0.885 | Test Loss: 0.362 | Test AUC 0.912 (8821.84 s) \n",
      "Epoch 255, Step 380 | Training Acc: 0.955 | Test Acc: 0.867 | Test Loss: 0.377 | Test AUC 0.905 (8828.63 s) \n",
      "Epoch 255, Step 570 | Training Acc: 0.959 | Test Acc: 0.887 | Test Loss: 0.355 | Test AUC 0.916 (8835.39 s) \n",
      "Epoch 255, Step 760 | Training Acc: 0.963 | Test Acc: 0.855 | Test Loss: 0.379 | Test AUC 0.900 (8842.20 s) \n",
      "Model saved to file: checkpoints/vDNN_kst_rho0_end.ckpt-255\n",
      "Training Complete. Time elapsed: 8850.642 s\n",
      "Architecture: rho0 - kst | Layers: 12 | Dropout: 0.75 | Base LR: 0.0008 | Epochs: 256\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "Classification on a new instance is given by the softmax of the output of the final readout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "Building SELU architecture\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/vDNN_kst_rho0_end.ckpt-255\n",
      "checkpoints/vDNN_kst_rho0_end.ckpt-255 restored.\n",
      "Validation accuracy: 0.830248\n",
      "Validation AUC: 0.815689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtan/anaconda3/lib/python3.6/site-packages/matplotlib/font_manager.py:1297: UserWarning: findfont: Font family ['sans-serif'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.887453\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAisAAAF6CAYAAADYlBjnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRu3/AXYTrJecz5g6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b03ab9a53c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Duration: 4.58042 s\n"
     ]
    }
   ],
   "source": [
    "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "vDNN = vanillaDNN(config, training = False)\n",
    "network_output = vDNN.predict(ckpt)\n",
    "\n",
    "np.save(os.path.join(directories.checkpoints, '{}_{}_y_pred.npy'.format(config.channel, config.mode)), network_output)\n",
    "np.save(os.path.join(directories.checkpoints, '{}_{}_y_test.npy'.format(config.channel, config.mode)), df_y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
